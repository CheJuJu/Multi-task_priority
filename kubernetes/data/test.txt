2 2 1.8 2.0 1.6 2.0 1.6 2.0 2.0 2.0 0.0 0 0.0 1 10 16 66	Preprocessing config files for kubectl . I am wondering if it is possible to support < URL > as a configuration file format for : kubectl . . What do you think about supporting it ? Background I noticed that I have to manage similar but different configuration files for our dev , staging and production environments . For example , some debug flags are enabled in dev environments ; some monitoring stuff are enabled in staging and production environments ; each environment has its own set of credentials . On the other hand , other parts of the environments are very similar to each other and must be consistent . Jsonnet is a good way to consistently generate this kind of similar but different configuration files . So it is very helpful for me if kubectl itself recognizes Jsonnet without generating JSON files from Jsonnet .	2	1
1 1 0.8 1.0 1.0 1.0 1.05 1.0 0.6666666666666666 1.0 0.7027027027027027 37 1.0 6 17 87 313	Wait for waitforattach goroutine before exiting . This test suite fails in weird ways without waiting for the goroutine .	1	0
1 1 0.6 1.0 1.1 1.0 1.05 1.0 0.6666666666666666 1.0 1.096774193548387 31 1.0 17 29 86 297	Enhancement for scheduler perf . /priority important-longterm /sig scheduling /assign Currently , the scheduler perf works pretty well for most of the time , but some issues could affect the benchmark numbers . We could improve the accuracy and usability through the following tasks : [ x ] Get a precise benchmarking test result for each test [ x ] Remove the cost of the : defer . function in the benchmark scheduling < URL > [ x ] Use watch apiserver instead of listing all the pods and remove the : time . Sleep . in benchmark scheduling tests < URL > [ ] Evaluate the semantics of b . N in Golang , and use it properly in the benchmark tests . Right now we manually set b . N to the pods ' number of a test case < URL > [ ] Consider making the workload customizable and easy to use , e.g. pass workload YAMLs instead of hard-coded specs < URL > @bsalamat do you have any thoughts on improving the current scheduler perf ? Thank @Huang -Wei for coaching and pointing out issues in the current scheduling performance tests .	2	1
1 1 0.8 1.0 0.9 1.0 0.75 1.0 1.0 1.0 0.6551724137931034 29 1.0 4 11 22 111	Fix formatting for kubelet memcg notification threshold . /kind bug What this PR does / why we need it : This fixes the following errors ( found in < URL > ): : eviction_manager . go : 256 ] eviction manager attempting to integrate with kernel memcg notification api . : threshold_notifier_linux . go : 70 ] eviction : setting notification threshold to 4828488Ki . : eviction_manager . go : 272 ] eviction manager : failed to create hard memory threshold notifier : invalid argument . Special notes for your reviewer : This needs to be cherrypicked back to 1.10 . This regression was added in < URL > because the : quantity . being used was changed from a DecimalSI to BinarySI , which changes how it is printed out in the String () method . To make it more explicit that we want the value , just convert Value () to a string . Release note : : Fix memory cgroup notifications , and reduce associated log spam . .	1	0
1 1 1.6 2.0 1.5 2.0 1.55 2.0 1.6666666666666667 2.0 1.2647058823529411 34 1.0 1 4 13 54	Allow admins to implement controller loops from the CLI ( IFTTT ) . It should be possible to script Kube from the cli via simple commands in kubectl that implement the list-and-watch pattern and pass those objects to scripts . Examples I want to watch all services and and tweak the external load balancer settings when it runs : : kubectl get services -- watch -- all -t ' {{ . portalIP }}\n ' | xargs ... . Needs something that makes it easy to exec a process per change , and potentially filter based on event type .	2	1
1 1 1.0 1.0 1.0 1.0 0.9 1.0 1.3333333333333333 1.0 0.0 0 0.0 3 24 75 311	when kubelet restarted , keep the readiness value intact until probed again . What type of PR is this ? /kind bug What this PR does / why we need it : if the kubelet is restart , the status of the pod in the node will be set to failure , and then set to ready . the pod will be remove from ep and than add to ep . If the number of the service is 1 , the result is that the service can't connect by other service at that duration , but the pod is work . if lots of kubelet is restart , it will affect more services Which issue(s ) this PR fixes : Fixes #78733 Special notes for your reviewer : tested on kubernetes v 1.16.2 Does this PR introduce a user-facing change ? : : NONE .	1	0
1 1 1.4 1.0 1.0 1.0 1.05 1.0 1.3333333333333333 1.0 1.0 20 1.0 21 48 84 169	Point CSI to official 0.2.0 tag . Is this a BUG REPORT or FEATURE REQUEST ? : Uncomment only one , leave it on its own line : /kind bug /kind feature What happened : Kubernetes vendor directory for CSI should be point at CSI v 0.2 for k8s v 1.10 . There should be no functional changes . What you expected to happen : How to reproduce it ( as minimally and precisely as possible ) : Anything else we need to know ? : Environment : - Kubernetes version ( use : kubectl version . ): - Cloud provider or hardware configuration : - OS ( e.g. from /etc/os-release ): - Kernel ( e.g. : uname -a . ): - Install tools : - Others : incorrectly	0	0
0 0 0.2 0.0 0.4 0.0 0.6 1.0 0.3333333333333333 0.0 0.6645367412140575 313 1.0 18 44 68 312	  Automated cherry pick of #82579 : Expose etcd metric port in tests . Cherry pick of #82579 on release- 1.16 . 82579 : Expose etcd metric port in tests	0	0
1 1 1.0 1.0 1.0 1.0 0.9 1.0 1.0 1.0 0.875 16 0.5 8 15 18 49	Automated cherry pick of #58223 : Adjust the Stackdriver Logging length test . Cherry pick of #58223 on release- 1.9 . 58223 : Adjust the Stackdriver Logging length test This cherry-pick also includes changes from < URL > : NONE .	1	0
1 1 1.0 1.0 1.0 1.0 0.85 1.0 1.0 1.0 0.75 48 1.0 8 22 87 278	Run block tests for gce-pd csi driver . Improve skip block test function name . /kind cleanup /kind feature /priority important-soon /sig storage /assign @msau42 : NONE .	1	1
1 1 1.4 1.0 1.4 1.5 1.25 1.0 1.3333333333333333 1.0 0.0 0 0.0 10 11 28 128	PVC pending Failed to get GCE GCECloudProvider with error . Is this a BUG REPORT or FEATURE REQUEST ? : Uncomment only one , leave it on its own line : /kind bug /kind feature What happened : -- cloud-provider = gce for kubelet service account that has permission to create gce pd added to the VM install k8s cluster using kubeadm in the VM Created a Dynamic StorageClass slow using provisioner : kubernetes.io/gce-pd in k8s cluster , according to official doc example : < URL > Create a PVC bind with StorageClass Slow error : Failed to provision volume with StorageClass ' slow ' : Failed to get GCE GCECloudProvider with error What you expected to happen : PVC is created correctly and bind with StorageClass Slow How to reproduce it ( as minimally and precisely as possible ) : - kubeadm install 1.11.1 in gcloud VM , single node - create service account in gcloud that has permission to create gce pd , and grant the permission to the VM - set -- cloud-provider to gce for kubelet in KUBELET_KUBECONFIG_ARGS in /etc/systemd/system/kubelet . service . d/10-kubeadm . conf - set /etc/kubernetes/cloud-config with content : [ Global ] project-id = ' xxx ' Anything else we need to know ? : Environment : - Kubernetes version ( use : kubectl version . ): 1.11.1 - Cloud provider or hardware configuration : gce - OS ( e.g. from /etc/os-release ): Ubuntu 16.04.4 LTS - Kernel ( e.g. : uname -a . ): 4.13.0 -1019-gcp #23 -Ubuntu SMP - Install tools : kubeadm - Others : Basically same issue as < URL > but that one weirdly is closed because some people thought it was local storage while it's not .	2	0
1 1 0.6 1.0 0.9 1.0 0.9 1.0 0.6666666666666666 1.0 0.9322033898305084 59 1.0 14 33 59 285	dynamic delegated authn header reload . Owed for comment on < URL > Allow delegated authentication to dynamically react to changes to header names or allows subjectes . @kubernetes /sig-auth-pr-reviews /priority important-soon /kind bug : NONE .	1	0
1 1 1.2 1.0 1.2 1.0 1.25 1.0 1.0 1.0 1.12 25 1.0 8 17 56 183	tests : Replaces dnsutils image used with agnhost ( part 4 ) . What type of PR is this ? /kind feature /sig testing /sig windows /area conformance What this PR does / why we need it : Quite a few images are only used a few times in a few tests . Thus , the images are being centralized into the agnhost image , reducing the number of images that have to be pulled and used . This PR replaces the usage of the following images with agnhost : dnsutils dnsmasq is a Linux specific binary . In order for the tests to also pass on Windows , CoreDNS should be used instead . Which issue(s ) this PR fixes : Related : #76342 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	1	1
1 1 0.6 1.0 0.5 0.5 0.75 1.0 1.0 1.0 0.8 5 1.0 5 21 101 275	Automated cherry pick of #70616 : flush iptable chains first and then remove them . while cleaning up ipvs mode . flushing iptable chains first and then remove the chains . this avoids trying to remove chains that are still referenced by rules in other chains . issue : #70615 /sig network /kind bug /priority important-soon	1	0
1 1 0.6 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.1818181818181819 22 1.5 10 24 49 230	Add missing VisitArbitrary methods in kubectl explain . What type of PR is this ? /kind bug /priority important-longterm /sig cli What this PR does / why we need it : This is alternative approach to < URL > and I think is a better one . Which issue(s ) this PR fixes : This was noticed in < URL > Special notes for your reviewer : /assign @liggitt @sttts @roycaihw @apelisse Does this PR introduce a user-facing change ? : : NONE .	2	0
1 1 1.2 1.0 1.3 1.0 1.1 1.0 1.3333333333333333 1.0 0.0 0 0.0 10 16 70 316	fix label mismatching which broke e2e serial test . What type of PR is this ? /kind bug What this PR does / why we need it : mismatched label broke e2e serial test Which issue(s ) this PR fixes : Fixes #78651 Does this PR introduce a user-facing change ? : NONE incorrectly	0	0
1 1 1.2 1.0 1.3 1.0 1.25 1.0 1.0 1.0 0.8571428571428571 7 1.0 13 35 56 297	kubelet : guarantee at most only one cinfo per containerID . What type of PR is this ? /kind bug What this PR does / why we need it : I have been attempting to track down and find how duplicate metrics can be injected into the /metrics endpoint of the Kubelet . These duplicate metrics cause the endpoint to 500 due to logic within the prometheus library to error out if a duplicate metric is seen . This fix guarantees that the most recent and active reference of the container is returned . The removed for loop assumes that every container after a certain time is active . This is a bad assumption with containers within a pod starting and stopping . ref : < URL > /cc @sjenning @derekwaynecarr @Random -Liu Which issue(s ) this PR fixes : < URL > Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : NONE .	1	0
1 1 1.4 1.0 1.2 1.0 1.35 1.0 1.0 1.0 0.0 0 0.0 2 10 18 61	Document multiple scheduler feature . . We need a doc that explains how to setup and run multiple schedulers in a cluster . In addition , the doc should also explain how the pods can be scheduled using these schedulers . cc @davidopp @roberthbailey 	1	2
1 1 0.8 1.0 1.2 1.0 1.4 1.5 1.0 1.0 1.5 10 1.5 0 6 12 64	Update the hack/after-build/ verify-linkcheck.sh . < URL > This tool checks the link in types . go is valid . 1 . It needs to be updated since we have multiple groups and unversioned types . go ; 2 . Its regexp also needs to be adapted when cherry-picked to release branch to prevent problems like #16228 . 	2	2
1 1 1.0 1.0 0.8 1.0 1.15 1.0 1.0 1.0 1.2857142857142858 7 1.0 20 40 67 303	Add MetadataProducerFactory for predicates . /kind cleanup /kind feature What this PR does / why we need it : This adds a factory for : predicates . MetadataProducer . , matching the stack for priorities . This is necessary to configure the metadata producer , such as when adding defaults . Also shortened names from : predicates . PredicateMetadata . to : predicates . Metadata . and similarly for priorities . Does this PR introduce a user-facing change ? : : NONE . /priority important-soon	1	1
2 2 0.8 1.0 0.7 1.0 0.8 1.0 1.0 1.0 0.26666666666666666 15 0.0 3 11 54 141	Automated cherry pick of #64026 : Add SELinux support to CSI . Cherry pick of #64026 on release- 1.10 . 64026 : Add GetSELinuxSupport to mounter . : Fixed SELinux relabeling of CSI volumes . .	2	0
1 1 1.0 1.0 1.0 1.0 1.3 1.0 0.6666666666666666 1.0 0.0 0 0.0 2 11 24 67	 1.8 regression : Can't pull images from docker.io without explicit path on centos/fedora/rhel . < URL > bumped the vendored docker library and introduced a regression where pods with images that don't contain an explicit ' docker.io/ ' repo path result in : ErrImagePull . failures . : apiVersion : v1 kind : Pod metadata : name : busybox spec : containers : - name : busybox image : busybox command : - sleep - ' 100 ' terminationGracePeriodSeconds : 0 restartPolicy : Never . Results in the following error : : Failed to pull image ' busybox ' : rpc error : code = Unknown desc = no such image : ' docker.io/library/busybox:latest ' . Pod gets : ErrImagePull . : docker images . shows that the image has been pulled , though not by that name : $ docker images | grep busybox docker.io/busybox latest d20ae45477cb 2 weeks ago 1.13 MB . Changing the image to : docker.io/busybox . in the pod spec corrects the issue . I think the new code that is changing the behavior is somewhere in here < URL > : familiarizeName () . is a new function brought in by the bump . If I checkout the commit before the bump , the problem does not occur . @derekwaynecarr @DirectXMan12 @dashpole related : < URL >	0	0
1 1 1.4 1.0 1.2 1.0 1.2 1.0 1.6666666666666667 2.0 1.3571428571428572 14 1.0 7 15 56 276	Add support for pre-allocated hugepages with 2+ sizes . What type of PR is this ? /kind feature What this PR does / why we need it : Remove the validation for pre-allocated hugepages on node level . Validation is currently the only thing making it impossible to use pre-allocated huge pages in more than one size . We have now quite a few reports from real users that this feature is welcome . Implements the first part of this KEP : < URL > Which issue(s ) this PR fixes : ref #77251 , #82469 , #82544 & #80452 Special notes for your reviewer : This is just a proof of concept , as this will require a KEP because of the API change . Does this PR introduce a user-facing change ? : : Add support for pre-allocated hugepages for more than one page size . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	2	1
1 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9387755102040817 196 1.0 3 15 50 193	Automated cherry pick of #88610 : fix : azure file mount timeout issue . Cherry pick of #88610 on release- 1.16 . 88610 : fix : azure file mount timeout issue For details on the cherry pick process , see the < URL > page .	1	0
0 0 1.4 2.0 1.4 1.5 1.15 1.0 1.0 1.0 0.0 0 0.0 1 4 6 25	Kubectl Get jobs with label selector returns blank . Kubernetes version ( use : kubectl version . ): 1.3.4 What happened : Performing a : kubectl get jobs -l < any label selector > . returns no jobs despite their existence . What you expected to happen : List of jobs matching the label selector How to reproduce it ( as minimally and precisely as possible ): Create a job with a label . Call : kubectl get jobs -l < selector > . Anything else do we need to know : I am not sure if this is similar to #22517 or not	1	0
1 1 1.2 1.0 1.2 1.0 1.4 1.5 1.3333333333333333 1.0 1.75 8 2.0 8 10 33 83	Provide more explanation on networking in the Guestbook App help documentation . . Feedback from users at Hackathon : -- My guestbook didn't come up and I couldn't figure out why . Turns out it was networking . The whole thing about networks , IPs etc is super confusing -- 1 ) why do docs show public IP when source doesn't ? ( copy-and-paste workflow breaks ) , 2 ) createExternalLoadBalancer syntax is not illustrated in the docs , 3 ) Participant was having trouble getting publicIPs method to work , though syntax & configs look right ( Ken had same result -- all looked fine but couldn't connect ) -- First reference to createExternalLoadBalancer is confusing , this concept hasn't been introduced . 	1	2
2 2 1.2 1.0 0.9 1.0 1.0 1.0 1.6666666666666667 2.0 1.1363636363636365 44 1.0 9 38 56 157	Log when client rate limiter latency is very high at a lower log level . What type of PR is this ? : /kind feature /priority important-soon /sig api-machinery /cc @lavalamp What this PR does / why we need it : Currently , high client side rate limiter latency logging only happens when at log level 3 , it would be useful to also log this information at lower log levels , but we need to avoid spamming the logs . This PR logs a maximum of once per second whenever the rate limiter latency exceeds 1s . Does this PR introduce a user-facing change ? : : API request throttling ( due to a high rate of requests ) is now reported in client-go logs at log level 2 . The messages are of the form Throttling request took 1.50705208 s , request : GET : < URL > The presence of these messages , may indicate to the administrator the need to tune the cluster accordingly . .	1	1
1 1 1.2 1.0 1.2 1.0 1.1 1.0 1.0 1.0 0.6550925925925926 432 1.0 6 14 27 221	Lifecycle generator updates . What type of PR is this ? /kind feature What this PR does / why we need it : * Logs at lower verbosity to match other generators * Switches major/minor return types to int * Prefixes generated method names with APILifecycle to avoid collisions * Adds a : +k8s : prerelease-lifecycle-gen : replacement = group , version , kind . tag for indicating a replacement API : NONE . /cc @deads2k	2	1
0 0 0.6 1.0 0.5 0.5 0.65 1.0 0.6666666666666666 1.0 1.0 1 1.0 5 16 30 90	Workaround go-junit-report bug for TestApps . What this PR does / why we need it : Fix output from pkg/kubectl/apps/TestApps unit test Which issue this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close that issue when PR gets merged ) : fixes #51253 Special notes for your reviewer : Literally copy-pasta of the approach taken in #45320 . Maybe a sign that this should be extracted into something shared . I'm just trying to see if we can make < URL > and < URL > a little more green for now . : NONE .	1	0
2 2 2.0 2.0 1.7 2.0 1.75 2.0 2.0 2.0 1.641025641025641 39 2.0 0 0 4 18	CLI-based discovery of resources . I can discover what API Groups are present on my cluster using : kubectl api-versions . . However , I cannot find out what Kinds are in each Group . If I knew what kinds there were , then I could do : kubectl get $KIND . or : kubectl explain $KIND . . Top-down exploration will become more important as there are more TPRs and AAs being installed into clusters . The request here is to have a command that tells the kinds inside a specific api-version .	2	1
1 1 1.2 1.0 1.0 1.0 0.9 1.0 1.3333333333333333 1.0 0.6776315789473685 456 1.0 7 26 38 195	CSR v1 - promote RotateKubeletClientCertificate to GA . What type of PR is this ? /kind feature What this PR does / why we need it : Promotes the RotateKubeletClientCertificate feature to GA xref < URL > Does this PR introduce a user-facing change ? : : The RotateKubeletClientCertificate feature gate has been promoted to GA , and the kubelet -- feature-gate RotateKubeletClientCertificate parameter will be removed in 1.20 . . /milestone v 1.19 /priority important-soon /sig auth node /cc @munnerz @mtaufen	1	1
2 2 1.2 1.0 1.0 1.0 1.15 1.0 1.6666666666666667 2.0 1.0 3 1.0 3 9 13 65	 Brokwn download link in gh-pages . Similar to #16393 , in < URL > the download links doesn't work : - < URL > - < URL > cc @bgrant0607 @caesarxuchao @nikhiljindal @krousey 	0	2
1 1 1.0 1.0 0.8 1.0 1.0 1.0 1.0 1.0 1.4444444444444444 9 1.0 8 41 78 272	Cluster Autoscaler 1.1.0 -beta1 . This PR will be shortly followed with one updating Cluster Autoscaler to 1.1.0 ( final ) . : NONE . incorrectly	0	1
2 2 1.6 2.0 1.6 2.0 1.35 1.0 1.3333333333333333 2.0 1.5 2 1.5 0 0 3 20	Bug : reflector . go stores objects in cache by name only , broken in namespaces . This is broken for resources that are namespaced . We need to store objects in the cache by namespace+name . Example : Two resources with same name in different namespaces will cause a cache clash . incorrectly	0	0
1 1 1.0 1.0 1.2 1.0 1.35 1.0 1.0 1.0 0.0 0 0.0 11 20 36 125	  vendor : update hcsshim to v 0.6.11 . Ran all the godep scripts according to < URL > seems like some other lines in Godeps . json got modified as well but they are all the same sha only slightly longer ... Signed-off-by : Jess Frazelle < URL > cc @liggitt @cjcullen @tallclair @philips : Updated hcsshim dependency to v 0.6.11 .	0	0
2 2 1.8 2.0 1.3 1.0 1.1 1.0 1.6666666666666667 2.0 0.9558823529411765 68 1.0 12 39 73 302	Enable the RuntimeClass admission controller for scheduling . What type of PR is this ? /kind feature What this PR does / why we need it : With < URL > the RuntimeClass admission controller is no longer only tied to PodOverhead , so enable it whenever the RuntimeClass feature is enabled . Which issue(s ) this PR fixes : For #81016 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : - [ KEP ]: < URL > . /priority important-soon /milestone v 1.16 /sig node /cc @egernst @draveness /assign @thockin	1	1
0 0 0.6 1.0 0.8 1.0 1.0 1.0 0.6666666666666666 1.0 0.6666666666666666 3 1.0 3 9 15 68	  Create pod latency increase . The node e2e density tests have been failing consistently : < URL > The only PRs in the diff that touch the kubelet seem related to CNI : < URL > with the likely candidate being #51250 In the run prior to it failing , the latency ranged from 3s to 4s . In the first failing run , 50% percentile latency ranged from ~ 16s to ~ 19s , which is a major increase .. cc @kubernetes /sig-node-bugs @kubernetes /sig-network-bugs cc @dixudx	0	0
0 0 0.6 0.0 0.7 0.5 1.0 1.0 0.3333333333333333 0.0 1.0 23 1.0 2 7 23 111	Failing test : [ sig-testing ] Deferred TearDown failing a number of jobs in sig-release-master-blocking and master-upgrade . Failing Job < URL > ( < URL > ) < URL > ( < URL > ) < URL > ( < URL > ) < URL > ( < URL > ) < URL > ( < URL > ) < URL > ( < URL > ) < URL > ( < URL > ) Failing Test < URL > Has been failing consistently since 4/24 . < URL > /kind bug /priority failing-test /priority important-soon /sig testing /milestone 1.11 @kubernetes /sig-testing-bugs cc @jberkus @tpepper /assign @BenTheElder for triage	1	0
1 1 1.4 1.0 1.1 1.0 1.25 1.0 1.6666666666666667 2.0 0.0 0 0.0 2 6 10 61	apiserver allows duplicate service port . Is this a BUG REPORT or FEATURE REQUEST ? : /kind bug What happened : I was able to add a conflicting service port to an existing service . Attempting to remove the port by modifying the yml and applying fails with : The Service ' atest ' is invalid : spec . ports : Required value . Note : Kubernetes 1.8 will not accept a service . yml with conflicting ports if creating for the first time What you expected to happen : The server should reject the request and ( in this case ) : kubectl . should error . How to reproduce it ( as minimally and precisely as possible ) : - Created service . yml with a single port definition and successfully applied it : apiVersion : v1 kind : Service metadata : name : atest spec : ports : - port : 8080 targetPort : 80 protocol : TCP name : http . Updated service . yml with an additional identical port definition and successfully applied it : apiVersion : v1 kind : Service metadata : name : atest spec : ports : - port : 8080 targetPort : 80 protocol : TCP name : http - port : 8080 targetPort : 80 protocol : TCP name : dummy . Updated service . yml by removing the additional port but applying it failed Anything else we need to know ? : On adding the additional port , the service definition ( on the server ) still only has the original port but : kubectl.kubernetes.io/last-applied-configuration . is now in an inconsistent state making : kubectl . attempt to remove the port when it calculates its diff for the patch request . Environment : - Kubernetes version ( use : kubectl version . ): 1.8 - Cloud provider or hardware configuration : AWS - OS ( e.g. from /etc/os-release ): Ubuntu 16.04 - Kernel ( e.g. : uname -a . ): 4.4 - Install tools : - Others :	2	0
0 0 0.6 0.0 1.3 2.0 1.3 1.5 0.6666666666666666 0.0 1.0 2 1.0 1 6 11 63	PATCH with {'$patch ' : ' delete ' } creates key if non-existing . Having a deployment like : : apiVersion : extensions/v1beta1 kind : Deployment spec : template : spec : containers : - name : mycontainer ... . and applying the following patch : : kubectl patch deployment mydeployment -p ' {' spec ' : {' template ' : {' spec ' : {' containers ' : [{' name ' : ' mycontainer ' , ' env ' : [{'$patch ' : ' delete ' , ' name ' : ' FOOBAR ' }]}]}}}}' . results in : : apiVersion : extensions/v1beta1 kind : Deployment spec : template : spec : containers : - name : mycontainer env : - name : FOOBAR ... . Applying it over and over again results in oscillating behaviour : The key is deleted and re-created over and over again . Expected behaviour is that the operation is idempotent , leaving the key non-existing . Kubernetes v 1.11 /kind bug	2	0
2 2 1.4 2.0 1.3 1.0 1.3 1.0 2.0 2.0 0.0 0 0.0 11 28 41 221	Use correct path when installing go-bindata . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespace from that line : /kind api-change /kind bug /kind cleanup /kind deprecation /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : Use correct path when installing go-bindata When using : go get -u xxx/xxx/ ... . , the path like : xxx/xxx/ ... . is ok . But if we use : go install xxx/xxx/ ... . , it could be made error . : 鉃?kubernetes git :( master ) make generated_files warning : ignoring symlink /root/goproject/kubernetes/src/ k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes go : warning : ' k8s.io/kubernetes/vendor/github.com/go-bindata/go-bindata/ ... ' matched no packages 鉃?kubernetes git :( master ) go version go version go 1.13.8 linux/amd64 鉃?kubernetes git :( master ) echo $GOPATH /root/goproject/kubernetes 鉃?kubernetes git :( master ) echo $PATH /root/goproject/kubernetes/bin : /usr/local/sbin : /usr/local/bin : /usr/sbin : /usr/bin : /sbin : /bin : /usr/games : /usr/local/games : /usr/local/java/bin : /usr/local/go/bin : /root/golang/bin 鉃?kubernetes git :( master ) pwd /root/goproject/kubernetes/src/ k8s.io/kubernetes 鉃?kubernetes git :( master ) go install k8s.io/kubernetes/vendor/github.com/go-bindata/go-bindata/ ... go : warning : ' k8s.io/kubernetes/vendor/github.com/go-bindata/go-bindata/ ... ' matched no packages 鉃?kubernetes git :( master ) . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	1	0
2 2 1.0 1.0 0.9 1.0 1.15 1.0 1.3333333333333333 1.0 1.2372881355932204 59 1.0 17 29 64 284	increase auth cache size . The benchmark in < URL > demonstrates that cache performance gets drastically worse when the number of tokens exceeds the size of the cache . We advertise 5k nodes and 10k namespaces , it's reasonable to assume each node has a token and each namespace has a service account , which has a token . This PR makes the cache accept 32k tokens . ( It was 4k . ) : Authentication token cache size is increased ( from 4k to 32k ) to support clusters with many nodes or many namespaces with active service accounts . . /kind bug /priority important-soon	1	0
2 2 1.2 1.0 1.2 1.0 1.3 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 2 13 157	Kubernetes should not mount default service account credentials by default . /kind bug What happened : Kube < URL > default service account credentials , which allows any compromised pod to run API commands against the cluster . This seems like a very odd choice from a security standpoint - I only just discovered this was the case after a couple years of running a Kube cluster in production . What you expected to happen : Pods not having cluster-modifying power by default . How to reproduce it ( as minimally and precisely as possible ) : Run a default Kube cluster and pod . Anything else we need to know ? : n/a Environment : - Kubernetes version ( use : kubectl version . ): Client Version : version . Info{Major:' 1 ' , Minor:' 8 ' , GitVersion:' v 1.8.4 ' , GitCommit:' 9befc2b8928a9426501d3bf62f72849d5cbcd5a3 ' , GitTreeState:' clean ' , BuildDate:' 2017-11-20T 05:28:34 Z ' , GoVersion:' go 1.8.3 ' , Compiler:' gc ' , Platform:' darwin/amd64 ' } Server Version : version . Info{Major:' 1 ' , Minor:' 8+' , GitVersion:' v 1.8.4 -gke . 1 ' , GitCommit:' 04502ae78d522a3d410de3710e1550cfb16dad4a ' , GitTreeState:' clean ' , BuildDate:' 2017-12-08T 17:24:53 Z ' , GoVersion:' go 1.8.3 b4 ' , Compiler:' gc ' , Platform:' linux/amd64 ' } - Cloud provider or hardware configuration : GKE	2	1
2 2 1.2 1.0 1.1 1.0 1.05 1.0 1.3333333333333333 1.0 1.0129870129870129 77 1.0 19 40 62 245	Move resource-based priority functions to their Score plugins [ Migration Phase 2 ] . /sig scheduling /priority important-soon See #86399 as an example . There are two parts to this : PR 1 : move RequestedToCapacityRatio plugins/requestedtocapacityratio/requested_to_capacity_ratio . go to plugins/noderesources/requested_to_capacity_ratio . go PR 2 : move resource_allocation . go to plugins/noderesources/ and move the logic in Least/Most/Balanced and RequestedToCapacityRatio logic to their Score plugins . This needs to be done in a single PR . Part of #85822 /assign @notpad	1	1
0 0 0.8 1.0 1.2 1.0 1.2 1.0 1.0 1.0 1.4 5 1.0 14 41 66 331	Fix memory leak from not closing hcs containers . What type of PR is this ? /kind bug What this PR does / why we need it : Windows containers opened through : hcsshim . OpenContainer . need to be freed through : container . Close () . or they leak memory . This is a memory dump from a Windows kubelet I took with Go's pprof : Which issue(s ) this PR fixes : Fixes #78555 Does this PR introduce a user-facing change ? : : Fixes a memory leak in Kubelet on Windows caused by not not closing containers when fetching container metrics . /assign @feiskyer @PatrickLang /sig windows Only thing worth noting is that I decided if for some reason : container . Close () . does error , to return the error rather than logging it and proceeding . Seems like something is substantially wrong if this happens .	1	0
1 1 1.2 1.0 1.0 1.0 1.1 1.0 1.3333333333333333 1.0 0.0 0 0.0 3 9 35 173	add method for async deletion in vmss client without waiting on future . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespace from that line : /kind api-change /kind bug /kind cleanup /kind deprecation /kind design /kind documentation /kind failing-test /kind feature /kind flake /kind feature What this PR does / why we need it : Supports deleting VMSS instances without waiting on the asynchronous future . This is needed for autoscaler since certain node deletion paths assume non-blocking operations or else the main loop will hang . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : . /area provider/azure /sig cloud-provider /priority important-soon	1	1
1 1 1.8 2.0 1.7 2.0 1.8 2.0 1.6666666666666667 2.0 1.3333333333333333 63 1.0 4 11 26 88	kubectl apply should have a way to indicate ' no change ' . Some configuration management systems distinguish between ' no change ' , ' changed ' , and ' failed ' . It should be possible to use kubectl apply and know when any changes were applied from normal shell scripting Since ' n o-o p ' is also success , and we don't expect clients to parse our stdout/stderr , it seems reasonable that we should allow a kubectl apply caller to request that a n o-o p be given a special exit code that we ensure no other result can return . Since today we return 1 for almost all errors , we have the option to begin defining ' special ' errors . Possible options : : kubectl apply ... -- fail-when-unchanged = 2 . returns exit code 2 ( allows user to control exit code ) : kubectl apply ... -- fail-when-unchanged . returns exit code 2 always ( means we can document the exit code as per UNIX norms ) The latter is probably better . Naming of course is up in the air . @kubernetes /sig-cli-feature-requests I rate this as high importance for integration with config management ( like Ansible ) which expects to be able to discern this .	2	1
0 0 0.4 0.0 0.5 0.5 0.65 1.0 0.3333333333333333 0.0 0.58 200 0.0 13 31 62 232	  Automated cherry pick of #73443 : update json-patch to pick up bug fixes . Cherry pick of #73443 on release- 1.11 . 73443 : update json-patch to pick up bug fixes	0	0
1 1 1.0 1.0 0.7 0.5 1.0 1.0 1.0 1.0 1.0 6 1.0 20 37 60 111	Umbrella : dynamic audit configuration to beta . This holds a list of issues to be completed before dynamic audit configuration moves to beta . [ ] scalability tests < URL > [ ] webhook authentication < URL > [ ] complete policy < URL > ( nice to have , not required ) [ ] cheap buffers < URL > [ x ] e2e test < URL > [ x ] integration test < URL > [ ] webhook versioning < URL > [ ] remove feature gate and update api version < URL >	2	1
0 0 0.8 1.0 0.9 1.0 0.75 1.0 0.6666666666666666 1.0 0.5 4 0.5 14 45 67 129	fix kubeadm upgrade regression . What type of PR is this ? /kind bug What this PR does / why we need it : To fix kubeadm v 1.13 error when upgrading clusters created with kubeadm v 1.12 . Which issue(s ) this PR fixes : Fixes # < URL > Special notes for your reviewer : The error happens when connecting to etcd , and this regression was introduced by < URL > kubeadm v 1.12 cluster are deployed with etcd listening on localhost only , while kubeadm v 1.13 assumes etcd is listening both on localhost and on the advertising address . This PR makes kubadm v 1.13 support both cases . Does this PR introduce a user-facing change ? : : NONE . /sig cluster-lifecycle /priority critical-urgent /cc @timothysc /cc @rdodev /cc @neolit123 @kubernetes /sig-cluster-lifecycle-pr-reviews incorrectly	0	0
2 2 1.6 2.0 1.6 2.0 1.65 2.0 2.0 2.0 0.0 0 0.0 4 6 23 74	Feature : Request text present in HTTP health check . < URL > probe documentation should state if it support HTTPS or not . Also ( like Google Monitoring ) it should allow to specify a text present in the response . Often it may return 200 but parts don't work and the expected text would not be present . Sure a : exec . may do that kind of check but then it wouldn't be high level like the : httpGet . check .	2	1
1 1 1.2 1.0 1.4 1.5 1.35 1.0 1.3333333333333333 1.0 1.4285714285714286 7 2.0 0 0 10 43	Scheduling of pods with RWO volumes . Since RWO volumes work only on a single host , wouldn't it be reasonable for the scheduler to schedule pods for a { RC , RS , D , etc . } that uses such a volume on the same node ? Otherwise , pods that land on different nodes are essentially blocked . Deployments are the most affected since you can have a Deployment with just one pod and maxSurge = 1 ( which is actually the default currently ) and with a RWO volume , it will block . @kubernetes /sig-scheduling @kubernetes /deployment @kubernetes /sig-storage	1	1
1 1 1.0 1.0 0.9 1.0 0.95 1.0 0.6666666666666666 1.0 0.7058823529411765 51 0.0 13 47 81 293	kubeadm : show warning in case of missing kube-proxy ConfigMap . What this PR does / why we need it : Changes made : - Add a new error type : kubeProxyConfigMap . - Return this error in cases where the ConfigMap cannot be fetched - If the same error is found show a warning instead of failing - Add checks if the KubeProxy config is nil during : - upgrade - when applying dynamic defaults ( : SetClusterDynamicDefaults . ) This change allows the user to skip the kube-proxy addon phase and later kubeadm command like join or upgrade would not fail , but show a warning the the cluster is in a unsupported state . The change falls under cluster variants and should be considered a bug fix at this point , until the proposal for cluster variants and addon management is more clear . Which issue(s ) this PR fixes : Fixes kubernetes/kubeadm #1349 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : kubeadm : handle cases of absent kube-proxy addon . /assign @fabriziopandini @kubernetes /sig-cluster-lifecycle-pr-reviews /kind bug /priority important-longterm /hold	2	0
1 1 1.2 1.0 1.3 1.0 1.3 1.0 1.3333333333333333 1.0 1.4 10 1.5 6 36 81 317	Adding FQDN address type for EndpointSlice . What type of PR is this ? /kind feature What this PR does / why we need it : This PR adds a new FQDN address type for EndpointSlices to expand the types of endpoints that can be supported . This is one of the additions identified in the corresponding KEP . Does this PR introduce a user-facing change ? : : Adds FQDN addressType support for EndpointSlice . . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : - Enhancement Issue : < URL > - KEP : < URL > /sig network /priority important-longterm /cc @freehan @bowei	2	1
1 1 1.4 1.0 1.2 1.0 1.3 1.0 1.3333333333333333 1.0 0.0 1 0.0 20 34 84 256	Add instructions for installing go-bindata . What type of PR is this ? /kind cleanup /kind documentation What this PR does / why we need it : Add instructions for installing go-bindata . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . 	2	2
2 2 1.8 2.0 1.5 2.0 1.35 1.0 1.6666666666666667 2.0 2.0 3 2.0 6 11 22 82	kubectl -- validate doesn't work for API group objects inside lists . : $ kubectl create -f hpa . yaml horizontalpodautoscaler ' myhpa ' created $ kubectl delete hpa -- all horizontalpodautoscaler ' myhpa ' deleted $ kubectl create -f hpas . yaml error validating ' hpas . yaml ' : error validating data : couldn't find type : v1beta1 . HorizontalPodAutoscaler ; if you choose to ignore these errors , turn validation off with -- validate = false $ kubectl create -f hpas . yaml -- validate = false horizontalpodautoscaler ' myhpa ' created . hpa . yaml : : apiVersion : extensions/v1beta1 kind : HorizontalPodAutoscaler metadata : name : myhpa spec : maxReplicas : 1 minReplicas : 1 scaleRef : kind : ReplicationController name : rc subresource : scale . hpas . yaml : : apiVersion : v1 items : - apiVersion : extensions/v1beta1 kind : HorizontalPodAutoscaler metadata : name : myhpa spec : maxReplicas : 1 minReplicas : 1 scaleRef : kind : ReplicationController name : rc subresource : scale kind : List metadata : {} .	2	0
2 2 2.0 2.0 1.7 2.0 1.45 2.0 2.0 2.0 1.5 4 1.5 0 4 5 26	[ Federation ] ClustersSynced(clusters [] * federationapi . Clusters ) documentation doesn't seem to match the implementation . It appears that there is a bit of an impedance mismatch between how the FederatedInformer discusses ClustersSynced(clusters [] federationapi . Clusters ) and the actual implementation of that method . Conceptually , the idea of ClustersSynced(clusters [] federationapi . Clusters ) seems to be to provide an ongoing update of whether the Federated store is synchronized with the underlying clusters ' stores . In practice , it does this by checking the HasSynced () method on each of the clusters ' informers . HasSynced () only promises to return whether , at some point in the past , the informer was synced , and from the implementation it appears that it does this by checking that the queue has been populated with some Add/Update/Delete operations , or was populated with some Replace operations that completed . Interestingly , if there is an in-flight Add , Update or Delete operation , HasSynced () appears to return true before that operation has completed . So , ClustersSynced(clusters [] * federationapi . Clusters ) doesn't quite provide the depth of information that it implies in its name and its documentation . Note that the ClustersSynced () method on FederationView does note this in the comment . It may be worth renaming these to HaveClustersBeenSynced () or some such to make it clear that they do not provide information about the current sync state , but only that at some point they were synced ( or at some point were slated to be synced , which seems to be the promise that is provided ) . jk Someone else may want to look into this code to corroborate these findings . 	2	2
0 0 0.4 0.0 0.8 1.0 0.9 1.0 0.3333333333333333 0.0 0.0 1 0.0 3 23 55 174	Fix PV allocation on non-English vSphere . What type of PR is this ? /kind bug What this PR does / why we need it : Fixes PV allocation on a non-English vSphere installation Which issue(s ) this PR fixes : Fixes #71997 Special notes for your reviewer : I have manually verified that PV allocation still works on an English vSphere . I can not test it on a non-English vsphere as I don't have that at hand , however since the testcase contains the error message @ipointffogl mentioned in #71997 I'd argue its safe to assume that this will fix the issue . The whole thing still feels like a rubberduck fix because the proper solution would be to not use API error messages to gather infos . /shrug /area vsphere Does this PR introduce a user-facing change ? : : Fixed a bug that caused PV allocation on non-English vSphere installations to fail .	2	0
1 1 0.6 1.0 0.9 1.0 1.0 1.0 0.6666666666666666 1.0 1.0 47 1.0 13 22 76 298	Updated NewSnapshot interface to accept a NodeInfoMap . What type of PR is this ? /kind feature What this PR does / why we need it : Changes the NewSnapshot interface to accept a NodeInfoMap instead of a list of nodes and pods . This will make it easier for CA to create a Snapshot from the NodeInfoMap that it maintains across simulations . Which issue(s ) this PR fixes : Fixes < URL > Does this PR introduce a user-facing change ? : : NONE . /assign @alculquicondor /priority important-soon	1	1
2 2 1.8 2.0 1.6 2.0 1.7 2.0 1.6666666666666667 2.0 1.5238095238095237 42 2.0 3 24 48 255	Properly close the file in makeFile . What type of PR is this ? /kind bug What this PR does / why we need it : In makeFile , the File pointer returned from os . OpenFile may be nil . We shouldn't blindly call Close on the File pointer . : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	2	0
0 0 0.2 0.0 0.3 0.0 0.7 0.5 0.0 0.0 0.16666666666666666 6 0.0 8 17 50 256	 Update Cluster Autoscaler version to 1.13.0 -rc . 2 . This PR updates CA version in gce manifests to 1.13.0 -rc . 2 . We need to merge it during freeze because , Cluster Autoscaler imports large amount of k8s code to simulate scheduler behavior , yet it lives in separate repository . Therefore we need to build final release candidate just at the end of k8s release cycle . : Update Cluster Autoscaler version to 1.13.0 -rc . 2 . Release notes : < URL > .	0	0
2 2 1.2 1.0 1.0 1.0 1.05 1.0 1.3333333333333333 1.0 0.0 0 0.0 2 4 12 65	The v 1.1 of Kubernetes Installation for CentOS is point to v 0.18 . Hi , I follow the document [ 1 ] to install the v 1.1.1 which Kubernetes released the very nice new features and improvements [ 2 ] . The environment that I use is CentOS 7.1 . I found the virt7-testing is pointing to v 0.18 , and also I tried default repo is v 1.0.3 , and virt7-docker-common-candidate repo is 1.2.0 . alpha1 . I wonder whether CentOS users could have an official way to install the v 1.1.1 by the yum ? Thanks . References : [ 1 ] < URL > [ 2 ] < URL > 	2	2
1 1 1.4 2.0 1.3 1.5 1.25 1.0 1.0 1.0 1.0 1 1.0 3 3 7 21	Update examples to use kubectl . Cleanup related to #2144 ; need to replace kubecfg commands with kubectl equivalents in < URL > . 	1	2
0 0 1.4 2.0 1.5 2.0 1.35 1.0 1.0 1.0 0.0 0 0.0 6 25 63 284	avoid major-version package names . Packages in this repository with paths like : k8s.io/api/core/v1 . are named according to their major version ( : package v1 . ) instead of a more semantically meaningful part of the path ( : package core . ) . That forces users of these packages to pretty much always rename them upon import . Unfortunately , fixing the package names for existing packages would be a breaking change . However , for future packages , it might be a good idea to use more descriptive package names in the < URL > . See also < URL >	2	1
1 1 1.0 1.0 1.1 1.0 0.95 1.0 1.0 1.0 1.5454545454545454 11 2.0 9 31 68 304	Remove the pod Info in waitingPodsMap When the pod is deleted . What would you like to be added : When the pod is scheduled to the : permit . phase and I delete the pod , the pod still remains in the : waitingPodsMap . until the plugin allow/reject/timeout(Especially when waiting for a long time ) . Why is this needed : We need to take the initiative to delete the data in the : waitingPodsMap . and terminate the process in waiting state of : permit . plugin to ensure data accuracy , if the pod has been deleted /cc @ahg -g /priority important-soon /assign /sig scheduling	1	1
2 2 1.8 2.0 1.7 2.0 1.7 2.0 2.0 2.0 1.2407407407407407 54 1.0 13 15 50 300	apiserver doesn't auto-recover from `kubectl delete -- all apiservice` . What happened : Deleting apiserver objects deletes all built in api registrations , and they do not come back until apiserver is restarted . ( See report on twitter : < URL > What you expected to happen : I expect that admin credentials are required to do this ; if that's not true , it should be . ( I haven't had time to check . ) I expect that either 1 . it should not be possible to delete the built in registrations , or 2 . that a controller should recreate them if they vanish , or 3 . e.g. apiserver stops reporting healthy if they vanish , so that kubelet will restart it . 3 is maybe the easiest thing to implement . How to reproduce it ( as minimally and precisely as possible ) : : $ kubectl delete -- all apiservice . Don't do it on a cluster you care about .	2	0
0 0 0.6 1.0 0.9 1.0 1.1 1.0 0.6666666666666666 1.0 0.0 0 0.0 3 5 16 60	Add admission metrics for webhooks . Implements the < URL > design . Fixes : < URL > ref : < URL > : Metrics have been added for monitoring admission plugins , including the new dynamic ( webhook-based ) ones . .	1	1
0 0 0.4 0.0 0.8 0.5 0.9 1.0 0.0 0.0 1.5303030303030303 66 2.0 12 19 48 190	Make verify-api-groups.sh not depend on GOPATH . This script quietly depended on being under a GOPATH , and failed completely when it was not . This change sorts the input ( to make comparing results easier ) and operates on files , rather than packages until the last moment when we add back the package prefix . Verified by instrumenting the code and comparing runs inside and outside of GOPATH . /kind bug : NONE .	2	0
1 1 1.0 1.0 0.8 1.0 0.9 1.0 1.0 1.0 1.3333333333333333 9 1.0 5 12 13 62	  v 1.1 known issues / FAQ accumulator . This issue is intended to accumulate known errata / issues / workarounds so we can produce a useful doc upon release . If you have a known issue that is worth putting in our 1.1 release notes , please add a comment here of the form ' NEW : ... ' and write a paragraph explaining the issue and the workaround or repercussions or whatever . cc/ @thockin @bgrant0607 	0	2
2 2 1.2 1.0 1.4 1.5 1.45 1.5 1.3333333333333333 2.0 0.4 10 0.0 1 7 12 46	docs : stop tracking placeholder documentation . What type of PR is this ? /kind cleanup /kind documentation What this PR does / why we need it : The placeholder documentation introduces a couple of problems : - it complicates the contributor-experience ( forces the CI to run N times before the contributor finds out that they need to call an . sh script and include certain files from docs/) - it forces CLI related pull requests for tools like kubeadm and kubectl to require top level approval from docs/OWNERS as such PRs still need to touch the . generated_docs file Stop tracking the placeholder documentation by applying the following actions : - remove the utility set-placeholder-gen-docs () - make verify-generated-docs.sh only generate in a temporary folder and not match . generated_docs - mark generate-docs.sh as an alias for update-generated-docs.sh - remove all current placeholder files in docs folders admin , man , user-guide , yaml - ignore the above folders and . generated_docs in a . gitignore file Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes kubernetes/kubernetes #26205 Special notes for your reviewer : NONE Does this PR introduce a user-facing change ? : : NONE . ^ ? /assign @spiffxp @ixdy @lavalamp /priority important-longterm /cc @kubernetes /sig-docs-maintainers /cc @kubernetes /sig-contributor-experience-pr-reviews /cc @timothysc @fabriziopandini @tengqm 	2	2
2 2 1.6 2.0 1.7 2.0 1.7 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 2 15 95	kubectl top node/pod should have option like ' -- watch ' . /kind feature kubectl top node/pod should have option like ' -- watch ' for watching node/pod resource usage .	2	1
2 2 1.6 2.0 1.5 1.5 1.65 2.0 1.6666666666666667 2.0 1.489795918367347 49 2.0 4 25 74 312	Close the file after reading in verifydependencies #main . What type of PR is this ? /kind bug What this PR does / why we need it : For verifydependencies , in the main () func , after the file is done being read , we should close it . : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	2	0
1 1 1.0 1.0 1.1 1.0 1.05 1.0 1.0 1.0 0.9347826086956522 46 1.0 9 19 64 253	Use consistent imageRef during container startup . What type of PR is this ? /kind bug What this PR does / why we need it : Ensure consistency in container image handling . Special notes for your reviewer : Does this PR introduce a user-facing change ? : --> : NONE . /priority important-soon /sig node	1	0
2 2 1.4 1.0 1.3 1.0 1.2 1.0 1.6666666666666667 2.0 0.9285714285714286 14 1.0 8 24 37 182	api : update Service . Spec . IPFamily docs . What type of PR is this ? /kind cleanup /kind documentation What this PR does / why we need it : Updates the docs on : Service . Spec . IPFamily . to be more accurate and to warn that users shouldn't rely on it other than as specifically documented ( following up on discussions on the mailing list and elsewhere , which make it seem likely that we will probably change it in a future release ) . Does this PR introduce a user-facing change ? : : Updated the API documentation for Service . Spec . IPFamily to warn that its exact semantics will probably change before the dual-stack feature goes GA , and users should look at ClusterIP or Endpoints , not IPFamily , to figure out if an existing Service is IPv4 , IPv6 , or dual-stack . . /sig network /priority important-soon /assign @thockin 	1	2
1 1 0.8 1.0 0.7 1.0 0.8 1.0 1.0 1.0 2.0 2 2.0 19 36 70 184	  GCE : Fix operation polling and error handling . Cloud functions using the generated API are bursting operation GET calls because we don't wait a minimum amount of time . Fixes #64712 Fixes #64858 Changes - : operationPollInterval . is now 1 second instead of 3 seconds . - : operationPollRateLimiter . is now configured with 5 QPS / 5 burst instead of 10 QPS / 10 burst . - : gceRateLimiter . is now configured with a : MinimumRateLimiter . to wait the above : operationPollInterval . duration before waiting on the token rate limiter . - Operations are now rate limited on the very first GET call . - Operations are polled until : DONE . or context times out ( even if operations . get fails continuously ) . - Compute operations are checked for errors when they're recognized as : DONE . . - All ' wrapper ' funcs now generate a context with an hour timeout . : ingress-gce . will need to update its vendor and utilize the : MinimumRateLimiter . as well . Since ingress creates rate limiters based off flags , we'll need to check the resource type and operation while parsing the flags and wrap the appropriate one . Special notes for your reviewer : /assign bowei /cc bowei Fix Example Creating an external load balancer without fix : < URL > with fix : < URL > ( a difference of about 200 GET calls ) Release note : : GCE : Fixes operation polling to adhere to the specified interval . Furthermore , operation errors are now returned instead of ignored . .	0	0
1 1 1.2 1.0 1.3 1.5 1.35 2.0 1.0 1.0 1.1 10 1.0 0 1 17 66	Kubelet doesn't attempt to re-register . In the process of setting up HA , I blew away my master state , which means that node information was lost . The kubelet doesn't attempt to re-register itself . It probably should . This isn't a v 1.0 feature , though . @dchen1107	2	1
2 2 1.0 1.0 1.2 1.0 1.3 1.0 0.6666666666666666 0.0 1.4 35 2.0 15 47 88 292	Fix handling empty result when invoking kubectl get . What type of PR is this ? /kind bug /sig cli /priority important-longterm What this PR does / why we need it : We were wrongly checking : IgnoreNotFound . error when printing results which might lead to panic when an empty file was passed through : kubectl get -f empty_file -o name . Special notes for your reviewer : /assign @juanvallejo Does this PR introduce a user-facing change ? : : NONE .	2	0
1 1 1.4 1.0 1.2 1.0 1.15 1.0 1.6666666666666667 2.0 1.0 1 1.0 9 33 55 280	Move port_allocator to apimachinery . What type of PR is this ? /kind feature What this PR does / why we need it : Move port_allocator to staging so that it can be reused by other projects . Which issue(s ) this PR fixes : Fixes #82590 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	2	1
1 1 0.6 1.0 1.1 1.0 0.75 1.0 1.0 1.0 0.625 8 1.0 9 16 65 292	Automated cherry pick of #71522 : Use Node-Problem-Detector v 0.6.0 . Cherry pick of #71522 on release- 1.10 . 71522 : Use Node-Problem-Detector v 0.6.0	1	1
1 1 1.6 2.0 1.5 1.5 1.4 1.0 1.3333333333333333 1.0 0.0 0 0.0 5 7 15 54	azure : waiting for cluster initialization . Following the instructions on < URL > cluster/ kube-up.sh creates the VMs but hangs forever at ' waiting for cluster initialization ' . I can see the master and minion VMs are all up and running , but it looks like kubernetes cannot reach the APIs . Is this a known issue ? What's the best way to investigate ?   	0	2
2 2 1.4 1.0 1.4 1.5 1.3 1.0 1.3333333333333333 1.0 1.0 2 1.0 1 4 12 69	NewNvidiaGPUManager should not depend directly on Docker libdocker . Interface . < URL > : // The interface which could get GPU mapping from all the containers . // TODO : Should make this independent of Docker in the future . dockerClient libdocker . Interface . Related to < URL >	1	0
1 1 1.0 1.0 1.0 1.0 0.8 1.0 0.6666666666666666 1.0 0.7142857142857143 28 0.5 5 9 13 44	Automated cherry pick of #57805 : Avoid error on closed pipe . Cherry pick of #57805 on release- 1.8 . 57805 : Avoid error on closed pipe : NONE .	1	0
1 1 1.4 1.0 1.4 1.0 1.15 1.0 1.3333333333333333 1.0 0.8780487804878049 41 1.0 4 11 28 171	Scheduler preemption logic as a PostFilter plugin . What type of PR is this ? /kind feature /sig scheduling /priority important-soon What this PR does / why we need it : [ x ] Implement preemption logic as a PostFilter plugin [ x ] New unit tests and old tests have been migrated . Which issue(s ) this PR fixes : Part of #91038 . Special notes for your reviewer : This PR doesn't enable the : defaultpreemption . plugin in the framework registry . Will remove the old hard-coded preemption logic and enable this plugin in a followup , as well as migrating current UTs and add new integration tests . Does this PR introduce a user-facing change ? : : NONE .	1	1
2 2 1.8 2.0 1.8 2.0 1.7 2.0 2.0 2.0 0.0 0 0.0 16 23 82 287	Fixed typo . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind documentation What this PR does / why we need it : Make comment clear Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : NONE : NONE . 	2	2
1 1 1.0 1.0 0.9 1.0 0.95 1.0 0.6666666666666666 1.0 0.9315068493150684 73 1.0 14 21 55 235	 Add single-item list/watch to delegated authentication reader role . cherrypick of #85375 What type of PR is this ? /kind bug What this PR does / why we need it : Adds single-item list/watch permission to the delegated authentication reader role Which issue(s ) this PR fixes : Fixes #85374 < URL > switched to a list/watch , but did not add required permissions to the authentication reader role . Does this PR introduce a user-facing change ? : : NONE . /sig auth /assign @liggitt	0	0
1 1 0.8 1.0 0.6 1.0 0.75 1.0 0.6666666666666666 1.0 1.25 8 1.0 12 23 59 259	Automated cherry pick of #83036 : Use ipv4 in wincat port forward . Cherry pick of #83036 on release- 1.16 . 83036 : Use ipv4 in wincat port forward For details on the cherry pick process , see the < URL > page .	1	0
0 0 1.0 1.0 1.1 1.0 1.05 1.0 1.0 1.0 1.3333333333333333 3 1.0 18 27 101 286	Fix Windows to read VM UUIDs from serial numbers . What type of PR is this ? /kind bug What this PR does / why we need it : Fixes the Windows implementation of retrieving the VM's UUID . Some versions of vSphere do not report the value correctly from : wmic csproduct get UUID . . This is the same problem with : /sys/class/dmi/id/product_uuid . that was addressed in #59519 . Which issue(s ) this PR fixes : Fixes #74888 Does this PR introduce a user-facing change ? : Assuming we get it into the same release as #71147 this doesn't need a new release note I think . : NONE .	1	0
0 0 0.2 0.0 0.1 0.0 0.3 0.0 0.0 0.0 1.0 2 1.0 2 23 100 271	 Automated cherry pick of #74336 : cri_stats_provider : overload nil as 0 for exited containers . Cherry pick of #74336 on release- 1.13 . 74336 : cri_stats_provider : overload nil as 0 for exited containers	0	0
1 1 0.8 1.0 0.7 1.0 0.7 1.0 1.0 1.0 0.9166666666666666 96 1.0 13 32 68 266	Add e2e test for Lease API .	2	1
0 0 1.0 1.0 1.2 1.0 1.3 1.0 1.0 1.0 1.6 5 2.0 13 38 66 327	Ephemeral Containers : Automatically taint pods with ephemeral containers . What would you like to be added : Automatically adding an annotation to pods in which an ephemeral container has been added . Why is this needed : A common usage pattern for ephemeral containers is to perform some debugging action in a pod that changes its state in some fashion . Since the pod is no longer pristine , it should be marked as such so it could automatically be deleted .	2	1
2 2 1.2 1.0 1.2 1.0 1.15 1.0 1.0 1.0 1.5714285714285714 7 2.0 6 19 25 55	  New alpha fields in 1.8 are not cleared in API requests . Several alpha fields introduced during the 1.8 dev cycle are not being dropped from incoming requests when the feature gates are disabled . examples : podSpec . Volumes[i ] . EmptyDir . SizeLimit podSpec . Priority podSpec . PriorityClassName node . Spec . ConfigSource This has to be rectified prior to release to avoid compatibility issues with future clients cc @kubernetes /sig-api-machinery-bugs	0	0
2 2 1.8 2.0 1.7 2.0 1.55 2.0 2.0 2.0 1.8823529411764706 17 2.0 2 3 10 99	Inadequate monitoring documentation . Issues : - the documentation we have in cluster/addons/cluster-monitoring/ README.md is terse . - it should describe what metrics are monitored , and link to and from docs about limits , such as docs/ resources.md - it is not under the docs directory where we keep user docs . Some kind of overview should be in : docs . . - it is described as an addon but some form of cluster monitoring it seems to be enabled by default on GCE , AWS , GKE , Azure , vagrant , rackspace , ubuntu , etc . 	1	2
1 1 1.4 1.0 1.4 1.0 1.5 1.5 1.3333333333333333 1.0 1.3333333333333333 3 1.0 5 22 46 322	Add check-conformance-test-requirements . go . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind bug What this PR does / why we need it : We have defined requirements of conformance test as < URL > , and this adds coding check for one requirement ' it works for all providers ' . If a conformance test contains SkipIfProviderIs () like : $ git diff diff -- git a/test/e2e/kubectl/kubectl . go b/test/e2e/kubectl/kubectl . go index 3f197b2b64 .. 0a1c562339 100644 --- a/test/e2e/kubectl/kubectl . go +++ b/test/e2e/kubectl/kubectl . go @@ -274 , 7 +274 , 7 @@ var _ = SIGDescribe('Kubectl client ' , func () { */ framework . ConformanceIt('should create and stop a replication controller ' , func () { defer cleanupKubectlInputs(nautilus , ns , updateDemoSelector ) - + framework . SkipIfProviderIs('aws ' ) ginkgo . By('creating a replication controller ' ) framework . RunKubectlOrDieInput(nautilus , ' create ' , ' -f ' , ' -' , fmt . Sprintf('--namespace=%v ' , ns )) framework . ValidateController(c , nautilusImage , 2 , ' update-demo ' , updateDemoSelector , getUDData('nautilus . jpg ' , ns ) , ns ) . this script detects like : test/e2e/kubectl/kubectl . go : Conformance test should not call SkipIfProviderIs()/SkipUnlessProviderIs ()  : We need to fix the above errors . exit status 1 . Which issue(s ) this PR fixes : Fixes #74432 Does this PR introduce a user-facing change ? : : NONE .	2	0
2 2 1.0 1.0 0.8 1.0 0.75 1.0 1.3333333333333333 1.0 0.0 0 0.0 11 20 32 96	Automated cherry pick of #55782 . Cherry pick of #55782 on release- 1.8 . 55782 : Bump addon manager version used to 6.5	1	1
1 1 1.8 2.0 1.8 2.0 1.55 2.0 1.6666666666666667 2.0 0.45161290322580644 31 0.0 4 37 64 291	Create /var/lib/etcd with 0700 . If we let the hostpath with DirectoryOrCreate to create this directory it defaults to 0755 . A default install should use 0700 for better security especially if the directory is not present . Change-Id : Idc0266685895767b0d1c5710c8a4fb704805652f What type of PR is this ? /kind bug What this PR does / why we need it : Which issue(s ) this PR fixes : Fixes < URL > Special notes for your reviewer : Does this PR introduce a user-facing change ? : : kubeadm : Create /var/lib/etcd with correct permissions ( 0700 ) by default . .	2	0
2 2 0.8 1.0 0.8 1.0 1.0 1.0 1.0 1.0 2.0 2 2.0 19 43 83 292	Token request : proactively remove unused secret based SA tokens . Once it is possible to opt out of secret based SA tokens , any preexisting secrets should be removed . Action items : Clean up legacy tokens for the controller service accounts after a release /sig auth /sig api-machinery @kubernetes /sig-auth-feature-requests xref : #70679 #71275 #72179 #77599 /priority important-longterm	2	1
0 0 1.0 1.0 0.9 1.0 0.8 1.0 1.0 1.0 0.6153846153846154 13 1.0 14 19 56 268	Fix race condition between actual and desired state in kublet volume manager . This PR fixes the issue #75345 . This fix modified the checking volume in actual state when validating whether volume can be removed from desired state or not . Only if volume status is already mounted in actual state , it can be removed from desired state . For the case of mounting fails always , it can still work because the check also validate whether pod still exist in pod manager . In case of mount fails , pod should be able to removed from pod manager so that volume can also be removed from desired state . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : .	1	0
1 1 0.6 1.0 0.8 1.0 0.7 1.0 0.6666666666666666 1.0 0.25 4 0.0 3 12 24 159	Automated cherry pick of #93034 : Skip ensuring VMSS in pool for nodes which should be excluded . Cherry pick of #93034 on release- 1.17 . 93034 : Skip ensuring VMSS in pool for nodes which should be excluded For details on the cherry pick process , see the < URL > page . incorrectly	0	0
1 1 0.4 0.0 0.9 1.0 0.85 1.0 0.6666666666666666 1.0 0.5783132530120482 83 0.0 12 15 29 117	Automated cherry pick of #63492 : Always track kubelet -> API connections . Cherry pick of #63492 on release- 1.8 . 63492 : Always track kubelet -> API connections	1	0
1 1 0.6 1.0 0.7 1.0 1.0 1.0 0.6666666666666666 1.0 0.5 2 0.5 4 22 44 283	 Remove alpha from Flex Volume docs . /kind bug Flex volume docs still stay the feature is Alpha . We made it GA in Kube 1.8 time frame . We have to update the docs to reflect it . /sig storage	0	0
