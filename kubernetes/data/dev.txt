1 1 0.8 1.0 0.6 1.0 0.35 0.0 1.0 1.0 1.6666666666666667 3 2.0 40 58 85 283	Automated cherry pick of #83027 : Update go mod hcsshim version to fix the kube-proxy issue cannot acce 鈥?. Cherry pick of #83027 on release- 1.15 . 83027 : Update go mod hcsshim version to fix the kube-proxy issue cannot access service by self nodeip : port on windows What type of PR is this ? /kind bug What this PR does / why we need it : fix the kube-proxy issue cannot access service by self nodeip : port on windows Which issue(s ) this PR fixes : Fixes #79515 Does this PR introduce a user-facing change ? : : Fixes kube-proxy bug accessing self nodeip : port on windows . /sig network windows @feiskyer @liggitt @BenTheElder @PatrickLang @dims	1	0
2 2 1.6 2.0 1.4 1.0 1.2 1.0 2.0 2.0 1.4285714285714286 14 1.0 4 18 59 243	move nodepreferavoidpods to score plugin . What type of PR is this ? /kind feature /sig scheduling /priority important-soon /release-note-none Which issue(s ) this PR fixes : Fixes #86407	1	1
0 0 0.4 0.0 0.5 0.5 0.55 1.0 0.0 0.0 0.42857142857142855 7 0.0 9 24 51 298	[ 1.10 ] Automated cherry pick of #61373 : Use inner volume name instead of outer volume name for subpath directory . Cherry pick of #61373 on release- 1.10 . 61373 : Use inner volume name instead of outer volume name for subpath directory Release note : : ACTION REQUIRED : In-place node upgrades to this release from versions 1.7.14 , 1.8.9 , and 1.9.4 are not supported if using subpath volumes with PVCs . Such pods should be drained from the node first . . incorrectly	0	0
0 0 1.0 1.0 0.8 1.0 0.55 0.0 0.6666666666666666 0.0 0.0 0 0.0 8 16 27 177	migrate kubelet -- bootstrap-kubeconfig to kubelet.config.k8s.io . What this PR does / why we need it : migrate kubelet -- boot-kubeconfig to kubelet.config.k8s.io Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes #61680 Special notes for your reviewer : @mtaufen Release note : : The Kubelet's -- bootstrap-kubeconfig flag can now be set via the kubelet.config.k8s.io/v1beta1 API by specifying the KubeletConfiguration . BootstrapKubeconfig field . .	1	1
2 2 2.0 2.0 1.8 2.0 1.5 2.0 2.0 2.0 1.2307692307692308 13 1.0 5 16 34 91	Add troubleshooting info for viewing resource usage in cAdvisor , etc . . 	2	2
2 2 1.8 2.0 1.5 2.0 1.65 2.0 2.0 2.0 1.2142857142857142 14 2.0 16 31 47 253	 restore pre- 1.11 behavior of `kubectl get -- template = ... ` . Release note : : NONE . Restores old behavior to the : -- template . flag in : get . go . . In old releases , providing a : -- template . flag value and no : -- output . value implicitly assigned a default value (' go-template ' ) to : -- output . , printing using the provided template argument . Example : : # this should print using GoTemplate printer , but currently does not $ kubectl get pod foo -- template='{{ . metadata.name }}' . cc @deads2k @soltysh	0	0
1 1 0.8 1.0 1.0 1.0 0.95 1.0 1.0 1.0 0.5 2 0.5 22 57 79 259	  Add wildcard tolerations to kube-proxy . Add wildcard tolerations to kube-proxy . Add : nvidia.com/gpu . toleration to nvidia-gpu-device-plugin . Related to #55080 and #44445 . /kind bug /priority critical-urgent /sig scheduling Release note : : kube-proxy addon tolerates all NoExecute and NoSchedule taints by default . . /assign @davidopp @bsalamat @vishh @jiayingz	0	0
0 0 1.0 1.0 1.4 2.0 1.35 2.0 0.6666666666666666 0.0 0.625 8 1.0 39 57 84 283	need a test of NetworkPolicy with IPBlock . Except . : test/e2e/network/network_policy . go . needs a test to ensure that : ipBlock . except . clauses are implemented correctly . Specifically , that they are not implemented as ' deny ' rules . eg , given a client pod with IP : A . B . C . D . , if you add a NetworkPolicy with : : spec : ingress : - from : - ipBlock : cidr : 0.0.0.0 /0 except : - A . B . C . 0/24 . then it should block ingress from that pod . But if you then add a second policy with : : spec : ingress : - from : - ipBlock : cidr : A . B . C . D/32 . then it should reallow ingress from that pod . Then if you delete the first policy , ingress should still be allowed , and if you create the first policy again , ingress should still be allowed . ( ie , the result does not depend on which order the two policies were created in ) . /sig network /priority important-longterm	2	1
1 1 1.4 2.0 1.1 1.0 1.15 1.0 1.0 1.0 1.0 4 1.0 1 17 33 82	 kubeadm : add mandatory configuration to ' phase preflight ' . What this PR does / why we need it : Add the : - mandatory flag ' -- config ' to the preflight phase and parse the specified config file for either ' master ' or ' node ' . - flag ' -- ignore-preflight-errors ' to the preflight phase to allow skipping errors . - the function AddIgnorePreflightsFlag () to ' options/generic . go ' , because the flag is used in multiple commands . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes kubernetes/kubeadm #924 Special notes for your reviewer : this is following : < URL > Release note : : kubeadm : add mandatory ' -- config ' flag to ' kubeadm alpha phase preflight ' . @kubernetes /sig-cluster-lifecycle-pr-reviews /assign @fabriziopandini /milestone 1.12 /kind bug	0	0
1 1 1.0 1.0 1.0 1.0 0.95 1.0 1.0 1.0 1.0689655172413792 29 1.0 1 27 79 273	[ migration phase 1 ] InterPodAffinityPriority as filter plugin . What would you like to be added : Add a filter plugin that calls into InterPodAffinityPriority predicate , example #83460 Why is this needed : Part of #83554 /sig scheduling /help /priority important-soon	1	1
0 0 0.0 0.0 0.6 0.5 0.4 0.0 0.0 0.0 0.5906735751295337 193 0.0 10 29 53 174	  Automated cherry pick of #72856 : Fix nil panic propagation . Cherry pick of #72856 on release- 1.13 . 72856 : Fix nil panic propagation	0	0
0 0 0.0 0.0 0.1 0.0 0.35 0.0 0.0 0.0 0.7037037037037037 27 1.0 3 20 33 122	  Setup dns servers and search domains for Windows Pods . What this PR does / why we need it : Kubelet is depending on docker container's ResolvConfPath ( e.g. /var/lib/docker/containers/439efe31d70fc17485fb6810730679404bb5a6d721b10035c3784157966c7e17/resolv . conf ) to setup dns servers and search domains . While this is ok for Linux containers , ResolvConfPath is always an empty string for windows containers . So that the DNS setting for windows containers is always not set . This PR setups DNS for Windows sandboxes . In this way , Windows Pods could also use kubernetes dns policies . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes #61579 Special notes for your reviewer : Requires Docker EE version > = 17.10.0 . Release note : : Setup dns servers and search domains for Windows Pods in dockershim . Docker EE version > = 17.10.0 is required for propagating DNS to containers . . /cc @PatrickLang @taylorb -microsoft @michmike @JiangtianLi	0	0
0 0 1.2 1.0 1.4 1.5 1.3 1.0 1.0 1.0 0.0 0 0.0 0 0 5 25	The CRI dockershim doesn't support ClusterFirstWithHostNet . Kubernetes version ( use : kubectl version . ): HEAD ( ~ v 1.6.0 -beta . 4 ) Environment : any What happened : I want to use ClusterFirstWithHostNet which was implemented in < URL > but only dockertools ( legacy/non-CRI ) was updated to support this feature . The CRI dockershim should be modified to support this rewriting the resolv . conf file of Pods with HostNetwork = true as well . This is a bug in v 1.6 , I hope it can be prioritized to be fixed in v 1.6.1 , since it's blocking me from using CRI . Since CRI is enabled by default , it's expected that it should have feature parity with the earlier implementation . @kubernetes /sig-node-bugs @freehan @yujuhong @feiskyer @vefimova @bowei @thockin	1	0
2 2 1.6 2.0 1.7 2.0 1.55 2.0 1.6666666666666667 2.0 0.53125 128 0.0 2 4 9 67	Add ability to include intermediates in CSR-issued certificates . Currently , if an intermediate CA is used to sign certs , the csr signing controller does not include the intermediate chain in the CSR status . This means that anything requesting a cert via the CSR API has to know the intermediates themselves ( unlikely ) or anything interacting with a component presenting or serving using an intermediate-issued cert must know about the intermediate issuer . Ideally , we would have a way to : 1 . indicate to the signing controller an optional intermediate bundle to include 2 . include intermediates in the CSR status ( appending to the existing : Certificate . field , having a separate : Intermediates [] byte . field , etc ) /kind feature /sig auth /cc @mikedanese	2	1
1 1 0.2 0.0 0.4 0.0 0.4 0.0 0.3333333333333333 0.0 0.65625 32 0.0 6 7 16 44	Automated cherry pick of #57340 : Fix garbage collector when leader-elect = false . Cherry pick of #57340 #58306 on release- 1.9 . 57340 : Fix garbage collector when leader-elect = false 58306 : Track run status explicitly rather than non-nil check on : Fix garbage collection and resource quota when the controller-manager uses -- leader-elect = false .	1	0
1 1 1.0 1.0 1.0 1.0 1.2 1.0 1.0 1.0 0.660377358490566 53 1.0 15 41 61 281	Add Permit extension point for the scheduling framework . What would you like to be added : The < URL > is now implementable . We have < URL > plugin interfaces and extension points for ' < URL > ' and ' < URL > ' plugins . ' < URL > ' is an important extension point that enables building advanced scheduling features , such as gang scheduling ( AKA co-scheduling in K8s world ) . As a part of this effort , a new interface for permit plugins should be added . The main function of the plugin should return a ' < URL > ' and a timeout . We also need to build the record keeping for pods in ' wait ' state and add a clean-up mechanism for pods whose timeout has expired . /sig scheduling /priority important-soon ref/ kubernetes/enhancements #624	1	1
2 2 1.6 2.0 1.7 2.0 1.75 2.0 1.6666666666666667 2.0 0.0 0 0.0 7 30 57 257	Sort Field for ListOptions . I would like to have the ability to sort objects server side by a particular field on the object being returned when issuing a get request that returns a list of objects . I am wondering about the impact of potentially adding a : Sort . field to < URL > so that a sort field can be specified as part of a get request . If there is already an option available , I would like to know where I can find an example . What would you like to be added : An example of how to return a list of objects in a sorted order using a go-client or the ability through ListOptions to specify a sort field . Why is this needed : An example of where I would like to use such a field can be found as part of the < URL > project . The idea here would be to sort : pipelineruns . by their start times and then returned to the requestor .	2	1
0 0 0.8 1.0 1.1 1.0 1.2 1.0 0.6666666666666666 1.0 0.5882352941176471 17 1.0 19 38 67 252	Create-update-delete-deployment example using dynamic package . What type of PR is this ? /kind documentation What this PR does / why we need it : This PR ads documentation/example showing the use of dynamic package in client-go repo . Which issue(s ) this PR fixes : Fixes #76512 : NONE . 	1	2
1 1 0.6 1.0 0.7 1.0 0.7 1.0 0.6666666666666666 1.0 0.725 40 1.0 20 47 76 310	Move CSI volume expansion to beta . Move CSI volume expansion to beta . xref - < URL > fixes < URL > /sig storage : Move CSI volume expansion to beta .	1	1
1 1 1.2 1.0 1.4 1.5 1.25 1.0 1.6666666666666667 2.0 0.0 0 0.0 3 15 53 223	fix too many pdb update operations when nothing change . What type of PR is this ? /kind bug What this PR does / why we need it : There are too many pdb update operations even if no changes take place . The PR fix it . Which issue(s ) this PR fixes : Fixes #74240 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE .	1	0
1 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9347826086956522 46 1.0 24 26 58 296	Migrate Kubelet -- experimental-kernel-memcg-notification to kubelet.config.k8s.io or remove the flag . Flag name : : experimental-kernel-memcg-notification . Help text : If enabled , the kubelet will integrate with the kernel memcg notification to determine if memory eviction thresholds are crossed rather than polling . This is part of migrating the Kubelet command-line to a Kubernetes-style API . The : -- experimental-kernel-memcg-notification . flag should either be migrated to the Kubelet's : kubelet.config.k8s.io . API group , or simply removed from the Kubelet . If this could be considered an instance-specific flag , or a descriptor of local topology managed by the Kubelet , see : < URL > If this flag is only registered in os-specific builds , see : < URL > As : -- experimental-kernel-memcg-notification . is an alpha/experimental flag , the feature it configures must either be feature-gated , or graduated from alpha/experimental status prior to the migration . @sig -node-pr-reviews @sig -node-api-reviews /assign @mtaufen /sig node /kind feature /priority important-soon /milestone v 1.11 /status approved-for-milestone	1	1
0 0 0.2 0.0 0.6 1.0 0.7 1.0 0.0 0.0 0.5 6 0.5 11 26 52 221	  Automated cherry pick of #85027 : Fix bug about unintentional scale out during updating . Cherry pick of #85027 on release- 1.17 . 85027 : Fix bug about unintentional scale out during updating For details on the cherry pick process , see the < URL > page .	0	0
1 1 1.4 1.0 1.5 1.5 1.5 1.5 1.3333333333333333 1.0 0.0 0 0.0 2 4 41 281	correction of executable path doc . correct executable path by remove : /go . What type of PR is this ? /kind documentation What this PR does / why we need it : make the doc exactly 	2	2
2 2 1.2 1.0 1.1 1.0 0.85 1.0 1.3333333333333333 2.0 0.5 8 0.5 7 20 51 236	 Automated cherry pick of #65454 : Update Rescheduler's manifest . Cherry pick of #65454 on release- 1.11 . 65454 : Update Rescheduler's manifest	0	1
1 1 1.0 1.0 1.1 1.0 1.15 1.0 1.0 1.0 0.0 0 0.0 8 14 65 240	Kubelet changes for Windows GMSA support . What type of PR is this ? /kind feature What this PR does / why we need it : This patch comprises the kubelet changes outlined in the GMSA KEP ( < URL > to add GMSA support to Windows workloads . More precisely , it includes the logic proposed in the KEP to resolve which GMSA spec should be applied to which containers , and changes : dockershim . to copy the relevant GMSA credential specs to Windows registry values prior to creating the container , passing them down to docker itself , and finally removing the values from the registry afterwards ; both these changes need to be activated with the : WindowsGMSA . feature gate . Includes unit tests . Which issue(s ) this PR fixes : KEP at < URL > Special notes for your reviewer : Do Windows unit tests run as part of a regular build ? If not I'll need to change this PR slightly to still run the new tests on Linux . Does this PR introduce a user-facing change ? : : Allow the kubelet to pass Windows GMSA credentials down to Docker .	2	1
1 1 0.8 1.0 1.0 1.0 1.15 1.0 0.6666666666666666 1.0 1.1176470588235294 17 1.0 13 19 58 274	Implemented taints and tolerations priority function as a Score plugin . What type of PR is this ? /kind feature What this PR does / why we need it : Implements taints and tolerations priority as a Score Plugin . Which issue(s ) this PR fixes : Fixes #83531 Does this PR introduce a user-facing change ? : : NONE .	1	1
1 1 0.8 1.0 0.9 1.0 0.95 1.0 0.6666666666666666 1.0 0.9333333333333333 30 1.0 7 9 31 168	fix : determine the correct ip config based on ip family . What type of PR is this ? /kind bug What this PR does / why we need it : Determines the correct IP config based on clusterIP family before updating backend pool for VMSS Which issue(s ) this PR fixes : Fixes < URL > Special notes for your reviewer : Does this PR introduce a user-facing change ? : : fix : determine the correct ip config based on ip family . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : NONE . /area provider/azure /priority important-soon /assign @feiskyer @khenidak	1	0
1 1 0.8 1.0 0.8 1.0 1.1 1.0 0.6666666666666666 1.0 1.0108695652173914 92 1.0 17 38 53 147	  Invoke metrics . Register as early as possible in the scheduler initialization . What type of PR is this ? /kind bug What this PR does / why we need it : < URL > switched the scheduler from Prometheus to k8s wrapper in 1.16 , but didn't fix the registration . This caused the scheduler's : pending_pods . metric to break : this metric is reported by the scheduler queue . At the time of instantiating the queue , we obtain a reference to the : pending_pods . metric , but this happens before the metric is registered , which means the reference that the queue obtained is a noop metric . Which issue(s ) this PR fixes : Part of #87690 Special notes for your reviewer : I am sending a separate PR to fix the registration order to make it easy to backport to 1.16 Does this PR introduce a user-facing change ? : : scheduler's pending_pods metric to be reported back . . /cc @liu -cong	0	0
1 1 1.4 1.0 1.5 1.5 1.35 1.0 1.3333333333333333 1.0 1.0 4 1.0 14 33 47 259	bandwidth : handle new ' chain X ' output from ' tc filter show dev XYZ ' . tc started adding ' chain 0 ' into the output of ' tc filter show dev eth0 ' as of iproute2 commit 732f03461bc48cf94946ee3cc92ab5832862b989 and that confuses the bandwidth code . Related : < URL > /kind bug : NONE . @cadmuxe	1	0
1 1 1.8 2.0 1.9 2.0 1.8 2.0 1.6666666666666667 2.0 0.7241379310344828 29 1.0 3 3 13 42	 Recheck if transformed data is stale when doing live lookup during update . Fixes #49565 Caching storage can pass in a cached object to : GuaranteedUpdate . as a hint for the current object . If the hint is identical to the data we want to persist , before short-circuiting as a n o-o p update , we force a live lookup . We should check two things on the result of that live lookup before short-circuiting as a n o-o p update : 1 . the bytes we want to persist still match the transformed bytes read from etcd 2 . the state read from etcd didn't report itself as stale . this would mean the transformer used to read the data would not be the transformer used to write it , and ' n o-o p ' writes should still be performed , since transformation will make the underlying content actually different . After a live lookup , we checked byte equality , but not the stale indicator . This meant that key rotation or encrypted -> decrypted , and decrypted -> encrypted updates are broken . Introduced in #54780 and picked back to 1.8 in #55294 : Fixed encryption key and encryption provider rotation .	0	0
1 1 1.0 1.0 1.3 1.0 1.1 1.0 1.0 1.0 0.717948717948718 39 1.0 9 12 55 251	Add migration shim for verifyvolumeattachment and bulk verify . : VerifyVolumesAreAttached . and : BulkVolumeVerify . were not shimmed to CSI when migration was enabled for the verification plugin . This implements the shim layer for those functions . However , : VerifyVolumesAreAttached . for CSI is broken since it checks the : VolumeAttachment . object which is not currently necessarily representative of what's attached in the backend ( if something was detached out of band VolumeAttachment wont see that ) . and : BulkVolumeVerify . is not implemented for CSI . Those are separate issues that can be worked on after this PR . /kind feature /sig storage /assign @msau42 @ddebroy @leakingtapan @saad -ali /cc @gnufied @jsafrane @andrewsykim @adisky @andyzhangx : Add CSI Migration Shim for VerifyVolumesAreAttached and BulkVolumeVerify .	1	1
1 1 0.8 1.0 0.9 1.0 0.9 1.0 0.6666666666666666 1.0 0.7714285714285715 70 1.0 10 26 59 280	[ Windows ] Upload containerd logs to stackdriver . Log containerd output to a log file : containerd . log . , and upload the log to stackdriver . Note that we are using the : container-runtime . tag , which matches what we are using on linux < URL > I've validated the change in my cluster , and the log can be successfully uploaded : /cc @kubernetes /sig-windows-misc @yliaog @pjh Signed-off-by : Lantao Liu < URL > : none .	1	1
0 0 0.4 0.0 0.6 1.0 0.9 1.0 0.3333333333333333 0.0 0.5755813953488372 172 0.0 7 17 52 244	Investigate adding per-request binding or assertions to apiserver proxy . Investigate adding per-request binding or assertions that would let backends verify a particular request was intentionally forwarded from the apiserver proxy From < URL > < URL > /area security /area apiserver /priority important-longterm /kind cleanup /sig api-machinery	2	1
0 0 0.0 0.0 0.2 0.0 0.5 0.5 0.0 0.0 1.0 4 1.0 1 21 53 198	Cherry pick of #85689 : Export scheduler . Snapshot function . This was added as a workaround for Cluster Autoscaler , but didn't make the cut for release- 1.17 . Now we want to sync other cherry-pick fixes from release- 1.17 , but we can't do that without this commit on the branch . /kind cleanup incorrectly	0	0
0 0 0.8 1.0 1.0 1.0 0.95 1.0 0.6666666666666666 0.0 0.0 1 0.0 0 2 10 17	[ e2e ] Namespaces [ Serial ] should ensure that all pods are removed when a namespace is deleted . . Test is constantly failing . < URL > < URL > : /go/src/ k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/namespace.go:265 Expected error : < * errors . errorString | 0xc421a2cc80 > : { s: ' an empty namespace may not be set when a resource name is provided ' , } an empty namespace may not be set when a resource name is provided not to have occurred /go/src/ k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/namespace.go:95 . < URL > /cc @kubernetes /sig-api-machinery-bugs @kubernetes /kubernetes-release-managers @dchen1107 incorrectly	0	0
1 1 1.0 1.0 0.7 1.0 0.6 1.0 1.0 1.0 0.5714285714285714 7 1.0 5 17 23 52	Revert ' CPU manager wiring and `none` policy ' . Reverts kubernetes/kubernetes #51357 Seems likely this should fix < URL > cc @ConnorDoyle @derekwaynecarr incorrectly	0	0
2 2 2.0 2.0 2.0 2.0 1.8 2.0 2.0 2.0 0.0 0 0.0 3 6 19 61	Kubelet doc . go . I think we should write doc . go files for packages in Kubelet , like we do for the rest of Kubernetes . In some cases , it's not obvious what the code in a package does ( for example , without spending some time reading the code , it's not clear what kind of ' container ' files are in kubelet/container - some ' container ' source files are outside the folder ) . In some cases , I think the doc . go files need to be expanded upon ( kubelet/types/types . go says ' Common types in the Kubelet . ' , but there's also a kubelet/types . go file ) . I think having clear doc files will help structure the code better - contributors will know how they should organize their code when introducing a feature . 	2	2
1 1 1.2 1.0 1.3 1.0 1.15 1.0 1.3333333333333333 1.0 1.6 15 2.0 3 28 54 309	Use no-priority best-effort pod as the preemptor in BenchmarkGetPodsToPreempt . What type of PR is this ? /kind bug What this PR does / why we need it : I see the following when running BenchmarkGetPodsToPreempt : : panic : runtime error : invalid memory address or nil pointer dereference [ signal SIGSEGV : segmentation violation code = 0x1 addr = 0x280 pc = 0x1d2f91a ] goroutine 10 [ running ]: k8s.io/kubernetes/pkg/kubelet/types.IsCriticalPod(0x0 , 0x1019ace ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/types/pod_update.go:150 +0x5a k8s.io/kubernetes/pkg/kubelet/types.Preemptable(0x0 , 0xc000158380 , 0x20b1d69 ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/types/pod_update.go:165 +0x2f k8s.io/kubernetes/pkg/kubelet/preemption.sortPodsByQOS(0x0 , 0xc0000a0400 , 0x6e , 0x80 , 0x0 , 0x203000 , 0x203000 , 0xc0000a0400 , 0x11c4d5a , 0x6e , ... ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/preemption/preemption.go:231 +0xdf k8s.io/kubernetes/pkg/kubelet/preemption.getPodsToPreempt(0x0 , 0xc0000a0400 , 0x6e , 0x80 , 0xc00007cf48 , 0x1 , 0x1 , 0x80 , 0x1d045458 , 0x1d04545800000000 , ... ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/preemption/preemption.go:119 +0x6a k8s.io/kubernetes/pkg/kubelet/preemption.BenchmarkGetPodsToPreempt(0xc00013e1a0 ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/preemption/preemption_test.go:153 +0x1b2 . This PR fixes the panic by using no-priority best-effort pod as the preemptor in BenchmarkGetPodsToPreempt . : NONE .	2	0
0 0 0.8 0.0 1.1 1.5 1.0 1.0 0.0 0.0 0.0 0 0.0 8 14 42 225	 kubectl : error logging as string instead of [] byte . Changes the error output when duplicate keys found while creating : secret . or : configMap . ( BinaryData ) Show the data of the duplicated secret as : string . instead of : [] byte . Removes error output of binary data in : configMaps . closes #73969	0	0
2 2 1.2 2.0 0.9 1.0 0.75 1.0 1.3333333333333333 2.0 1.0 2 1.0 7 9 37 152	Automated cherry pick of #62464 : avoid dobule RLock () in cpumanager . Cherry pick of #62464 on release- 1.9 . 62464 : avoid dobule RLock () in cpumanager incorrectly	0	0
1 1 1.4 1.0 1.6 2.0 1.55 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 4 13 58	api returns error 415 on PATCH . PATCH < URL > {' spec ' :{ ' replicas ' : 0 }} answer : { ' kind ' : ' Status ' , ' apiVersion ' : ' v1beta3 ' , ' metadata ' : {} , ' status ' : ' Failure ' , ' message ' : ' the server responded with the status code 415 but did not return more information ' , ' details ' : {} , ' code ' : 415 } Before v 0.15.0 it had worked well .	2	0
0 0 0.0 0.0 0.2 0.0 0.2 0.0 0.0 0.0 0.24242424242424243 33 0.0 12 18 91 215	[ e2e failure ] [ sig-cluster-lifecycle ] Upgrade [ Feature : Upgrade ] cluster upgrade should maintain a functioning cluster [ Feature : ClusterUpgrade ] . /priority critical-urgent /priority failing-test /kind bug /status approved-for-milestone /area platform/gke @kubernetes /sig-cluster-lifecycle-test-failures owns the test @kubernetes /sig-gcp-test-failures this looks GKE-specific @kubernetes /sig-node-test-failures for the AppArmor cluster This test has been failing since at least 2017-11-13 for the following jobs : - < URL > - < URL > - < URL > - < URL > These jobs are on the < URL > , and prevent us from cutting v 1.9.0 -beta . 1 ( kubernetes/sig-release #34 ) . Is there work ongoing to bring this test back to green ? < URL > < URL > : ... Response : code = 400 , message = Cluster master cannot be upgrade to \\\' 1.10.0 -alpha . 0.55 +01c74145c7b655\\\' ... . < URL > : ... invalid AppArmor profile name : ' unconfined ' ... . Sample failure : < URL > incorrectly	0	0
1 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0172413793103448 58 1.0 6 26 62 284	Define algorithm providers in terms of plugins instead of predicates/priorities [ Migration Phase 2 ] . /assign @ahg -g @draveness /sig scheduling /priority important-soon Part of #85822	1	1
2 2 1.6 2.0 1.5 1.5 1.25 1.0 2.0 2.0 0.6590909090909091 44 1.0 6 9 13 49	Failed , Tainting nodes by conditions seems to visibly slow down cluster startup . After < URL > has been merged , our large tests started behaving a bit strange . In particular , when 5k node cluster was reported as up Failed , ( with all nodes being ready ) , the test itself was only discovering 4k or even less nodes as schedulable . Example is here : < URL > There were a couple issues that were fixed in the meantime by @mborsz mostly he fixed validation : < URL > That said , that issue clearly shows that tainting nodes by conditions visibly slows down the startup , because now , once the condition are reported as ' healthy ' the node has to be processed by nodelifecycle controller to remove the corresponding taints . I didn't yet take a look if the problem here is its throughput or is it related to the fact that there are so many nodes to process at once , but given my experience in the past , I suspect that it may be serialized processing with reporting that in a single thread ... @k82cn @bsalamat @kubernetes /sig-scalability-bugs @shyamjvs @mborsz incorrectly	0	0
2 2 1.8 2.0 1.7 2.0 1.45 1.5 2.0 2.0 0.8648648648648649 37 1.0 14 55 95 334	for aggregated apiserver availability , try multiple endpoints in parallel . Marking an aggregated API server as unavailable is a big deal . It affects the availability of an entire API group version . Right now , if a single endpoint is having difficulty ( maybe it started failing or its node network went down or the proxy has latent entries or any number of things ) , the entire apiservice is removed from rotation . Instead of doing that on a single bad request , we can check multiple endpoints in parallel . If any one of them succeeds , we can consider the apiservice as available . Individual requests to a ' bad ' endpoint may still fail , but the API group version remains available overall . /kind bug /priority important-soon @kubernetes /sig-api-machinery-bugs : NONE .	1	0
1 1 0.8 1.0 1.0 1.0 1.4 2.0 0.6666666666666666 1.0 0.0 0 0.0 4 15 59 260	HPA incorrectly reported condition status . What type of PR is this ? /kind bug What this PR does / why we need it : This PR fixes HPA incorrectly reported condition status . Example : Consider a case where : desiredReplicas = 1 minimumAllowedReplicas = 2 hpaMinReplicas = 2 . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : NA Does this PR introduce a user-facing change ? : : NONE .	2	0
1 1 0.4 0.0 0.2 0.0 0.4 0.0 0.6666666666666666 1.0 1.0 3 1.0 22 38 63 245	Automated cherry pick of #86412 : It fixes a bug where AAD token obtained by kubectl is . Cherry pick of #86412 on release- 1.15 . 86412 : It fixes a bug where AAD token obtained by kubectl is For details on the cherry pick process , see the < URL > page .	1	0
0 0 0.6 0.0 0.6 0.0 0.55 0.0 1.0 1.0 1.0 2 1.0 9 33 66 166	  fix bug excludeCIDRs was not assign in func NewProxier . What this PR does / why we need it : fix bug excludeCIDRs was not assign in func NewProxier Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes # Special notes for your reviewer : Release note : : NONE .	0	0
2 2 2.0 2.0 1.5 2.0 1.45 2.0 2.0 2.0 1.6097560975609757 41 2.0 1 16 33 105	Generalize prevention of accidental deletion / mutation . PR #9975 added hardcoded protection of the default namespace to the Lifecycle admission controller . This mechanism at least needs to be configurable , since Openshift and Kubernetes have additional infrastructure namespaces they'd like to protect . However , it also needs to be possible to remove the protection to shut down a cluster #4630 , without making it susceptible to accidents . One solution would be to add a : protected . field to metadata of any object . : protected : true . would prevent deletion until the object was updated to set : protected : false . . This would also make it straightforward to protect the special Kubernetes services , addons , and other self-hosted components , while still making it possible to update them and to delete all resources during shutdown . cc @derekwaynecarr @lavalamp	1	1
2 2 1.4 1.0 1.3 1.0 1.25 1.0 1.6666666666666667 2.0 1.5 4 2.0 7 21 47 259	Update CHANGELOG-1.15.md . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : - Move KubeletPodResources to the Beta section . - Move NonPreemptingPriority to the Alpha section . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . 	2	2
2 2 1.6 2.0 1.4 1.0 1.2 1.0 1.6666666666666667 2.0 1.5 4 1.5 10 25 60 318	Revert ' kube-proxy : check KUBE-MARK-DROP ' . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespace from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : This reverts commit 1ca0ffeaf2c4401549b82f549f7481313308d4b9 . kube-proxy is not recreating the rules associated to the KUBE-MARK-DROP chain , that is created by the kubelet . Is preferrable to avoid the dependency between the kubelet and kube-proxy , so each of them handles their own rules . Which issue(s ) this PR fixes : Fixes #85414 Special notes for your reviewer : This is only needed for kube-proxy operating in dual-stack with iptables , and that PR wasn't merged . Also , there is a discussion about this topic with a better solution than this < URL > Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : . incorrectly	0	0
1 1 1.8 2.0 1.5 2.0 1.45 2.0 1.6666666666666667 2.0 0.6129032258064516 31 1.0 9 18 27 163	Improve error message when diff binary is not in PATH . What type of PR is this ? /kind bug What this PR does / why we need it : Which issue(s ) this PR fixes : Fixes partially #87343 Special notes for your reviewer : Examples , Before : PATH = /usr/bin/kubectl diff -f manifest . yaml error : executable file not found in $PATH KUBECTL_EXTERNAL_DIFF = unknown-binary kubectl diff -f manifest . yaml error : executable file not found in $PATH . After : PATH = /usr/bin/kubectl diff -f manifest . yaml error : failed to run ' diff ' : executable file not found in $PATH KUBECTL_EXTERNAL_DIFF = unknown-binary kubectl diff -f manifest . yaml error : failed to run ' unknown-binary ' : executable file not found in $PATH . Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	1	0
1 1 1.2 1.0 1.1 1.0 0.9 1.0 1.3333333333333333 1.0 0.0 0 0.0 5 21 69 258	The default-http-backend for handling 404 pages will now point to 404 鈥?. 鈥?handler with prometheus integration and provides metrics related to requests per second and the duration of responding to the requests for various percentile groupings . Please check < URL > for details about the 404-server-with-metrics . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : The behavior of the default handler for 404 requests fro the GCE Ingress load balancer is slightly modified in the sense that it now exports metrics using prometheus . The metrics exported include : - http_404_reques t_t otal ( the number of 404 requests handled ) - http_404_request_duration_ms ( the amount of time the server took to respond in ms ) Also includes percentile groupings . The directory for the default 404 handler includes instructions on how to enable prometheus for monitoring and setting alerts . .	1	1
2 2 1.4 1.0 1.5 1.5 1.5 1.5 1.3333333333333333 1.0 1.5483870967741935 31 2.0 5 18 87 278	Continue with remaining volumeAttached's in VerifyVolumesAreAttached . What type of PR is this ? /kind bug What this PR does / why we need it : In operationExecutor #VerifyVolumesAreAttached , : for _ , volumeAttached : = range nodeAttachedVolumes { . toward the end of the above loop , after the call to oe . VerifyVolumesAreAttachedPerNode , we break out of the loop and ignore remaining volumeAttached's This is inconsistent with the handling on line 707 of the bulk volume verification : : continue . This PR removes the break and continues with remaining volumeAttached's : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	1	0
2 2 1.4 2.0 1.6 2.0 1.6 2.0 1.3333333333333333 2.0 2.0 2 2.0 2 4 6 28	PodSecurityPolicy API is enabled but unusable by default , and doesn't have E2Es . The PodSecurityPolicy API got defaulted to on in < URL > but kube-up does not turn on the admission controller ( neither do kube-adm or GKE ) , and we don't run any tests against it . Right now , most deployments have an API object that can be created , but won't do anything ( until we decide sometime later that we actually want to enable PodSecurityPolicy ) . @bgrant0607 @pweil - @erictune	1	0
0 0 0.2 0.0 0.6 0.5 0.5 0.0 0.3333333333333333 0.0 1.0 2 1.0 19 24 74 212	 Remove event handler to satisfy alpha tests . What this PR does / why we need it : An original assumption of time out did not fix issue . The events look masked by lubelet flags so reducing test Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes #64578 /cc @msau42 Special notes for your reviewer : Release note : : NONE .	0	0
0 0 0.6 1.0 0.6 1.0 0.75 1.0 0.3333333333333333 0.0 0.6585365853658537 41 1.0 6 21 76 289	Automated cherry pick of #75792 : Updated regional PD minimum size ; changed regional PD . Cherry pick of #75792 on release- 1.14 . 75792 : Updated regional PD minimum size ; changed regional PD : NONE .	1	0
1 1 1.2 1.0 1.1 1.0 1.25 1.0 1.3333333333333333 1.0 1.0 1 1.0 8 19 99 328	SafeSysctlWhitelist : add net . ipv4 . ping_group_range ( allow ping without CAP_NET_RAW ) . What type of PR is this ? /kind feature (?) What this PR does / why we need it : Allow setting sysctl value : net . ipv4 . ping_group_range . , which can be used for allowing : ping . command without : CAP_NET_RAW . capability . e.g. : net . ipv4 . ping_group_range='0 42 ' . to allow ping for users with GID 0-GID 42 . This sysctl value was introduced in kernel 3.0 and has been namespaced since its birth . < URL > release-note : : SafeSysctlWhitelist : add net . ipv4 . ping_group_range .	1	1
1 1 0.8 1.0 1.0 1.0 1.2 1.0 1.0 1.0 1.0 2 1.0 1 16 69 256	Fix cronjob controller page list err . What type of PR is this ? /kind bug What this PR does / why we need it : When the number of jobs exceeds 500 , cronjob cannot schedule , bug of pager . List Which issue(s ) this PR fixes : Fixes # < URL > Special notes for your reviewer : Does this PR introduce a user-facing change ? : NONE : When the number of jobs exceeds 500 , cronjob should schedule without error . .	2	0
1 1 1.0 1.0 0.9 1.0 0.9 1.0 1.0 1.0 0.3888888888888889 18 0.0 9 18 59 148	 Add block volume support to internal provisioners . . What this PR does / why we need it : Internal provisioners now create filesystem PVs when block PVs are requested . This leads to unbindable PVCs . In this PR , volume plugins that support block volumes provision block PVs when block is requested . All the other provisioners return clear error in : kubectl describe pvc . : : Events : Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 7s ( x2 over 18s ) persistentvolume-controller Failed to provision volume with StorageClass ' standard ' : kubernetes.io/cinder does not support block volume provisioning . : AWS EBS , Azure Disk , GCE PD and Ceph RBD volume plugins support dynamic provisioning of raw block volumes . . cc @kubernetes /vmware for vsphere changes cc @andyzhangx for Azure changes /assign @copejon @mtanino	0	0
2 2 1.4 2.0 1.1 1.0 0.9 1.0 2.0 2.0 1.34375 32 2.0 9 16 65 268	Automated cherry pick of #76788 : Test kubectl cp escape . Cherry pick of #76788 on release- 1.13 . 76788 : Test kubectl cp escape	2	0
2 2 1.4 1.0 1.2 1.0 1.35 1.0 1.6666666666666667 2.0 0.0 0 0.0 6 37 68 265	removed extra hyphen in kubectl book . What type of PR is this ? /kind documentation What this PR does / why we need it : removed extra hyphen of : -- all-namespaces . option in kubectl book . < URL > Which issue(s ) this PR fixes : kubernetes/kubectl #729 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . 	2	2
0 0 1.2 1.0 1.1 1.0 1.15 1.0 1.0 1.0 0.9862068965517241 145 1.0 5 10 40 193	  Fix bug in reflector not recovering from ' Too large resource version'鈥?. Ref < URL > : Fix bug in reflector that couldn't recover from ' Too large resource version ' errors . /kind bug	0	0
0 0 0.6 1.0 1.2 1.0 1.0 1.0 0.6666666666666666 1.0 0.5555555555555556 45 0.0 18 23 64 223	Graduate Pod priority and preemption to GA . What type of PR is this ? /kind feature What this PR does / why we need it : Pod Priority and Preemption was graduated to Beta in 1.11 . This PR graduates the features to GA in 1.14 . Which issue(s ) this PR fixes : Fixes # kubernetes/enhancements #564 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : Graduate Pod Priority and Preemption to GA . . /sig scheduling	2	1
2 2 1.0 1.0 0.6 0.5 0.95 1.0 1.3333333333333333 1.0 2.0 4 2.0 20 59 98 280	Storage Validation : Promote multivolume test . . This PR promotes existing test to Validation Suite . A Validation test can then be promoted to a Conformance test if it is determined to be portable . /sig storage /sig testing /area conformance /kind test /release-note none	2	1
1 1 1.2 1.0 1.2 1.0 1.25 1.0 1.0 1.0 0.5714285714285714 56 1.0 21 64 102 285	Add network stats for Windows nodes and containers . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : Add network stats for Windows nodes and containers . Which issue(s ) this PR fixes : Fixes #74101 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : Add network stats for Windows nodes and containers . /sig windows /sig node /assign @yujuhong @PatrickLang @michmike	1	0
0 0 1.0 1.0 1.2 1.0 1.1 1.0 1.0 1.0 0.7555555555555555 45 1.0 4 20 31 187	Add events to PV when mount fails on filesystem mismatch . Add a event to PV when mount fails because of fs mismatch Filesystem mismatch is a special event . This could indicate either user has asked for incorrect filesystem or there is a error from which mount operation can not recover on retry . /sig storage /kind bug : Add a event to PV when filesystem on PV does not match actual filesystem on disk .	1	0
0 0 0.2 0.0 0.7 0.0 0.6 0.0 0.3333333333333333 0.0 0.5 6 0.5 7 42 83 229	 Fix output of `kubeadm migrate config` . The output should always be valid kubeadmapi . MasterConfiguration YAML . The general problem was that we printed with fmt . Fprintf but it turns out some of the default values have : % . s in them so this caused Go to think we were missing values that we wanted substituted . We don't want to do any substitution here . Signed-off-by : Chuck Ha < URL > What this PR does / why we need it : This PR fixes a small bug that cause kubeadm migrate config to print YAML that was not valid . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes kubernetes/kubeadm #904 : NONE . /cc @luxas @timothysc	0	0
1 1 1.2 1.0 1.1 1.0 1.25 1.0 1.0 1.0 2.0 3 2.0 4 14 21 63	hack/ verify-godeps.sh is failing on release- 1.1 branch . : package github.com/aws/aws-sdk-go/internal/apierr imports github.com/aws/aws-sdk-go/internal/apierr imports github.com/aws/aws-sdk-go/internal/apierr : cannot find package ' github.com/aws/aws-sdk-go/internal/apierr ' in any of : /root/ . gvm/gos/go 1.4 /src/ github.com/aws/aws-sdk-go/internal/apierr ( from $GOROOT ) /tmp/gopath . JUrP0w/src/ github.com/aws/aws-sdk-go/internal/apierr ( from $GOPATH ) godep : restore : exit status 1 package github.com/google/google-api-go-client/cloudmonitoring/v2beta2 imports github.com/google/google-api-go-client/cloudmonitoring/v2beta2 imports github.com/google/google-api-go-client/cloudmonitoring/v2beta2 : code in directory /tmp/gopath . JUrP0w/src/ github.com/google/google-api-go-client/cloudmonitoring/v2beta2 expects import ' google.golang.org/api/cloudmonitoring/v2beta2 ' godep : restore : exit status 1 !!!  in . /hack/ verify-godeps.sh:69 '' ${GODEP }' restore ' exited with status 1 Call stack : 1 : . /hack/ verify-godeps.sh:69 main (...) Exiting with status 1 .	1	0
0 0 0.4 0.0 0.6 0.5 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 0 2 16	Fixes kubectl cached discovery on Windows . Fixes < URL > The : kubectl . cached discovery makes use of : func ( f * File ) Chmod(mode FileMode ) error . which is not supported and errors out on Windows , making : kubectl get . and potentially a number of other commands to fail miserably on that platform . : os . Chmod . by file name , on the other hand , does not error out and should be used instead . Release note : : NONE . @deads2k @brendandburns @kubernetes /sig-cli-pr-reviews	1	0
1 1 1.4 1.0 1.3 1.0 1.15 1.0 1.3333333333333333 1.0 0.9322033898305084 177 1.0 3 17 34 153	add StatusConflict(409 ) as non-retriable error for disksClient . What type of PR is this ? /kind bug What this PR does / why we need it : This PR adds StatusConflict(409 ) as non-retriable error for disksClient , delete volume is triggered by k8s volume controller in a loop if it failed , no need to regard it as retriable error Already verified in azure disk CSI driver : < URL > Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : none . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : . /kind bug /assign @feiskyer /priority important-soon /sig cloud-provider /area provider/azure	1	0
2 2 1.6 2.0 1.3 2.0 1.4 2.0 2.0 2.0 0.0 0 0.0 13 25 61 277	link scripts in build/ README.md . This makes it easier to click through and learn what these do , for someone getting started browsing this developer doc . 	1	2
0 0 0.4 0.0 0.6 1.0 0.6 1.0 0.0 0.0 1.0 20 1.0 4 6 35 149	Failing Test : [ sig-testing ] ci-kubernetes-e2e-gce-scale-correctness : BeforeSuite . Failing Job < URL > Failing Test < URL > Triage results < URL > /kind bug /priority failing-test /priority important-soon /sig testing /milestone v 1.11 @kubernetes /sig-testing-bugs cc @jberkus @tpepper @shyamjvs /assign @BenTheElder for triage	1	0
1 1 0.8 1.0 0.5 0.5 0.5 0.0 0.6666666666666666 1.0 0.578125 64 0.0 17 33 74 261	 Prevent 1.9 e2es testing deprecated/removed features in 1.10 . 1.9 e2e tests get run against 1.10.0 + masters during upgrade tests . This version-gates testing deprecated features removed in 1.10 < URL > Fixes #60769 Fixes #60767	0	0
0 0 0.4 0.0 1.0 1.0 1.05 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 22 77 290	Reorder stackdriver setup in windows startup script . Stackdriver was failing to initialize correctly because it could not connect to metadata server . It could not connect to metadata server because the HNS initialization in the windows startup script , reset the network . So , move stackdriver startup block after HNS stabilizes . At the later stage of the init script , metadata server is available and stackdriver is setup correctly . What type of PR is this ? /kind bug What this PR does / why we need it : This PR fixes stackdriver logging for windows node pool . Without this the stackdriver setup fails inconsistently . Which issue(s ) this PR fixes : Special notes for your reviewer : The start up sequence of windows script is being refined . When looking for workload logs in stackdriver console , we discovered this issue . So by moving the block to after HNS setup , we improve the chances of correct stackdriver startup by many fold . Does this PR introduce a user-facing change ? : No	1	0
2 2 1.6 2.0 1.4 1.5 1.4 1.5 1.6666666666666667 2.0 1.5588235294117647 34 2.0 3 26 62 309	WithAuthentication should wrap WithMaxInFlightLimit . What type of PR is this ? /kind bug What this PR does / why we need it : This PR moves the WithMaxInFlightLimit later in insecure handler chain . Which issue(s ) this PR fixes : Fixes #81861 : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	2	0
2 2 1.6 2.0 1.6 2.0 1.45 1.5 1.6666666666666667 2.0 0.96 25 1.0 7 25 42 255	Stage or Remove the OpenStack Cloud Provider . What would you like to be added : As part of a long running effort to < URL > , we either deleted or staged the in-tree cloud providers in < URL > . OpenStack was not deleted or staged because of it's internal dep to : pkg/util/mount . . We should either continue refactoring work for : pkg/util/mount . so we can stage the open stack cloud provider as well or just remove it . Why is this needed : See KEP < URL > .	2	1
2 2 0.8 0.0 0.5 0.0 0.6 0.5 1.3333333333333333 2.0 0.0 0 0.0 8 24 59 178	Automated cherry pick of #72682 : Add `metrics-port` to kube-proxy cmd flags . . Cherry pick of #72682 on release- 1.12 . 72682 : Add : metrics-port . to kube-proxy cmd flags .	1	0
1 1 1.0 1.0 1.0 1.0 1.05 1.0 1.0 1.0 0.6666666666666666 54 1.0 15 31 75 283	Add Queue Sort extension point to the scheduling framework . What would you like to be added : The < URL > is now implementable . We have < URL > plugin interfaces and extension points for ' < URL > ' and ' < URL > ' plugins . A ' Queue Sort ' plugin defines the ordering of pods in the scheduling queue . Today , the scheduler sorts pods by their priority . While this makes sense for most use-cases , some users may have different scheduling requirements that need a different sorting logic . A ' Queue Sort ' plugin allows users to customize ordering of pods . A queue sort plugin essentially will provide a ' less(pod1 , pod2 ) bool ' function . Only one queue sort plugin may be enabled at a time . /sig scheduling /priority important-soon ref/ kubernetes/enhancements #624	1	1
1 1 1.4 1.0 1.4 1.5 1.25 1.0 1.3333333333333333 1.0 1.5714285714285714 7 2.0 10 28 59 277	Bugfix : fix chan leak when stop error . What type of PR is this ? /kind bug What this PR does / why we need it : fix chan leak when stop error Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE .	2	0
1 1 1.0 1.0 1.1 1.0 1.05 1.0 1.3333333333333333 1.0 0.0 0 0.0 14 21 62 153	Fix nodeport repair for ESIPP services . What this PR does / why we need it : The nodeport allocation repair controller does not scrape the : Service . Spec . healthCheckNodePort . value and would remove the allocation from memory and etcd after 10 minutes . This opens the door for other services to use the same nodeport and cause collisions . Which issue(s ) this PR fixes : Fixes #54885 Similar to #64349 Release note : : Fix issue of colliding nodePorts when the cluster has services with externalTrafficPolicy = Local .	2	0
0 0 1.2 2.0 1.2 1.0 1.05 1.0 1.3333333333333333 2.0 0.5714285714285714 14 0.5 16 35 60 101	 kubeadm : use client-go's MakeCSRFromTemplate () in ' renew ' . What type of PR is this ? /kind bug What this PR does / why we need it : Create CSR using the mentioned function which also encodes the type CertificateRequestBlockType . Without that ' certs renew ' is failing with : ' PEM block type must be CERTIFICATE REQUEST ' Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes kubernetes/kubeadm #1216 Special notes for your reviewer : NONE Does this PR introduce a user-facing change ? : : kubeadm : fix a bug in ' certs renew ' related to unknown certificate requests . /area kubeadm /assign @liztio @timothysc @kubernetes /sig-cluster-lifecycle-pr-reviews /priority critical-urgent	0	0
1 1 1.0 1.0 1.0 1.0 1.15 1.0 1.3333333333333333 1.0 2.0 5 2.0 2 4 7 61	Terminating pod status is not refreshed correctly . Using Kubernetes 1.1.2 with Vagrant provider , running two pods : : . /cluster/ kubectl.sh get po NAME READY STATUS RESTARTS AGE mysql-pod 1/1 Running 0 21s wildfly-rc-l2cto 1/1 Running 0 21s . One of the pods was deleted : : . /cluster/ kubectl.sh delete po wildfly-rc-l2cto pod ' wildfly-rc-l2cto ' deleted . Watching the status of pods are shown as : : . /cluster/ kubectl.sh get -w po NAME READY STATUS RESTARTS AGE mysql-pod 1/1 Running 0 1m wildfly-rc-2o8vd 1/1 Running 0 13s wildfly-rc-l2cto 1/1 Terminating 0 1m NAME READY STATUS RESTARTS AGE wildfly-rc-l2cto 0/1 Terminating 0 1m wildfly-rc-l2cto 0/1 Terminating 0 1m wildfly-rc-l2cto 0/1 Terminating 0 1m . Two issues : - Refreshed status shows the only for the changed pod and shows it three times - Even after waiting for 5 minutes , the status does not refresh to Terminated Just checking the status as : kubectl.sh get po . shows that the pod has been terminated . But its confusing that with : -w . the status never updates to Terminated or something intuitive .	2	0
0 0 1.0 1.0 0.9 1.0 0.95 1.0 1.0 1.0 2.0 1 2.0 0 5 10 24	 Kubelet in standalone doesn't work with example YAML files . I'm trying containervm with kubelet built from head and this YAML file : < URL > The YAML file is v1beta2 and doesn't set a name to the Pod . When starting kubelet , it doesn't start the ' nc ' echo server and complains about this on the logs : : config . go : 307 ] Pod[1 ] ( < empty-name > . url-a75326da ) from http failed validation , ignoring : name : required value '' . Chatting with @dchen1107 on IM she mentioned it might be due to a change from @thockin that now requires non-empty pod names ( replaces random generation with validation for non-emptiness ) but that means all existing YAML files ( even the ones with an older version ) will stop working ... Is that expected ? Thanks ! Filipe	0	0
1 1 1.2 1.0 1.1 1.0 1.1 1.0 1.0 1.0 1.25 4 1.0 10 29 53 227	Improve fake clientset performance . What type of PR is this ? /kind bug What this PR does / why we need it : The fake clientset used a slice to store each kind of objects , it's quite slow to init the clientset with massive objects because it checked existence of an object by traversing all objects before adding it , which leads to O(n^2 ) time complexity . Also , the Create , Update , Get , Delete methods needs to traverse all objects , which affects the time statistic of code that calls them . This patch changed to use a map to store each kind of objects , reduced the time complexity of initializing clientset to O(n ) and the Create , Update , Get , Delete to O(1 ) . For example : Before this patch , it took ~ 29s to init a clientset with 30000 Pods , and 2 ~ 4ms to create and get an Pod . After this patch , it took ~ 50ms to init a clientset with 30000 Pods , and tens of 碌s to create and get an Pod . Which issue(s ) this PR fixes : Fixes #89574 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	2	0
1 1 1.2 1.0 1.0 1.0 1.0 1.0 1.6666666666666667 2.0 1.1071428571428572 28 1.0 4 14 56 244	[ Scheduler ] Expand the internal range of score to [ 0 , 100 ] . /sig scheduling /priority important-soon /assign What would you like to be added : We plan to update the internal score range to [ 0 , 100 ] . Why is this needed : < URL > enough . Now that we are transitioning to the framework , we can consider expanding the range . We'll add two constant into the framework interface and multiply the original score by 10 : < URL > < URL > This is an internal change and compatible ( forward and backwards ) the developers won't feel it . See also : < URL > which try to introduce the change . < URL > cc/ @bsalamat @ahg -g @Huang -Wei	1	1
0 0 0.2 0.0 0.2 0.0 0.35 0.0 0.3333333333333333 0.0 0.8461538461538461 13 1.0 0 1 24 145	Not validating front proxy CA Key when using External CA . . What this PR does / why we need it : ' That the front ca key is not required as the front proxy client tls keypair can be managed by the third party . ' This PR don't validate the front CA Key but check if it already exists . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes < URL > Special notes for your reviewer : @yanndegat @timothysc @stealthybox @fabriziopandini Release note : : NONE .	1	0
1 1 0.8 1.0 0.9 1.0 0.75 1.0 0.6666666666666666 1.0 1.5961538461538463 52 2.0 18 41 84 243	 Automated cherry pick of #52569 upstream release 1.8 . : Kube-proxy adds forward rules to ensure NodePorts work . Backport of #52569 @tmjd xref #39823	0	0
2 2 1.2 1.0 1.2 1.0 1.3 1.5 1.0 1.0 1.8 5 2.0 3 9 17 67	 Links in docs to releases.k8s.io/ ... pages are just redirecting to kubernetes.io landing page . I'm seeing this on Chrome on OS X . All requests to releases.k8s.io are just getting a 302 and redirecting to the kubernetes.io landing page , rather than into the docs , regardless of whether I use HTTP or HTTPS . For example , try any of the 3 links in the < URL > . @RichieEscarez @lavalamp @brendandburns 	0	2
1 1 1.4 1.0 1.6 2.0 1.6 2.0 1.6666666666666667 2.0 1.5 4 1.5 5 11 12 60	Document the different flavor of updates and when to use them . replace , patch , apply , ... There used to be an issue , filed as a documentation issue , called something like ' explain the different kinds of updates and when to use them ' but I can't find it . @jackgr @bgrant0607 	2	2
1 1 1.4 1.0 1.0 1.0 1.1 1.0 1.6666666666666667 2.0 2.0 2 2.0 14 16 55 255	E2E Proposal : Verify tcpSocket property of v1 . Probe along with success and failure threshold . . What would you like to be added : A E2E to verify : core . v1 . tcpSocketAction . resource along with probe's important properties like : successThreshold . , : failureThreshold . , and : periodSeconds . . Note : following default values are in effect : delay = 0s timeout = 1s period = 10s #success = 1 #failure =3 . Why is this needed : To improve API coverage based on Kind/Resources and their important properties . < URL > E2E behavior could be : Create a pod with a container having : tcpSocket . based readiness and liveness probes provided with successThreshold , failureThreshold and periodSeconds values and container should not accept any connection after ~ 30 sec of it's start . Container MUST be ready after : successThreshold * periodSeconds . (<~ 30s ) is reached . Container MUST NOT be in ready state after : failureThreshold * periodSeconds . (>~ 30s ) is reached . Note : May need to implement connection closing mechanisam at certain time for : gcr.io/kubernetes-e2e-test-images/liveness:1.1 . image to test failureThreshold for : tcp . requests . /area conformance /sig testing @kubernetes /sig-node-feature-requests	2	1
1 1 1.0 1.0 1.1 1.0 0.8 1.0 1.0 1.0 0.8405797101449275 69 1.0 18 61 100 282	Automated cherry pick of #74715 : add Azure Container Registry anonymous repo support . Cherry pick of #74715 on release- 1.13 . 74715 : add Azure Container Registry anonymous repo support	1	0
2 2 1.0 1.0 0.9 1.0 0.7 0.5 1.0 1.0 0.875 8 1.0 9 31 77 142	Add condition ' len(cfg . DiscoveryToken ) ! = 0 ' to ValidateArgSelection . . What this PR does / why we need it : as per < URL > only when the conditions having len(cfg . DiscoveryToken ) ! = 0 means ' using token-based discovery ' as is mentioned in the error message . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : ref : #54982 Special notes for your reviewer : /cc @kubernetes /sig-cluster-lifecycle-pr-reviews Release note : : NONE .	1	0
2 2 1.0 1.0 1.1 1.0 1.15 1.0 0.6666666666666666 0.0 0.0 0 0.0 8 29 70 134	allow ELB Healthcheck configuration via Service annotations . What this PR does / why we need it : The default settings which are set on the ELB HC work well but there are cases when it would be better to tweak its parameters -- for example , faster detection of unhealthy backends . This PR makes it possible to override any of the healthcheck's parameters via annotations on the Service , with the exception of the Target setting which continues to be inferred from the Service's spec . Release note : : It is now possible to override the healthcheck parameters for AWS ELBs via annotations on the corresponding service . The new annotations are `healthy-threshold` , `unhealthy-threshold` , `timeout` , `interval` ( all prefixed with ` service.beta.kubernetes.io/aws-load-balancer-healthcheck-`) . incorrectly	0	1
1 1 1.0 1.0 0.8 1.0 0.7 1.0 1.0 1.0 0.84375 32 1.0 9 24 59 317	Automated cherry pick of #81856 : Convert tbe e2e to integration test #84036 : Ensure TaintBasedEviction int test not rely on #84766 : Fix a TaintBasedEviction integration test flake #84883 : Update test logic to simulate NodeReady/False and . Cherry pick of #81856 #84036 #84766 #84883 on release- 1.16 . 81856 : Convert tbe e2e to integration test 84036 : Ensure TaintBasedEviction int test not rely on 84766 : Fix a TaintBasedEviction integration test flake 84883 : Update test logic to simulate NodeReady/False and For details on the cherry pick process , see the < URL > page . Part of #85515 .	1	0
0 0 0.4 0.0 0.4 0.0 0.55 1.0 0.0 0.0 0.8333333333333334 18 1.0 10 25 51 257	  Automated cherry pick of #79349 : printer : fix a nil pointer dereference . Cherry pick of #79349 on release- 1.15 . 79349 : printer : fix a nil pointer dereference	0	0
0 0 0.2 0.0 0.1 0.0 0.15 0.0 0.3333333333333333 0.0 1.0833333333333333 12 1.0 16 31 98 196	 Bump Cluster Autoscaler to 1.1.2 . Contains fixes around GPUs and base image change . : Cluster Autoscaler 1.1.2 - release notes : < URL > .	0	0
1 1 1.2 1.0 1.4 1.5 1.3 1.5 1.6666666666666667 2.0 1.0 2 1.0 1 4 5 38	Break out kubectl as a separate download from the server components . : gcloud . already ships it separately . The user cases here differ dramatically . ref #28435 	1	2
2 2 0.6 0.0 0.4 0.0 0.45 0.0 1.0 1.0 2.0 1 2.0 13 31 68 276	 Patch glbc manifest to use version 1.0.0 . Also add rate limiting flags . Will also add a release note to the 1.10 google doc as well . Fixes : #61305 /assign @bowei /cc @nicksardo Release Note : : Bump ingress-gce image in glbc . manifest to 1.0.0 .	0	0
2 2 1.6 2.0 1.6 2.0 1.4 1.5 1.6666666666666667 2.0 0.5 6 0.5 15 31 84 296	Doc changes for nodelocaldns graduating to beta . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : Documentation about nodelocaldns graduating to Beta . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NodeLocal DNSCache graduating to beta . . 	1	2
