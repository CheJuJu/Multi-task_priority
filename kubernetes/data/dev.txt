1 1 0.8 1.0 0.6 1.0 0.35 0.0 1.0 1.0 1.6666666666666667 3 2.0 40 58 85 283	Automated cherry pick of #83027 : Update go mod hcsshim version to fix the kube-proxy issue cannot acce éˆ¥?. Cherry pick of #83027 on release- 1.15 . 83027 : Update go mod hcsshim version to fix the kube-proxy issue cannot access service by self nodeip : port on windows What type of PR is this ? /kind bug What this PR does / why we need it : fix the kube-proxy issue cannot access service by self nodeip : port on windows Which issue(s ) this PR fixes : Fixes #79515 Does this PR introduce a user-facing change ? : : Fixes kube-proxy bug accessing self nodeip : port on windows . /sig network windows @feiskyer @liggitt @BenTheElder @PatrickLang @dims	1	0
2 2 1.6 2.0 1.4 1.0 1.2 1.0 2.0 2.0 1.4285714285714286 14 1.0 4 18 59 243	move nodepreferavoidpods to score plugin . What type of PR is this ? /kind feature /sig scheduling /priority important-soon /release-note-none Which issue(s ) this PR fixes : Fixes #86407	1	1
0 0 0.4 0.0 0.5 0.5 0.55 1.0 0.0 0.0 0.42857142857142855 7 0.0 9 24 51 298	[ 1.10 ] Automated cherry pick of #61373 : Use inner volume name instead of outer volume name for subpath directory . Cherry pick of #61373 on release- 1.10 . 61373 : Use inner volume name instead of outer volume name for subpath directory Release note : : ACTION REQUIRED : In-place node upgrades to this release from versions 1.7.14 , 1.8.9 , and 1.9.4 are not supported if using subpath volumes with PVCs . Such pods should be drained from the node first . . incorrectly	0	0
0 0 1.0 1.0 0.8 1.0 0.55 0.0 0.6666666666666666 0.0 0.0 0 0.0 8 16 27 177	migrate kubelet -- bootstrap-kubeconfig to kubelet.config.k8s.io . What this PR does / why we need it : migrate kubelet -- boot-kubeconfig to kubelet.config.k8s.io Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes #61680 Special notes for your reviewer : @mtaufen Release note : : The Kubelet's -- bootstrap-kubeconfig flag can now be set via the kubelet.config.k8s.io/v1beta1 API by specifying the KubeletConfiguration . BootstrapKubeconfig field . .	1	1
2 2 2.0 2.0 1.8 2.0 1.5 2.0 2.0 2.0 1.2307692307692308 13 1.0 5 16 34 91	Add troubleshooting info for viewing resource usage in cAdvisor , etc . . 	2	2
2 2 1.8 2.0 1.5 2.0 1.65 2.0 2.0 2.0 1.2142857142857142 14 2.0 16 31 47 253	 restore pre- 1.11 behavior of `kubectl get -- template = ... ` . Release note : : NONE . Restores old behavior to the : -- template . flag in : get . go . . In old releases , providing a : -- template . flag value and no : -- output . value implicitly assigned a default value (' go-template ' ) to : -- output . , printing using the provided template argument . Example : : # this should print using GoTemplate printer , but currently does not $ kubectl get pod foo -- template='{{ . metadata.name }}' . cc @deads2k @soltysh	0	0
1 1 0.8 1.0 1.0 1.0 0.95 1.0 1.0 1.0 0.5 2 0.5 22 57 79 259	  Add wildcard tolerations to kube-proxy . Add wildcard tolerations to kube-proxy . Add : nvidia.com/gpu . toleration to nvidia-gpu-device-plugin . Related to #55080 and #44445 . /kind bug /priority critical-urgent /sig scheduling Release note : : kube-proxy addon tolerates all NoExecute and NoSchedule taints by default . . /assign @davidopp @bsalamat @vishh @jiayingz	0	0
0 0 1.0 1.0 1.4 2.0 1.35 2.0 0.6666666666666666 0.0 0.625 8 1.0 39 57 84 283	need a test of NetworkPolicy with IPBlock . Except . : test/e2e/network/network_policy . go . needs a test to ensure that : ipBlock . except . clauses are implemented correctly . Specifically , that they are not implemented as ' deny ' rules . eg , given a client pod with IP : A . B . C . D . , if you add a NetworkPolicy with : : spec : ingress : - from : - ipBlock : cidr : 0.0.0.0 /0 except : - A . B . C . 0/24 . then it should block ingress from that pod . But if you then add a second policy with : : spec : ingress : - from : - ipBlock : cidr : A . B . C . D/32 . then it should reallow ingress from that pod . Then if you delete the first policy , ingress should still be allowed , and if you create the first policy again , ingress should still be allowed . ( ie , the result does not depend on which order the two policies were created in ) . /sig network /priority important-longterm	2	1
1 1 1.4 2.0 1.1 1.0 1.15 1.0 1.0 1.0 1.0 4 1.0 1 17 33 82	 kubeadm : add mandatory configuration to ' phase preflight ' . What this PR does / why we need it : Add the : - mandatory flag ' -- config ' to the preflight phase and parse the specified config file for either ' master ' or ' node ' . - flag ' -- ignore-preflight-errors ' to the preflight phase to allow skipping errors . - the function AddIgnorePreflightsFlag () to ' options/generic . go ' , because the flag is used in multiple commands . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes kubernetes/kubeadm #924 Special notes for your reviewer : this is following : < URL > Release note : : kubeadm : add mandatory ' -- config ' flag to ' kubeadm alpha phase preflight ' . @kubernetes /sig-cluster-lifecycle-pr-reviews /assign @fabriziopandini /milestone 1.12 /kind bug	0	0
1 1 1.0 1.0 1.0 1.0 0.95 1.0 1.0 1.0 1.0689655172413792 29 1.0 1 27 79 273	[ migration phase 1 ] InterPodAffinityPriority as filter plugin . What would you like to be added : Add a filter plugin that calls into InterPodAffinityPriority predicate , example #83460 Why is this needed : Part of #83554 /sig scheduling /help /priority important-soon	1	1
0 0 0.0 0.0 0.6 0.5 0.4 0.0 0.0 0.0 0.5906735751295337 193 0.0 10 29 53 174	  Automated cherry pick of #72856 : Fix nil panic propagation . Cherry pick of #72856 on release- 1.13 . 72856 : Fix nil panic propagation	0	0
0 0 0.0 0.0 0.1 0.0 0.35 0.0 0.0 0.0 0.7037037037037037 27 1.0 3 20 33 122	  Setup dns servers and search domains for Windows Pods . What this PR does / why we need it : Kubelet is depending on docker container's ResolvConfPath ( e.g. /var/lib/docker/containers/439efe31d70fc17485fb6810730679404bb5a6d721b10035c3784157966c7e17/resolv . conf ) to setup dns servers and search domains . While this is ok for Linux containers , ResolvConfPath is always an empty string for windows containers . So that the DNS setting for windows containers is always not set . This PR setups DNS for Windows sandboxes . In this way , Windows Pods could also use kubernetes dns policies . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes #61579 Special notes for your reviewer : Requires Docker EE version > = 17.10.0 . Release note : : Setup dns servers and search domains for Windows Pods in dockershim . Docker EE version > = 17.10.0 is required for propagating DNS to containers . . /cc @PatrickLang @taylorb -microsoft @michmike @JiangtianLi	0	0
0 0 1.2 1.0 1.4 1.5 1.3 1.0 1.0 1.0 0.0 0 0.0 0 0 5 25	The CRI dockershim doesn't support ClusterFirstWithHostNet . Kubernetes version ( use : kubectl version . ): HEAD ( ~ v 1.6.0 -beta . 4 ) Environment : any What happened : I want to use ClusterFirstWithHostNet which was implemented in < URL > but only dockertools ( legacy/non-CRI ) was updated to support this feature . The CRI dockershim should be modified to support this rewriting the resolv . conf file of Pods with HostNetwork = true as well . This is a bug in v 1.6 , I hope it can be prioritized to be fixed in v 1.6.1 , since it's blocking me from using CRI . Since CRI is enabled by default , it's expected that it should have feature parity with the earlier implementation . @kubernetes /sig-node-bugs @freehan @yujuhong @feiskyer @vefimova @bowei @thockin	1	0
2 2 1.6 2.0 1.7 2.0 1.55 2.0 1.6666666666666667 2.0 0.53125 128 0.0 2 4 9 67	Add ability to include intermediates in CSR-issued certificates . Currently , if an intermediate CA is used to sign certs , the csr signing controller does not include the intermediate chain in the CSR status . This means that anything requesting a cert via the CSR API has to know the intermediates themselves ( unlikely ) or anything interacting with a component presenting or serving using an intermediate-issued cert must know about the intermediate issuer . Ideally , we would have a way to : 1 . indicate to the signing controller an optional intermediate bundle to include 2 . include intermediates in the CSR status ( appending to the existing : Certificate . field , having a separate : Intermediates [] byte . field , etc ) /kind feature /sig auth /cc @mikedanese	2	1
1 1 0.2 0.0 0.4 0.0 0.4 0.0 0.3333333333333333 0.0 0.65625 32 0.0 6 7 16 44	Automated cherry pick of #57340 : Fix garbage collector when leader-elect = false . Cherry pick of #57340 #58306 on release- 1.9 . 57340 : Fix garbage collector when leader-elect = false 58306 : Track run status explicitly rather than non-nil check on : Fix garbage collection and resource quota when the controller-manager uses -- leader-elect = false .	1	0
1 1 1.0 1.0 1.0 1.0 1.2 1.0 1.0 1.0 0.660377358490566 53 1.0 15 41 61 281	Add Permit extension point for the scheduling framework . What would you like to be added : The < URL > is now implementable . We have < URL > plugin interfaces and extension points for ' < URL > ' and ' < URL > ' plugins . ' < URL > ' is an important extension point that enables building advanced scheduling features , such as gang scheduling ( AKA co-scheduling in K8s world ) . As a part of this effort , a new interface for permit plugins should be added . The main function of the plugin should return a ' < URL > ' and a timeout . We also need to build the record keeping for pods in ' wait ' state and add a clean-up mechanism for pods whose timeout has expired . /sig scheduling /priority important-soon ref/ kubernetes/enhancements #624	1	1
2 2 1.6 2.0 1.7 2.0 1.75 2.0 1.6666666666666667 2.0 0.0 0 0.0 7 30 57 257	Sort Field for ListOptions . I would like to have the ability to sort objects server side by a particular field on the object being returned when issuing a get request that returns a list of objects . I am wondering about the impact of potentially adding a : Sort . field to < URL > so that a sort field can be specified as part of a get request . If there is already an option available , I would like to know where I can find an example . What would you like to be added : An example of how to return a list of objects in a sorted order using a go-client or the ability through ListOptions to specify a sort field . Why is this needed : An example of where I would like to use such a field can be found as part of the < URL > project . The idea here would be to sort : pipelineruns . by their start times and then returned to the requestor .	2	1
0 0 0.8 1.0 1.1 1.0 1.2 1.0 0.6666666666666666 1.0 0.5882352941176471 17 1.0 19 38 67 252	Create-update-delete-deployment example using dynamic package . What type of PR is this ? /kind documentation What this PR does / why we need it : This PR ads documentation/example showing the use of dynamic package in client-go repo . Which issue(s ) this PR fixes : Fixes #76512 : NONE . 	1	2
1 1 0.6 1.0 0.7 1.0 0.7 1.0 0.6666666666666666 1.0 0.725 40 1.0 20 47 76 310	Move CSI volume expansion to beta . Move CSI volume expansion to beta . xref - < URL > fixes < URL > /sig storage : Move CSI volume expansion to beta .	1	1
1 1 1.2 1.0 1.4 1.5 1.25 1.0 1.6666666666666667 2.0 0.0 0 0.0 3 15 53 223	fix too many pdb update operations when nothing change . What type of PR is this ? /kind bug What this PR does / why we need it : There are too many pdb update operations even if no changes take place . The PR fix it . Which issue(s ) this PR fixes : Fixes #74240 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE .	1	0
1 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9347826086956522 46 1.0 24 26 58 296	Migrate Kubelet -- experimental-kernel-memcg-notification to kubelet.config.k8s.io or remove the flag . Flag name : : experimental-kernel-memcg-notification . Help text : If enabled , the kubelet will integrate with the kernel memcg notification to determine if memory eviction thresholds are crossed rather than polling . This is part of migrating the Kubelet command-line to a Kubernetes-style API . The : -- experimental-kernel-memcg-notification . flag should either be migrated to the Kubelet's : kubelet.config.k8s.io . API group , or simply removed from the Kubelet . If this could be considered an instance-specific flag , or a descriptor of local topology managed by the Kubelet , see : < URL > If this flag is only registered in os-specific builds , see : < URL > As : -- experimental-kernel-memcg-notification . is an alpha/experimental flag , the feature it configures must either be feature-gated , or graduated from alpha/experimental status prior to the migration . @sig -node-pr-reviews @sig -node-api-reviews /assign @mtaufen /sig node /kind feature /priority important-soon /milestone v 1.11 /status approved-for-milestone	1	1
0 0 0.2 0.0 0.6 1.0 0.7 1.0 0.0 0.0 0.5 6 0.5 11 26 52 221	  Automated cherry pick of #85027 : Fix bug about unintentional scale out during updating . Cherry pick of #85027 on release- 1.17 . 85027 : Fix bug about unintentional scale out during updating For details on the cherry pick process , see the < URL > page .	0	0
1 1 1.4 1.0 1.5 1.5 1.5 1.5 1.3333333333333333 1.0 0.0 0 0.0 2 4 41 281	correction of executable path doc . correct executable path by remove : /go . What type of PR is this ? /kind documentation What this PR does / why we need it : make the doc exactly 	2	2
2 2 1.2 1.0 1.1 1.0 0.85 1.0 1.3333333333333333 2.0 0.5 8 0.5 7 20 51 236	 Automated cherry pick of #65454 : Update Rescheduler's manifest . Cherry pick of #65454 on release- 1.11 . 65454 : Update Rescheduler's manifest	0	1
1 1 1.0 1.0 1.1 1.0 1.15 1.0 1.0 1.0 0.0 0 0.0 8 14 65 240	Kubelet changes for Windows GMSA support . What type of PR is this ? /kind feature What this PR does / why we need it : This patch comprises the kubelet changes outlined in the GMSA KEP ( < URL > to add GMSA support to Windows workloads . More precisely , it includes the logic proposed in the KEP to resolve which GMSA spec should be applied to which containers , and changes : dockershim . to copy the relevant GMSA credential specs to Windows registry values prior to creating the container , passing them down to docker itself , and finally removing the values from the registry afterwards ; both these changes need to be activated with the : WindowsGMSA . feature gate . Includes unit tests . Which issue(s ) this PR fixes : KEP at < URL > Special notes for your reviewer : Do Windows unit tests run as part of a regular build ? If not I'll need to change this PR slightly to still run the new tests on Linux . Does this PR introduce a user-facing change ? : : Allow the kubelet to pass Windows GMSA credentials down to Docker .	2	1
1 1 0.8 1.0 1.0 1.0 1.15 1.0 0.6666666666666666 1.0 1.1176470588235294 17 1.0 13 19 58 274	Implemented taints and tolerations priority function as a Score plugin . What type of PR is this ? /kind feature What this PR does / why we need it : Implements taints and tolerations priority as a Score Plugin . Which issue(s ) this PR fixes : Fixes #83531 Does this PR introduce a user-facing change ? : : NONE .	1	1
1 1 0.8 1.0 0.9 1.0 0.95 1.0 0.6666666666666666 1.0 0.9333333333333333 30 1.0 7 9 31 168	fix : determine the correct ip config based on ip family . What type of PR is this ? /kind bug What this PR does / why we need it : Determines the correct IP config based on clusterIP family before updating backend pool for VMSS Which issue(s ) this PR fixes : Fixes < URL > Special notes for your reviewer : Does this PR introduce a user-facing change ? : : fix : determine the correct ip config based on ip family . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : NONE . /area provider/azure /priority important-soon /assign @feiskyer @khenidak	1	0
1 1 0.8 1.0 0.8 1.0 1.1 1.0 0.6666666666666666 1.0 1.0108695652173914 92 1.0 17 38 53 147	  Invoke metrics . Register as early as possible in the scheduler initialization . What type of PR is this ? /kind bug What this PR does / why we need it : < URL > switched the scheduler from Prometheus to k8s wrapper in 1.16 , but didn't fix the registration . This caused the scheduler's : pending_pods . metric to break : this metric is reported by the scheduler queue . At the time of instantiating the queue , we obtain a reference to the : pending_pods . metric , but this happens before the metric is registered , which means the reference that the queue obtained is a noop metric . Which issue(s ) this PR fixes : Part of #87690 Special notes for your reviewer : I am sending a separate PR to fix the registration order to make it easy to backport to 1.16 Does this PR introduce a user-facing change ? : : scheduler's pending_pods metric to be reported back . . /cc @liu -cong	0	0
1 1 1.4 1.0 1.5 1.5 1.35 1.0 1.3333333333333333 1.0 1.0 4 1.0 14 33 47 259	bandwidth : handle new ' chain X ' output from ' tc filter show dev XYZ ' . tc started adding ' chain 0 ' into the output of ' tc filter show dev eth0 ' as of iproute2 commit 732f03461bc48cf94946ee3cc92ab5832862b989 and that confuses the bandwidth code . Related : < URL > /kind bug : NONE . @cadmuxe	1	0
1 1 1.8 2.0 1.9 2.0 1.8 2.0 1.6666666666666667 2.0 0.7241379310344828 29 1.0 3 3 13 42	 Recheck if transformed data is stale when doing live lookup during update . Fixes #49565 Caching storage can pass in a cached object to : GuaranteedUpdate . as a hint for the current object . If the hint is identical to the data we want to persist , before short-circuiting as a n o-o p update , we force a live lookup . We should check two things on the result of that live lookup before short-circuiting as a n o-o p update : 1 . the bytes we want to persist still match the transformed bytes read from etcd 2 . the state read from etcd didn't report itself as stale . this would mean the transformer used to read the data would not be the transformer used to write it , and ' n o-o p ' writes should still be performed , since transformation will make the underlying content actually different . After a live lookup , we checked byte equality , but not the stale indicator . This meant that key rotation or encrypted -> decrypted , and decrypted -> encrypted updates are broken . Introduced in #54780 and picked back to 1.8 in #55294 : Fixed encryption key and encryption provider rotation .	0	0
1 1 1.0 1.0 1.3 1.0 1.1 1.0 1.0 1.0 0.717948717948718 39 1.0 9 12 55 251	Add migration shim for verifyvolumeattachment and bulk verify . : VerifyVolumesAreAttached . and : BulkVolumeVerify . were not shimmed to CSI when migration was enabled for the verification plugin . This implements the shim layer for those functions . However , : VerifyVolumesAreAttached . for CSI is broken since it checks the : VolumeAttachment . object which is not currently necessarily representative of what's attached in the backend ( if something was detached out of band VolumeAttachment wont see that ) . and : BulkVolumeVerify . is not implemented for CSI . Those are separate issues that can be worked on after this PR . /kind feature /sig storage /assign @msau42 @ddebroy @leakingtapan @saad -ali /cc @gnufied @jsafrane @andrewsykim @adisky @andyzhangx : Add CSI Migration Shim for VerifyVolumesAreAttached and BulkVolumeVerify .	1	1
1 1 0.8 1.0 0.9 1.0 0.9 1.0 0.6666666666666666 1.0 0.7714285714285715 70 1.0 10 26 59 280	[ Windows ] Upload containerd logs to stackdriver . Log containerd output to a log file : containerd . log . , and upload the log to stackdriver . Note that we are using the : container-runtime . tag , which matches what we are using on linux < URL > I've validated the change in my cluster , and the log can be successfully uploaded : /cc @kubernetes /sig-windows-misc @yliaog @pjh Signed-off-by : Lantao Liu < URL > : none .	1	1
0 0 0.4 0.0 0.6 1.0 0.9 1.0 0.3333333333333333 0.0 0.5755813953488372 172 0.0 7 17 52 244	Investigate adding per-request binding or assertions to apiserver proxy . Investigate adding per-request binding or assertions that would let backends verify a particular request was intentionally forwarded from the apiserver proxy From < URL > < URL > /area security /area apiserver /priority important-longterm /kind cleanup /sig api-machinery	2	1
0 0 0.0 0.0 0.2 0.0 0.5 0.5 0.0 0.0 1.0 4 1.0 1 21 53 198	Cherry pick of #85689 : Export scheduler . Snapshot function . This was added as a workaround for Cluster Autoscaler , but didn't make the cut for release- 1.17 . Now we want to sync other cherry-pick fixes from release- 1.17 , but we can't do that without this commit on the branch . /kind cleanup incorrectly	0	0
0 0 0.8 1.0 1.0 1.0 0.95 1.0 0.6666666666666666 0.0 0.0 1 0.0 0 2 10 17	[ e2e ] Namespaces [ Serial ] should ensure that all pods are removed when a namespace is deleted . . Test is constantly failing . < URL > < URL > : /go/src/ k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/namespace.go:265 Expected error : < * errors . errorString | 0xc421a2cc80 > : { s: ' an empty namespace may not be set when a resource name is provided ' , } an empty namespace may not be set when a resource name is provided not to have occurred /go/src/ k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/namespace.go:95 . < URL > /cc @kubernetes /sig-api-machinery-bugs @kubernetes /kubernetes-release-managers @dchen1107 incorrectly	0	0
1 1 1.0 1.0 0.7 1.0 0.6 1.0 1.0 1.0 0.5714285714285714 7 1.0 5 17 23 52	Revert ' CPU manager wiring and `none` policy ' . Reverts kubernetes/kubernetes #51357 Seems likely this should fix < URL > cc @ConnorDoyle @derekwaynecarr incorrectly	0	0
2 2 2.0 2.0 2.0 2.0 1.8 2.0 2.0 2.0 0.0 0 0.0 3 6 19 61	Kubelet doc . go . I think we should write doc . go files for packages in Kubelet , like we do for the rest of Kubernetes . In some cases , it's not obvious what the code in a package does ( for example , without spending some time reading the code , it's not clear what kind of ' container ' files are in kubelet/container - some ' container ' source files are outside the folder ) . In some cases , I think the doc . go files need to be expanded upon ( kubelet/types/types . go says ' Common types in the Kubelet . ' , but there's also a kubelet/types . go file ) . I think having clear doc files will help structure the code better - contributors will know how they should organize their code when introducing a feature . 	2	2
1 1 1.2 1.0 1.3 1.0 1.15 1.0 1.3333333333333333 1.0 1.6 15 2.0 3 28 54 309	Use no-priority best-effort pod as the preemptor in BenchmarkGetPodsToPreempt . What type of PR is this ? /kind bug What this PR does / why we need it : I see the following when running BenchmarkGetPodsToPreempt : : panic : runtime error : invalid memory address or nil pointer dereference [ signal SIGSEGV : segmentation violation code = 0x1 addr = 0x280 pc = 0x1d2f91a ] goroutine 10 [ running ]: k8s.io/kubernetes/pkg/kubelet/types.IsCriticalPod(0x0 , 0x1019ace ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/types/pod_update.go:150 +0x5a k8s.io/kubernetes/pkg/kubelet/types.Preemptable(0x0 , 0xc000158380 , 0x20b1d69 ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/types/pod_update.go:165 +0x2f k8s.io/kubernetes/pkg/kubelet/preemption.sortPodsByQOS(0x0 , 0xc0000a0400 , 0x6e , 0x80 , 0x0 , 0x203000 , 0x203000 , 0xc0000a0400 , 0x11c4d5a , 0x6e , ... ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/preemption/preemption.go:231 +0xdf k8s.io/kubernetes/pkg/kubelet/preemption.getPodsToPreempt(0x0 , 0xc0000a0400 , 0x6e , 0x80 , 0xc00007cf48 , 0x1 , 0x1 , 0x80 , 0x1d045458 , 0x1d04545800000000 , ... ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/preemption/preemption.go:119 +0x6a k8s.io/kubernetes/pkg/kubelet/preemption.BenchmarkGetPodsToPreempt(0xc00013e1a0 ) /Users/yute/go-workspace/src/ k8s.io/kubernetes/pkg/kubelet/preemption/preemption_test.go:153 +0x1b2 . This PR fixes the panic by using no-priority best-effort pod as the preemptor in BenchmarkGetPodsToPreempt . : NONE .	2	0
0 0 0.8 0.0 1.1 1.5 1.0 1.0 0.0 0.0 0.0 0 0.0 8 14 42 225	 kubectl : error logging as string instead of [] byte . Changes the error output when duplicate keys found while creating : secret . or : configMap . ( BinaryData ) Show the data of the duplicated secret as : string . instead of : [] byte . Removes error output of binary data in : configMaps . closes #73969	0	0
2 2 1.2 2.0 0.9 1.0 0.75 1.0 1.3333333333333333 2.0 1.0 2 1.0 7 9 37 152	Automated cherry pick of #62464 : avoid dobule RLock () in cpumanager . Cherry pick of #62464 on release- 1.9 . 62464 : avoid dobule RLock () in cpumanager incorrectly	0	0
1 1 1.4 1.0 1.6 2.0 1.55 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 4 13 58	api returns error 415 on PATCH . PATCH < URL > {' spec ' :{ ' replicas ' : 0 }} answer : { ' kind ' : ' Status ' , ' apiVersion ' : ' v1beta3 ' , ' metadata ' : {} , ' status ' : ' Failure ' , ' message ' : ' the server responded with the status code 415 but did not return more information ' , ' details ' : {} , ' code ' : 415 } Before v 0.15.0 it had worked well .	2	0
0 0 0.0 0.0 0.2 0.0 0.2 0.0 0.0 0.0 0.24242424242424243 33 0.0 12 18 91 215	[ e2e failure ] [ sig-cluster-lifecycle ] Upgrade [ Feature : Upgrade ] cluster upgrade should maintain a functioning cluster [ Feature : ClusterUpgrade ] . /priority critical-urgent /priority failing-test /kind bug /status approved-for-milestone /area platform/gke @kubernetes /sig-cluster-lifecycle-test-failures owns the test @kubernetes /sig-gcp-test-failures this looks GKE-specific @kubernetes /sig-node-test-failures for the AppArmor cluster This test has been failing since at least 2017-11-13 for the following jobs : - < URL > - < URL > - < URL > - < URL > These jobs are on the < URL > , and prevent us from cutting v 1.9.0 -beta . 1 ( kubernetes/sig-release #34 ) . Is there work ongoing to bring this test back to green ? < URL > < URL > : ... Response : code = 400 , message = Cluster master cannot be upgrade to \\\' 1.10.0 -alpha . 0.55 +01c74145c7b655\\\' ... . < URL > : ... invalid AppArmor profile name : ' unconfined ' ... . Sample failure : < URL > incorrectly	0	0
1 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0172413793103448 58 1.0 6 26 62 284	Define algorithm providers in terms of plugins instead of predicates/priorities [ Migration Phase 2 ] . /assign @ahg -g @draveness /sig scheduling /priority important-soon Part of #85822	1	1
2 2 1.6 2.0 1.5 1.5 1.25 1.0 2.0 2.0 0.6590909090909091 44 1.0 6 9 13 49	Failed , Tainting nodes by conditions seems to visibly slow down cluster startup . After < URL > has been merged , our large tests started behaving a bit strange . In particular , when 5k node cluster was reported as up Failed , ( with all nodes being ready ) , the test itself was only discovering 4k or even less nodes as schedulable . Example is here : < URL > There were a couple issues that were fixed in the meantime by @mborsz mostly he fixed validation : < URL > That said , that issue clearly shows that tainting nodes by conditions visibly slows down the startup , because now , once the condition are reported as ' healthy ' the node has to be processed by nodelifecycle controller to remove the corresponding taints . I didn't yet take a look if the problem here is its throughput or is it related to the fact that there are so many nodes to process at once , but given my experience in the past , I suspect that it may be serialized processing with reporting that in a single thread ... @k82cn @bsalamat @kubernetes /sig-scalability-bugs @shyamjvs @mborsz incorrectly	0	0
2 2 1.8 2.0 1.7 2.0 1.45 1.5 2.0 2.0 0.8648648648648649 37 1.0 14 55 95 334	for aggregated apiserver availability , try multiple endpoints in parallel . Marking an aggregated API server as unavailable is a big deal . It affects the availability of an entire API group version . Right now , if a single endpoint is having difficulty ( maybe it started failing or its node network went down or the proxy has latent entries or any number of things ) , the entire apiservice is removed from rotation . Instead of doing that on a single bad request , we can check multiple endpoints in parallel . If any one of them succeeds , we can consider the apiservice as available . Individual requests to a ' bad ' endpoint may still fail , but the API group version remains available overall . /kind bug /priority important-soon @kubernetes /sig-api-machinery-bugs : NONE .	1	0
1 1 0.8 1.0 1.0 1.0 1.4 2.0 0.6666666666666666 1.0 0.0 0 0.0 4 15 59 260	HPA incorrectly reported condition status . What type of PR is this ? /kind bug What this PR does / why we need it : This PR fixes HPA incorrectly reported condition status . Example : Consider a case where : desiredReplicas = 1 minimumAllowedReplicas = 2 hpaMinReplicas = 2 . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : NA Does this PR introduce a user-facing change ? : : NONE .	2	0
1 1 0.4 0.0 0.2 0.0 0.4 0.0 0.6666666666666666 1.0 1.0 3 1.0 22 38 63 245	Automated cherry pick of #86412 : It fixes a bug where AAD token obtained by kubectl is . Cherry pick of #86412 on release- 1.15 . 86412 : It fixes a bug where AAD token obtained by kubectl is For details on the cherry pick process , see the < URL > page .	1	0
0 0 0.6 0.0 0.6 0.0 0.55 0.0 1.0 1.0 1.0 2 1.0 9 33 66 166	  fix bug excludeCIDRs was not assign in func NewProxier . What this PR does / why we need it : fix bug excludeCIDRs was not assign in func NewProxier Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes # Special notes for your reviewer : Release note : : NONE .	0	0
2 2 2.0 2.0 1.5 2.0 1.45 2.0 2.0 2.0 1.6097560975609757 41 2.0 1 16 33 105	Generalize prevention of accidental deletion / mutation . PR #9975 added hardcoded protection of the default namespace to the Lifecycle admission controller . This mechanism at least needs to be configurable , since Openshift and Kubernetes have additional infrastructure namespaces they'd like to protect . However , it also needs to be possible to remove the protection to shut down a cluster #4630 , without making it susceptible to accidents . One solution would be to add a : protected . field to metadata of any object . : protected : true . would prevent deletion until the object was updated to set : protected : false . . This would also make it straightforward to protect the special Kubernetes services , addons , and other self-hosted components , while still making it possible to update them and to delete all resources during shutdown . cc @derekwaynecarr @lavalamp	1	1
2 2 1.4 1.0 1.3 1.0 1.25 1.0 1.6666666666666667 2.0 1.5 4 2.0 7 21 47 259	Update CHANGELOG-1.15.md . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : - Move KubeletPodResources to the Beta section . - Move NonPreemptingPriority to the Alpha section . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . 	2	2
2 2 1.6 2.0 1.4 1.0 1.2 1.0 1.6666666666666667 2.0 1.5 4 1.5 10 25 60 318	Revert ' kube-proxy : check KUBE-MARK-DROP ' . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespace from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : This reverts commit 1ca0ffeaf2c4401549b82f549f7481313308d4b9 . kube-proxy is not recreating the rules associated to the KUBE-MARK-DROP chain , that is created by the kubelet . Is preferrable to avoid the dependency between the kubelet and kube-proxy , so each of them handles their own rules . Which issue(s ) this PR fixes : Fixes #85414 Special notes for your reviewer : This is only needed for kube-proxy operating in dual-stack with iptables , and that PR wasn't merged . Also , there is a discussion about this topic with a better solution than this < URL > Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : . incorrectly	0	0
1 1 1.8 2.0 1.5 2.0 1.45 2.0 1.6666666666666667 2.0 0.6129032258064516 31 1.0 9 18 27 163	Improve error message when diff binary is not in PATH . What type of PR is this ? /kind bug What this PR does / why we need it : Which issue(s ) this PR fixes : Fixes partially #87343 Special notes for your reviewer : Examples , Before : PATH = /usr/bin/kubectl diff -f manifest . yaml error : executable file not found in $PATH KUBECTL_EXTERNAL_DIFF = unknown-binary kubectl diff -f manifest . yaml error : executable file not found in $PATH . After : PATH = /usr/bin/kubectl diff -f manifest . yaml error : failed to run ' diff ' : executable file not found in $PATH KUBECTL_EXTERNAL_DIFF = unknown-binary kubectl diff -f manifest . yaml error : failed to run ' unknown-binary ' : executable file not found in $PATH . Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	1	0
1 1 1.2 1.0 1.1 1.0 0.9 1.0 1.3333333333333333 1.0 0.0 0 0.0 5 21 69 258	The default-http-backend for handling 404 pages will now point to 404 éˆ¥?. éˆ¥?handler with prometheus integration and provides metrics related to requests per second and the duration of responding to the requests for various percentile groupings . Please check < URL > for details about the 404-server-with-metrics . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : The behavior of the default handler for 404 requests fro the GCE Ingress load balancer is slightly modified in the sense that it now exports metrics using prometheus . The metrics exported include : - http_404_reques t_t otal ( the number of 404 requests handled ) - http_404_request_duration_ms ( the amount of time the server took to respond in ms ) Also includes percentile groupings . The directory for the default 404 handler includes instructions on how to enable prometheus for monitoring and setting alerts . .	1	1
2 2 1.4 1.0 1.5 1.5 1.5 1.5 1.3333333333333333 1.0 1.5483870967741935 31 2.0 5 18 87 278	Continue with remaining volumeAttached's in VerifyVolumesAreAttached . What type of PR is this ? /kind bug What this PR does / why we need it : In operationExecutor #VerifyVolumesAreAttached , : for _ , volumeAttached : = range nodeAttachedVolumes { . toward the end of the above loop , after the call to oe . VerifyVolumesAreAttachedPerNode , we break out of the loop and ignore remaining volumeAttached's This is inconsistent with the handling on line 707 of the bulk volume verification : : continue . This PR removes the break and continues with remaining volumeAttached's : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	1	0
2 2 1.4 2.0 1.6 2.0 1.6 2.0 1.3333333333333333 2.0 2.0 2 2.0 2 4 6 28	PodSecurityPolicy API is enabled but unusable by default , and doesn't have E2Es . The PodSecurityPolicy API got defaulted to on in < URL > but kube-up does not turn on the admission controller ( neither do kube-adm or GKE ) , and we don't run any tests against it . Right now , most deployments have an API object that can be created , but won't do anything ( until we decide sometime later that we actually want to enable PodSecurityPolicy ) . @bgrant0607 @pweil - @erictune	1	0
0 0 0.2 0.0 0.6 0.5 0.5 0.0 0.3333333333333333 0.0 1.0 2 1.0 19 24 74 212	 Remove event handler to satisfy alpha tests . What this PR does / why we need it : An original assumption of time out did not fix issue . The events look masked by lubelet flags so reducing test Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes #64578 /cc @msau42 Special notes for your reviewer : Release note : : NONE .	0	0
0 0 0.6 1.0 0.6 1.0 0.75 1.0 0.3333333333333333 0.0 0.6585365853658537 41 1.0 6 21 76 289	Automated cherry pick of #75792 : Updated regional PD minimum size ; changed regional PD . Cherry pick of #75792 on release- 1.14 . 75792 : Updated regional PD minimum size ; changed regional PD : NONE .	1	0
1 1 1.2 1.0 1.1 1.0 1.25 1.0 1.3333333333333333 1.0 1.0 1 1.0 8 19 99 328	SafeSysctlWhitelist : add net . ipv4 . ping_group_range ( allow ping without CAP_NET_RAW ) . What type of PR is this ? /kind feature (?) What this PR does / why we need it : Allow setting sysctl value : net . ipv4 . ping_group_range . , which can be used for allowing : ping . command without : CAP_NET_RAW . capability . e.g. : net . ipv4 . ping_group_range='0 42 ' . to allow ping for users with GID 0-GID 42 . This sysctl value was introduced in kernel 3.0 and has been namespaced since its birth . < URL > release-note : : SafeSysctlWhitelist : add net . ipv4 . ping_group_range .	1	1
1 1 0.8 1.0 1.0 1.0 1.2 1.0 1.0 1.0 1.0 2 1.0 1 16 69 256	Fix cronjob controller page list err . What type of PR is this ? /kind bug What this PR does / why we need it : When the number of jobs exceeds 500 , cronjob cannot schedule , bug of pager . List Which issue(s ) this PR fixes : Fixes # < URL > Special notes for your reviewer : Does this PR introduce a user-facing change ? : NONE : When the number of jobs exceeds 500 , cronjob should schedule without error . .	2	0
1 1 1.0 1.0 0.9 1.0 0.9 1.0 1.0 1.0 0.3888888888888889 18 0.0 9 18 59 148	 Add block volume support to internal provisioners . . What this PR does / why we need it : Internal provisioners now create filesystem PVs when block PVs are requested . This leads to unbindable PVCs . In this PR , volume plugins that support block volumes provision block PVs when block is requested . All the other provisioners return clear error in : kubectl describe pvc . : : Events : Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 7s ( x2 over 18s ) persistentvolume-controller Failed to provision volume with StorageClass ' standard ' : kubernetes.io/cinder does not support block volume provisioning . : AWS EBS , Azure Disk , GCE PD and Ceph RBD volume plugins support dynamic provisioning of raw block volumes . . cc @kubernetes /vmware for vsphere changes cc @andyzhangx for Azure changes /assign @copejon @mtanino	0	0
2 2 1.4 2.0 1.1 1.0 0.9 1.0 2.0 2.0 1.34375 32 2.0 9 16 65 268	Automated cherry pick of #76788 : Test kubectl cp escape . Cherry pick of #76788 on release- 1.13 . 76788 : Test kubectl cp escape	2	0
2 2 1.4 1.0 1.2 1.0 1.35 1.0 1.6666666666666667 2.0 0.0 0 0.0 6 37 68 265	removed extra hyphen in kubectl book . What type of PR is this ? /kind documentation What this PR does / why we need it : removed extra hyphen of : -- all-namespaces . option in kubectl book . < URL > Which issue(s ) this PR fixes : kubernetes/kubectl #729 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . 	2	2
0 0 1.2 1.0 1.1 1.0 1.15 1.0 1.0 1.0 0.9862068965517241 145 1.0 5 10 40 193	  Fix bug in reflector not recovering from ' Too large resource version'éˆ¥?. Ref < URL > : Fix bug in reflector that couldn't recover from ' Too large resource version ' errors . /kind bug	0	0
0 0 0.6 1.0 1.2 1.0 1.0 1.0 0.6666666666666666 1.0 0.5555555555555556 45 0.0 18 23 64 223	Graduate Pod priority and preemption to GA . What type of PR is this ? /kind feature What this PR does / why we need it : Pod Priority and Preemption was graduated to Beta in 1.11 . This PR graduates the features to GA in 1.14 . Which issue(s ) this PR fixes : Fixes # kubernetes/enhancements #564 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : Graduate Pod Priority and Preemption to GA . . /sig scheduling	2	1
2 2 1.0 1.0 0.6 0.5 0.95 1.0 1.3333333333333333 1.0 2.0 4 2.0 20 59 98 280	Storage Validation : Promote multivolume test . . This PR promotes existing test to Validation Suite . A Validation test can then be promoted to a Conformance test if it is determined to be portable . /sig storage /sig testing /area conformance /kind test /release-note none	2	1
1 1 1.2 1.0 1.2 1.0 1.25 1.0 1.0 1.0 0.5714285714285714 56 1.0 21 64 102 285	Add network stats for Windows nodes and containers . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : Add network stats for Windows nodes and containers . Which issue(s ) this PR fixes : Fixes #74101 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : Add network stats for Windows nodes and containers . /sig windows /sig node /assign @yujuhong @PatrickLang @michmike	1	0
0 0 1.0 1.0 1.2 1.0 1.1 1.0 1.0 1.0 0.7555555555555555 45 1.0 4 20 31 187	Add events to PV when mount fails on filesystem mismatch . Add a event to PV when mount fails because of fs mismatch Filesystem mismatch is a special event . This could indicate either user has asked for incorrect filesystem or there is a error from which mount operation can not recover on retry . /sig storage /kind bug : Add a event to PV when filesystem on PV does not match actual filesystem on disk .	1	0
0 0 0.2 0.0 0.7 0.0 0.6 0.0 0.3333333333333333 0.0 0.5 6 0.5 7 42 83 229	 Fix output of `kubeadm migrate config` . The output should always be valid kubeadmapi . MasterConfiguration YAML . The general problem was that we printed with fmt . Fprintf but it turns out some of the default values have : % . s in them so this caused Go to think we were missing values that we wanted substituted . We don't want to do any substitution here . Signed-off-by : Chuck Ha < URL > What this PR does / why we need it : This PR fixes a small bug that cause kubeadm migrate config to print YAML that was not valid . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes kubernetes/kubeadm #904 : NONE . /cc @luxas @timothysc	0	0
1 1 1.2 1.0 1.1 1.0 1.25 1.0 1.0 1.0 2.0 3 2.0 4 14 21 63	hack/ verify-godeps.sh is failing on release- 1.1 branch . : package github.com/aws/aws-sdk-go/internal/apierr imports github.com/aws/aws-sdk-go/internal/apierr imports github.com/aws/aws-sdk-go/internal/apierr : cannot find package ' github.com/aws/aws-sdk-go/internal/apierr ' in any of : /root/ . gvm/gos/go 1.4 /src/ github.com/aws/aws-sdk-go/internal/apierr ( from $GOROOT ) /tmp/gopath . JUrP0w/src/ github.com/aws/aws-sdk-go/internal/apierr ( from $GOPATH ) godep : restore : exit status 1 package github.com/google/google-api-go-client/cloudmonitoring/v2beta2 imports github.com/google/google-api-go-client/cloudmonitoring/v2beta2 imports github.com/google/google-api-go-client/cloudmonitoring/v2beta2 : code in directory /tmp/gopath . JUrP0w/src/ github.com/google/google-api-go-client/cloudmonitoring/v2beta2 expects import ' google.golang.org/api/cloudmonitoring/v2beta2 ' godep : restore : exit status 1 !!!  in . /hack/ verify-godeps.sh:69 '' ${GODEP }' restore ' exited with status 1 Call stack : 1 : . /hack/ verify-godeps.sh:69 main (...) Exiting with status 1 .	1	0
0 0 0.4 0.0 0.6 0.5 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 0 2 16	Fixes kubectl cached discovery on Windows . Fixes < URL > The : kubectl . cached discovery makes use of : func ( f * File ) Chmod(mode FileMode ) error . which is not supported and errors out on Windows , making : kubectl get . and potentially a number of other commands to fail miserably on that platform . : os . Chmod . by file name , on the other hand , does not error out and should be used instead . Release note : : NONE . @deads2k @brendandburns @kubernetes /sig-cli-pr-reviews	1	0
1 1 1.4 1.0 1.3 1.0 1.15 1.0 1.3333333333333333 1.0 0.9322033898305084 177 1.0 3 17 34 153	add StatusConflict(409 ) as non-retriable error for disksClient . What type of PR is this ? /kind bug What this PR does / why we need it : This PR adds StatusConflict(409 ) as non-retriable error for disksClient , delete volume is triggered by k8s volume controller in a loop if it failed , no need to regard it as retriable error Already verified in azure disk CSI driver : < URL > Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : none . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : . /kind bug /assign @feiskyer /priority important-soon /sig cloud-provider /area provider/azure	1	0
2 2 1.6 2.0 1.3 2.0 1.4 2.0 2.0 2.0 0.0 0 0.0 13 25 61 277	link scripts in build/ README.md . This makes it easier to click through and learn what these do , for someone getting started browsing this developer doc . 	1	2
0 0 0.4 0.0 0.6 1.0 0.6 1.0 0.0 0.0 1.0 20 1.0 4 6 35 149	Failing Test : [ sig-testing ] ci-kubernetes-e2e-gce-scale-correctness : BeforeSuite . Failing Job < URL > Failing Test < URL > Triage results < URL > /kind bug /priority failing-test /priority important-soon /sig testing /milestone v 1.11 @kubernetes /sig-testing-bugs cc @jberkus @tpepper @shyamjvs /assign @BenTheElder for triage	1	0
1 1 0.8 1.0 0.5 0.5 0.5 0.0 0.6666666666666666 1.0 0.578125 64 0.0 17 33 74 261	 Prevent 1.9 e2es testing deprecated/removed features in 1.10 . 1.9 e2e tests get run against 1.10.0 + masters during upgrade tests . This version-gates testing deprecated features removed in 1.10 < URL > Fixes #60769 Fixes #60767	0	0
0 0 0.4 0.0 1.0 1.0 1.05 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 22 77 290	Reorder stackdriver setup in windows startup script . Stackdriver was failing to initialize correctly because it could not connect to metadata server . It could not connect to metadata server because the HNS initialization in the windows startup script , reset the network . So , move stackdriver startup block after HNS stabilizes . At the later stage of the init script , metadata server is available and stackdriver is setup correctly . What type of PR is this ? /kind bug What this PR does / why we need it : This PR fixes stackdriver logging for windows node pool . Without this the stackdriver setup fails inconsistently . Which issue(s ) this PR fixes : Special notes for your reviewer : The start up sequence of windows script is being refined . When looking for workload logs in stackdriver console , we discovered this issue . So by moving the block to after HNS setup , we improve the chances of correct stackdriver startup by many fold . Does this PR introduce a user-facing change ? : No	1	0
2 2 1.6 2.0 1.4 1.5 1.4 1.5 1.6666666666666667 2.0 1.5588235294117647 34 2.0 3 26 62 309	WithAuthentication should wrap WithMaxInFlightLimit . What type of PR is this ? /kind bug What this PR does / why we need it : This PR moves the WithMaxInFlightLimit later in insecure handler chain . Which issue(s ) this PR fixes : Fixes #81861 : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	2	0
2 2 1.6 2.0 1.6 2.0 1.45 1.5 1.6666666666666667 2.0 0.96 25 1.0 7 25 42 255	Stage or Remove the OpenStack Cloud Provider . What would you like to be added : As part of a long running effort to < URL > , we either deleted or staged the in-tree cloud providers in < URL > . OpenStack was not deleted or staged because of it's internal dep to : pkg/util/mount . . We should either continue refactoring work for : pkg/util/mount . so we can stage the open stack cloud provider as well or just remove it . Why is this needed : See KEP < URL > .	2	1
2 2 0.8 0.0 0.5 0.0 0.6 0.5 1.3333333333333333 2.0 0.0 0 0.0 8 24 59 178	Automated cherry pick of #72682 : Add `metrics-port` to kube-proxy cmd flags . . Cherry pick of #72682 on release- 1.12 . 72682 : Add : metrics-port . to kube-proxy cmd flags .	1	0
1 1 1.0 1.0 1.0 1.0 1.05 1.0 1.0 1.0 0.6666666666666666 54 1.0 15 31 75 283	Add Queue Sort extension point to the scheduling framework . What would you like to be added : The < URL > is now implementable . We have < URL > plugin interfaces and extension points for ' < URL > ' and ' < URL > ' plugins . A ' Queue Sort ' plugin defines the ordering of pods in the scheduling queue . Today , the scheduler sorts pods by their priority . While this makes sense for most use-cases , some users may have different scheduling requirements that need a different sorting logic . A ' Queue Sort ' plugin allows users to customize ordering of pods . A queue sort plugin essentially will provide a ' less(pod1 , pod2 ) bool ' function . Only one queue sort plugin may be enabled at a time . /sig scheduling /priority important-soon ref/ kubernetes/enhancements #624	1	1
1 1 1.4 1.0 1.4 1.5 1.25 1.0 1.3333333333333333 1.0 1.5714285714285714 7 2.0 10 28 59 277	Bugfix : fix chan leak when stop error . What type of PR is this ? /kind bug What this PR does / why we need it : fix chan leak when stop error Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE .	2	0
1 1 1.0 1.0 1.1 1.0 1.05 1.0 1.3333333333333333 1.0 0.0 0 0.0 14 21 62 153	Fix nodeport repair for ESIPP services . What this PR does / why we need it : The nodeport allocation repair controller does not scrape the : Service . Spec . healthCheckNodePort . value and would remove the allocation from memory and etcd after 10 minutes . This opens the door for other services to use the same nodeport and cause collisions . Which issue(s ) this PR fixes : Fixes #54885 Similar to #64349 Release note : : Fix issue of colliding nodePorts when the cluster has services with externalTrafficPolicy = Local .	2	0
0 0 1.2 2.0 1.2 1.0 1.05 1.0 1.3333333333333333 2.0 0.5714285714285714 14 0.5 16 35 60 101	 kubeadm : use client-go's MakeCSRFromTemplate () in ' renew ' . What type of PR is this ? /kind bug What this PR does / why we need it : Create CSR using the mentioned function which also encodes the type CertificateRequestBlockType . Without that ' certs renew ' is failing with : ' PEM block type must be CERTIFICATE REQUEST ' Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes kubernetes/kubeadm #1216 Special notes for your reviewer : NONE Does this PR introduce a user-facing change ? : : kubeadm : fix a bug in ' certs renew ' related to unknown certificate requests . /area kubeadm /assign @liztio @timothysc @kubernetes /sig-cluster-lifecycle-pr-reviews /priority critical-urgent	0	0
1 1 1.0 1.0 1.0 1.0 1.15 1.0 1.3333333333333333 1.0 2.0 5 2.0 2 4 7 61	Terminating pod status is not refreshed correctly . Using Kubernetes 1.1.2 with Vagrant provider , running two pods : : . /cluster/ kubectl.sh get po NAME READY STATUS RESTARTS AGE mysql-pod 1/1 Running 0 21s wildfly-rc-l2cto 1/1 Running 0 21s . One of the pods was deleted : : . /cluster/ kubectl.sh delete po wildfly-rc-l2cto pod ' wildfly-rc-l2cto ' deleted . Watching the status of pods are shown as : : . /cluster/ kubectl.sh get -w po NAME READY STATUS RESTARTS AGE mysql-pod 1/1 Running 0 1m wildfly-rc-2o8vd 1/1 Running 0 13s wildfly-rc-l2cto 1/1 Terminating 0 1m NAME READY STATUS RESTARTS AGE wildfly-rc-l2cto 0/1 Terminating 0 1m wildfly-rc-l2cto 0/1 Terminating 0 1m wildfly-rc-l2cto 0/1 Terminating 0 1m . Two issues : - Refreshed status shows the only for the changed pod and shows it three times - Even after waiting for 5 minutes , the status does not refresh to Terminated Just checking the status as : kubectl.sh get po . shows that the pod has been terminated . But its confusing that with : -w . the status never updates to Terminated or something intuitive .	2	0
0 0 1.0 1.0 0.9 1.0 0.95 1.0 1.0 1.0 2.0 1 2.0 0 5 10 24	 Kubelet in standalone doesn't work with example YAML files . I'm trying containervm with kubelet built from head and this YAML file : < URL > The YAML file is v1beta2 and doesn't set a name to the Pod . When starting kubelet , it doesn't start the ' nc ' echo server and complains about this on the logs : : config . go : 307 ] Pod[1 ] ( < empty-name > . url-a75326da ) from http failed validation , ignoring : name : required value '' . Chatting with @dchen1107 on IM she mentioned it might be due to a change from @thockin that now requires non-empty pod names ( replaces random generation with validation for non-emptiness ) but that means all existing YAML files ( even the ones with an older version ) will stop working ... Is that expected ? Thanks ! Filipe	0	0
1 1 1.2 1.0 1.1 1.0 1.1 1.0 1.0 1.0 1.25 4 1.0 10 29 53 227	Improve fake clientset performance . What type of PR is this ? /kind bug What this PR does / why we need it : The fake clientset used a slice to store each kind of objects , it's quite slow to init the clientset with massive objects because it checked existence of an object by traversing all objects before adding it , which leads to O(n^2 ) time complexity . Also , the Create , Update , Get , Delete methods needs to traverse all objects , which affects the time statistic of code that calls them . This patch changed to use a map to store each kind of objects , reduced the time complexity of initializing clientset to O(n ) and the Create , Update , Get , Delete to O(1 ) . For example : Before this patch , it took ~ 29s to init a clientset with 30000 Pods , and 2 ~ 4ms to create and get an Pod . After this patch , it took ~ 50ms to init a clientset with 30000 Pods , and tens of ç¢Œs to create and get an Pod . Which issue(s ) this PR fixes : Fixes #89574 Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NONE . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : .	2	0
1 1 1.2 1.0 1.0 1.0 1.0 1.0 1.6666666666666667 2.0 1.1071428571428572 28 1.0 4 14 56 244	[ Scheduler ] Expand the internal range of score to [ 0 , 100 ] . /sig scheduling /priority important-soon /assign What would you like to be added : We plan to update the internal score range to [ 0 , 100 ] . Why is this needed : < URL > enough . Now that we are transitioning to the framework , we can consider expanding the range . We'll add two constant into the framework interface and multiply the original score by 10 : < URL > < URL > This is an internal change and compatible ( forward and backwards ) the developers won't feel it . See also : < URL > which try to introduce the change . < URL > cc/ @bsalamat @ahg -g @Huang -Wei	1	1
0 0 0.2 0.0 0.2 0.0 0.35 0.0 0.3333333333333333 0.0 0.8461538461538461 13 1.0 0 1 24 145	Not validating front proxy CA Key when using External CA . . What this PR does / why we need it : ' That the front ca key is not required as the front proxy client tls keypair can be managed by the third party . ' This PR don't validate the front CA Key but check if it already exists . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : Fixes < URL > Special notes for your reviewer : @yanndegat @timothysc @stealthybox @fabriziopandini Release note : : NONE .	1	0
1 1 0.8 1.0 0.9 1.0 0.75 1.0 0.6666666666666666 1.0 1.5961538461538463 52 2.0 18 41 84 243	 Automated cherry pick of #52569 upstream release 1.8 . : Kube-proxy adds forward rules to ensure NodePorts work . Backport of #52569 @tmjd xref #39823	0	0
2 2 1.2 1.0 1.2 1.0 1.3 1.5 1.0 1.0 1.8 5 2.0 3 9 17 67	 Links in docs to releases.k8s.io/ ... pages are just redirecting to kubernetes.io landing page . I'm seeing this on Chrome on OS X . All requests to releases.k8s.io are just getting a 302 and redirecting to the kubernetes.io landing page , rather than into the docs , regardless of whether I use HTTP or HTTPS . For example , try any of the 3 links in the < URL > . @RichieEscarez @lavalamp @brendandburns 	0	2
1 1 1.4 1.0 1.6 2.0 1.6 2.0 1.6666666666666667 2.0 1.5 4 1.5 5 11 12 60	Document the different flavor of updates and when to use them . replace , patch , apply , ... There used to be an issue , filed as a documentation issue , called something like ' explain the different kinds of updates and when to use them ' but I can't find it . @jackgr @bgrant0607 	2	2
1 1 1.4 1.0 1.0 1.0 1.1 1.0 1.6666666666666667 2.0 2.0 2 2.0 14 16 55 255	E2E Proposal : Verify tcpSocket property of v1 . Probe along with success and failure threshold . . What would you like to be added : A E2E to verify : core . v1 . tcpSocketAction . resource along with probe's important properties like : successThreshold . , : failureThreshold . , and : periodSeconds . . Note : following default values are in effect : delay = 0s timeout = 1s period = 10s #success = 1 #failure =3 . Why is this needed : To improve API coverage based on Kind/Resources and their important properties . < URL > E2E behavior could be : Create a pod with a container having : tcpSocket . based readiness and liveness probes provided with successThreshold , failureThreshold and periodSeconds values and container should not accept any connection after ~ 30 sec of it's start . Container MUST be ready after : successThreshold * periodSeconds . (<~ 30s ) is reached . Container MUST NOT be in ready state after : failureThreshold * periodSeconds . (>~ 30s ) is reached . Note : May need to implement connection closing mechanisam at certain time for : gcr.io/kubernetes-e2e-test-images/liveness:1.1 . image to test failureThreshold for : tcp . requests . /area conformance /sig testing @kubernetes /sig-node-feature-requests	2	1
1 1 1.0 1.0 1.1 1.0 0.8 1.0 1.0 1.0 0.8405797101449275 69 1.0 18 61 100 282	Automated cherry pick of #74715 : add Azure Container Registry anonymous repo support . Cherry pick of #74715 on release- 1.13 . 74715 : add Azure Container Registry anonymous repo support	1	0
2 2 1.0 1.0 0.9 1.0 0.7 0.5 1.0 1.0 0.875 8 1.0 9 31 77 142	Add condition ' len(cfg . DiscoveryToken ) ! = 0 ' to ValidateArgSelection . . What this PR does / why we need it : as per < URL > only when the conditions having len(cfg . DiscoveryToken ) ! = 0 means ' using token-based discovery ' as is mentioned in the error message . Which issue(s ) this PR fixes ( optional , in : fixes # < issue number > ( , fixes # < issue_number > , ... ) . format , will close the issue(s ) when PR gets merged ) : ref : #54982 Special notes for your reviewer : /cc @kubernetes /sig-cluster-lifecycle-pr-reviews Release note : : NONE .	1	0
2 2 1.0 1.0 1.1 1.0 1.15 1.0 0.6666666666666666 0.0 0.0 0 0.0 8 29 70 134	allow ELB Healthcheck configuration via Service annotations . What this PR does / why we need it : The default settings which are set on the ELB HC work well but there are cases when it would be better to tweak its parameters -- for example , faster detection of unhealthy backends . This PR makes it possible to override any of the healthcheck's parameters via annotations on the Service , with the exception of the Target setting which continues to be inferred from the Service's spec . Release note : : It is now possible to override the healthcheck parameters for AWS ELBs via annotations on the corresponding service . The new annotations are `healthy-threshold` , `unhealthy-threshold` , `timeout` , `interval` ( all prefixed with ` service.beta.kubernetes.io/aws-load-balancer-healthcheck-`) . incorrectly	0	1
1 1 1.0 1.0 0.8 1.0 0.7 1.0 1.0 1.0 0.84375 32 1.0 9 24 59 317	Automated cherry pick of #81856 : Convert tbe e2e to integration test #84036 : Ensure TaintBasedEviction int test not rely on #84766 : Fix a TaintBasedEviction integration test flake #84883 : Update test logic to simulate NodeReady/False and . Cherry pick of #81856 #84036 #84766 #84883 on release- 1.16 . 81856 : Convert tbe e2e to integration test 84036 : Ensure TaintBasedEviction int test not rely on 84766 : Fix a TaintBasedEviction integration test flake 84883 : Update test logic to simulate NodeReady/False and For details on the cherry pick process , see the < URL > page . Part of #85515 .	1	0
0 0 0.4 0.0 0.4 0.0 0.55 1.0 0.0 0.0 0.8333333333333334 18 1.0 10 25 51 257	  Automated cherry pick of #79349 : printer : fix a nil pointer dereference . Cherry pick of #79349 on release- 1.15 . 79349 : printer : fix a nil pointer dereference	0	0
0 0 0.2 0.0 0.1 0.0 0.15 0.0 0.3333333333333333 0.0 1.0833333333333333 12 1.0 16 31 98 196	 Bump Cluster Autoscaler to 1.1.2 . Contains fixes around GPUs and base image change . : Cluster Autoscaler 1.1.2 - release notes : < URL > .	0	0
1 1 1.2 1.0 1.4 1.5 1.3 1.5 1.6666666666666667 2.0 1.0 2 1.0 1 4 5 38	Break out kubectl as a separate download from the server components . : gcloud . already ships it separately . The user cases here differ dramatically . ref #28435 	1	2
2 2 0.6 0.0 0.4 0.0 0.45 0.0 1.0 1.0 2.0 1 2.0 13 31 68 276	 Patch glbc manifest to use version 1.0.0 . Also add rate limiting flags . Will also add a release note to the 1.10 google doc as well . Fixes : #61305 /assign @bowei /cc @nicksardo Release Note : : Bump ingress-gce image in glbc . manifest to 1.0.0 .	0	0
2 2 1.6 2.0 1.6 2.0 1.4 1.5 1.6666666666666667 2.0 0.5 6 0.5 15 31 84 296	Doc changes for nodelocaldns graduating to beta . What type of PR is this ? Uncomment only one : /kind <> . line , hit enter to put that in a new line , and remove leading whitespaces from that line : /kind api-change /kind bug /kind cleanup /kind design /kind documentation /kind failing-test /kind feature /kind flake What this PR does / why we need it : Documentation about nodelocaldns graduating to Beta . Which issue(s ) this PR fixes : Fixes # Special notes for your reviewer : Does this PR introduce a user-facing change ? : : NodeLocal DNSCache graduating to beta . . 	1	2
