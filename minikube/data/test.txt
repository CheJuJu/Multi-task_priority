1 1 1.2 1.0 0.6 0.5 1.0 1.0 1.3333333333333333 1.0 1.0952380952380953 84 1.0 6 15 26 57	docs : links to netowkring is broken . here < URL > the link < URL > is broken addtitioanlly all of the links in the website , that links to our own website should be changed to this format : more information on < URL > instead of the full url < URL >    	1	2
0 0 0.6 1.0 0.6 0.5 0.9 1.0 0.6666666666666666 1.0 0.0 1 0.0 0 0 0 0	Support upgrading minikube VM with contents from a newer ISO . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): feature request There should be a feature for upgrading the minikube-iso without deleting k8s state .	2	1
2 2 1.2 2.0 1.0 1.0 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 2 3 27	minikube tunnel doesn't appear to support UDP LoadBalancers . Steps to reproduce the issue : minikube start -- vm-driver = docker -- memory 8192 -- cpus 6 kubectl apply -f : kind : Service apiVersion : v1 metadata : name : udp spec : ports : - name : nginx-udp protocol : UDP port : 11500 targetPort : 11500 type : LoadBalancer . minikube tunnel : 閴?minikube tunnel 棣冨籍 Starting tunnel for service nginx-udp . . sudo lsof -i -P -n | grep 11500 : ssh 14238 user 5u IPv6 0xe5c1af091405c8ef 0t0 TCP [: : 1 ]: 11500 ( LISTEN ) ssh 14238 user 6u IPv4 0xe5c1af092c227f77 0t0 TCP 127.0.0.1 : 11500 ( LISTEN ) . As observed here , the opened tunnel is TCP and not UDP which means I'm unable to route UDP traffic to my nginx load balancer .	2	1
2 2 1.2 1.0 1.1 1.0 0.9 1.0 1.0 1.0 1.1875 16 1.5 1 5 7 13	Hyper-V : Stable integration testing . Not sure what the blockers are here . There were some Jenkins communication issues .  	1	1
2 2 1.2 2.0 1.0 1.0 0.95 1.0 1.3333333333333333 2.0 0.8217391304347826 230 1.0 1 2 3 21	discourge and add warnning for vbox driver if there are other healthier options available . some users might have set config to use vbox driver in the past and minikube respects it and wont change to better drivers in these cases we should show a Box UI warnining that there are better optionns avilable and here are the commandns to use it  	1	1
1 1 1.2 1.0 1.3 1.0 1.25 1.0 0.6666666666666666 1.0 2.0 1 2.0 1 2 5 16	CRI-O systemd unit not picking up /etc/sysconfig/crio . minikube and thus not setting -- insecure-registry . Steps to reproduce the issue : Started thus : : minikube start / -- vm-driver = kvm2 / -- container-runtime = cri-o / -- cni = bridge / -- addons = registry / -- cpus = 4 / -- service-cluster-ip-range = 10.96.0.0 /12 . Now , do : minikube ssh . and : : sudo systemctl status crio . You will see that the command used is : /usr/bin/crio -- log-level = debug . . However , this is incomplete . Looking at : /etc/systemd/system/multi-user . target . wants/crio . service . you will see that it refers to : /etc/sysconfig/crio . minikube . , which importantly has extra arguments for : -- insecure-registry . . However , it seems that : /etc/sysconfig/crio . minikube . was not used . This breaks the registry add-on out of the box . As a workaround , do a quick : : sudo systemctl restart crio sudo systemctl status crio . And you will see that : -- insecure-registry . has now been set properly . My guess is that somehow the ' crio . minikube ' file was create too late during the setup process .  	1	0
1 1 0.8 1.0 1.1 1.0 1.15 1.0 0.3333333333333333 0.0 1.0 1 1.0 2 2 6 18	Add binary size diff to PR performance bot . . Related : #5398	2	1
0 0 1.4 2.0 1.2 1.0 1.1 1.0 1.3333333333333333 2.0 1.56 25 2.0 0 3 8 21	add gopogh for prettifying windows logs . simmilar to what I did for linux in common.sh < URL > to get pretty html integration test logs we need need to do follow for windows scripts : - Pipe the output of tests to a file tests . out - Generate : test . json . output using : go tool test2json . - install gopogh v 0.0.17 using go get - run gopogh -in : test . json . -o ' test . html ' ( add other optional args ) - upload to gcs ...	2	1
0 0 0.8 1.0 1.2 1.5 1.0 1.0 1.0 1.0 1.1521739130434783 46 1.0 1 2 8 79	Add root or sudo requirements for the KIC drivers to the documentation . Currently we just refer to the upstream documentation , but don't mention the extra step needed : < URL > and < URL > : $ sudo apt-get install docker-ce docker-ce-cli containerd.io . < URL > and < URL > : $ sudo apt-get -qq -y install podman . Then it will fail on the first : minikube start . instead , mentioning what needed to be done before . : 閴?' docker ' driver reported an issue : ' docker version -- format {{ . Server . Version }}' exit status 1 : Got permission denied while trying to connect to the Docker daemon socket at unix :// /var/run/docker . sock : Get < URL > dial unix /var/run/docker . sock : connect : permission denied 棣冩寱 Suggestion : Add your user to the ' docker ' group : ' sudo usermod -aG docker $USER && newgrp docker ' 棣冩憣 Documentation : < URL > . : 閴?' podman ' driver reported an issue : ' sudo -n podman version -- format {{ . Version }}' exit status 1 : sudo : a password is required 棣冩寱 Suggestion : Add your user to the ' sudoers ' file : ' $USER ALL =( ALL ) NOPASSWD : /usr/local/bin/podman ' 棣冩憣 Documentation : < URL > . So we might as well add those steps to the documentation , in the first place ? Note that : sudo docker . and : sudo podman . do work fine , but with a password prompt ... We should also take care to mention the security implications of removing the password . It means that anything can run : docker . and : podman . commands - not only minikube .  	2	2
1 1 0.4 0.0 1.0 1.0 1.3 1.5 0.3333333333333333 0.0 0.0 0 0.0 0 0 6 32	dashboard : Could not decode config /home/user/ . minikube/config/config . json : EOF . : user @ubu18041 : ~ $ minikube status host : Running kubelet : Running apiserver : Running kubectl : Correctly Configured : pointing to minikube-vm at 192.168.39.226 user @ubu18041 : ~ $ . : user @ubu18041 : ~ $ minikube dashboard - Enabling dashboard ... ! Unable to enable dashboard : Could not decode config /home/user/ . minikube/config/config . json : EOF . I am running Ubuntu 18.04.1	2	0
0 0 0.8 1.0 0.8 1.0 0.95 1.0 1.0 1.0 0.0 0 0.0 0 2 3 26	How to enable auditing ? . The exact command to reproduce the issue : : minikube start / -- vm-driver hyperkit / -- extra-config = apiserver . authorization-mode = RBAC / -- extra-config = apiserver . Audit . LogOptions . Path = /var/log/apiserver/audit . log / -- extra-config = apiserver . Audit . PolicyFile = /etc/kubernetes/addons/audit-policy . yaml . I tried several extra-config variants : - apiserver . Audit . LogOptions . Path , apiserver . Audit . PolicyFile - apiserver . audit-logoptions-path , apiserver . audit-policyfile - apiserver . audit-log-options-path , apiserver . audit-policy-file - apiserver . audit-policy-path , apiserver . audit-log-dir , apiserver . audit-log-max-age together with different values : - log options path / log path / path - /var/log/kube-apiserver-audit . log - /etc/kubernetes/logs/apiserver-audit . log - log dir - /var/log/kubernetes/ - policy path - /etc/kubernetes/addons/audit-policy . yaml I tried also : - : -- feature-gates = Auditing = true . - : -- feature-gates = AdvancedAuditing = true . It does not matter how many CPUs or memory I give , minikube does not start in any case :( The full output of the command that failed : Please see all attached files The output of the : minikube logs . command : No output because minikube does not start . The operating system version : MacBook Pro - macOS Mojave 10.14.5 Minikube v 1.1.1 I already had a look at the issue #1609 , but it doesn't helped :( I attached all attempts I did . If someone has ideas or can help it would be great ! Thanks guys !! < URL > < URL > < URL > < URL > < URL > < URL > < URL > < URL > < URL > < URL >	2	2
1 1 1.0 1.0 0.8 0.5 0.75 0.5 1.0 1.0 0.9661016949152542 59 1.0 1 2 6 27	Save info logs to ~ / . minikube/logs by default . : ~ / . minikube/logs . is created , but never used , which is a shame . It would be nice if switching to klog ( #5318 ) or some other PR address this so that we can always tell users to send us logs from their most recent run .  	1	0
0 0 0.8 1.0 1.1 1.0 1.05 1.0 0.3333333333333333 0.0 1.0458715596330275 109 1.0 5 9 20 82	ingress on kubernetes docs is broken . following the docs on linux with KVM2 driver . < URL > ( the docs has a few syntax errorr too but after fixing them ) curl hello-world.info would just hang .	0	0
1 1 1.8 2.0 1.7 2.0 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 2 7 31	Consider adding JSON/key value pair output to docker-env command . We use python to do a good amount of automation at my shop . When iterating locally it would be nice to be able to have a cross platform/language agnostic output of the : docker-env . command that a tool written in a non-shell script language could use . : $ minikube docker-env You can further specify your shell with either ' cmd ' or ' powershell ' with the -- shell flag . SET DOCKER_TLS_VERIFY = 1 SET DOCKER_HOST = tcp :/ / 127.0.0.1 : 61041 SET DOCKER_CERT_PATH = C : /Users/user/ . minikube/certs SET MINIKUBE_ACTIVE_DOCKERD = minikube REM To point your shell to minikube's docker-daemon , run : REM @FOR /f ' tokens=*' %i IN (' minikube -p minikube docker-env ' ) DO @%i . Ideally something like this : : $ minikube docker-env -- output-kvp { ' DOCKER_TLS_VERIFY ' : 1 , ' DOCKER_HOST ' : ' tcp :/ / 127.0.0.1 : 61041 ' , ' DOCKER_CERT_PATH ' : ' C :/ /Users//user// . minikube//certs ' , ' MINIKUBE_ACTIVE_DOCKERD ' : ' minikube ' } . I realize this could be done with regex libraries in the language of choice but having a convenience function would be fantastic .	2	1
0 0 1.0 1.0 0.9 0.5 0.85 1.0 1.0 1.0 0.0 0 0.0 1 3 9 30	Improve and standardize low-fi output prefixes ( MINIKUBE_IN_STYLE = 0 ) . Moved from < URL > The change to logging has cluttered output with worthless characters , or even worse emoji . Please add an environment variable to shut this off . Example : : $ minikube start -- container-runtime = cri-o -- cache-images o minikube v 0.34.1 on linux ( amd64 ) $ Caching images in the background ... > Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... - ' minikube ' IP address is 192.168.99.100 - Configuring CRI-O as the container runtime ... - Preparing Kubernetes environment ... : Waiting for image caching to complete ... - Pulling images required by Kubernetes v 1.13.3 ... - Launching Kubernetes v 1.13.3 using kubeadm ... - Configuring cluster permissions ... - Verifying component health ..... + kubectl is now configured to use ' minikube ' = Done ! Thank you for using minikube ! . should be : : $ export MINIKUBE_REASONABLE_OUTPUT = true $ minikube start -- container-runtime = cri-o -- cache-images minikube v 0.34.1 on linux ( amd64 ) Caching images in the background ... Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... ' minikube ' IP address is 192.168.99.100 Configuring CRI-O as the container runtime ... Preparing Kubernetes environment ... Waiting for image caching to complete ... Pulling images required by Kubernetes v 1.13.3 ... Launching Kubernetes v 1.13.3 using kubeadm ... Configuring cluster permissions ... Verifying component health ..... kubectl is now configured to use ' minikube ' Done ! Thank you for using minikube ! . These symbols provide no useful information and clutter the output .	0	1
2 2 1.4 2.0 1.3 1.5 1.3 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 4 5 27	[ FEATURE ] allow to set container-runtime via config . I know there is a < URL > : minikube start -- container-runtime = containerd . for start to use containerd but I would like to save the option to persistent config in hy $HOME directory along with other options I use all the time like cpu , memory , kubenertes-version , and vm-driver The exact command to reproduce the issue : : minikube config set container-runtime containerd . The full output of the command that failed : 棣冩寴 Set failed : property name ' container-runtime ' not found 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL >	0	1
1 1 1.0 1.0 1.1 1.0 1.1 1.0 1.3333333333333333 1.0 1.1229508196721312 122 1.0 0 1 4 18	The check for memory limits is currently broken for cgroups v1 . Currently it checks ' ( empty string ) , if cgroup2 is false : : // HasMemoryCgroup checks whether it is possible to set memory limit for cgroup . func HasMemoryCgroup () bool { memcg : = true if runtime . GOOS = = ' linux ' { var memory string if cgroup2 , err : = IsCgroup2UnifiedMode (); err = = nil && cgroup2 { memory = ' /sys/fs/cgroup/memory/memsw . limit_in_bytes ' } if _ , err : = os . Stat(memory ); os . IsNotExist(err ) { klog . Warning('Your kernel does not support memory limit capabilities or the cgroup is not mounted . ' ) memcg = false } } return memcg } . Since 7b0bf57f4c9b87a7b0c9362bdfbb5c0add2094d7 Most likely the code is also wrong for cgroups v2 as well ? The end result is no memory limits , for the docker driver . Seems to be the same issue as in #10935  	1	0
1 1 1.2 1.0 1.0 1.0 1.1 1.0 1.3333333333333333 1.0 0.3333333333333333 9 0.0 2 2 6 25	  Create public charts for time-to-k8s . Create a GitHub Action workflow that runs time-to-k8s benchmark daily against HEAD Store the results of the job as JSON and upload it to bucket Use the data in the bucket to generate a chart and upload it to the website	0	2
0 0 0.6 0.0 1.0 1.0 1.0 1.0 0.3333333333333333 0.0 1.135135135135135 74 1.0 0 2 7 22	Supply package cache for minikube container runtimes . Since we can't rely on the vendor packages being available , we should make sure to host our own package cache ? < URL > This would mostly include the container runtimes , but also other packages we install that are not in the distribution . Docker ( docker.com, < URL > | < URL > docker-ce docker-ce-client containerd.io Podman ( opensuse.org, < URL > podman cri-o conmon CRI cri-tools CNI plugins There are distribution packages available for docker/containerd/runc , but we're not happy about the versions ... ~ ~ Currently using a personal package account on bintray.io ( for podman and crio ) , but that is not maintainable ... ~ ~ ~ < URL > ~ ( 1.9.3 ) ~ < URL > ~ ( 1.18.2 ) We probably also need to document and automate the packaging building better , currenly using : pbuilder . . For the Docker packages we don't know if we can build them from source code at all ( that would be Moby ) Compare < URL >	2	1
0 0 1.0 1.0 0.9 1.0 0.7 0.5 0.6666666666666666 1.0 1.2222222222222223 9 1.0 0 6 7 28	Running containers with podman doesn't work . We install the : podman . tool into the VM , to do things such as loading or building OCI images . But currently there are some issues with the CNI configuration , if you try to use it for running : : $ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_) | |/') _ _ | |_ __ /' _ ` _ `/| |/' _ `/| || , < ( ) ( )| ' _`/ /'__`/ | ( ) ( ) || || ( ) || || |/`/ | (_) || |_) )( ___/ (_) (_) (_) (_) (_) (_) (_) (_) (_) `/___/'(_ , __/'`/____) $ sudo podman run -it busybox Trying to pull docker.io/library/busybox ... Getting image source signatures Copying blob 53071b97a884 done Copying config 64f5d945ef done Writing manifest to image destination Storing signatures Error : error parsing CNI plugin result ' IP4 :{ IP :{ IP : 10.1.0.7 Mask : ffff0000 } Gateway : 10.1.0.1 Routes :[{ Dst :{ IP : 0.0.0.0 Mask : 00000000 } GW : 10.1.0.1 }]} , DNS :{ Nameservers :[ ] Domain : Search :[ ] Options:[]}': cannot convert version [' ' 0.1.0 ' ' 0.2.0 ' ] to 0.4.0 : cannot convert version [' ' 0.1.0 ' ' 0.2.0 ' ] to 0.4.0 . Even though it is not the primary use-case ( normally use crictl/crio ) , this should work ... Maybe we can get the standard minikube CNI configuration compatible with Podman ?	2	0
2 2 1.0 1.0 0.8 0.5 1.0 1.0 0.6666666666666666 0.0 0.65 20 0.0 0 1 2 22	  Add explanations for all minikube error codes . We now populate documentation for all error/exit codes at < URL > but most of them don't have a comment explaining what they are . We should add those .	0	2
2 2 0.6 0.0 0.9 1.0 1.0 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 1 5 21	PersistentVolume claim doesn't work inside a namespace . System Info : Ubuntu 16.04.3 LTS kubectl client v 1.13.2 kubectl server v 1.13.4 minikube version : v 0.33.1 . Steps to reproduce create a namespace and then try to use a persistentvolumeclaim within that namespace Reproduced using < URL > : minikube start -- vm-driver none -- kubernetes-version v 1.13.4 -- memory 5120 -- cpus = 4 kubectl create namespace testing kubectl create -n testing -f < URL > kubectl create -n testing -f < URL > kubectl describe -n testing pod/cassandra-0 . Expected Result : it creates the persistentvolume and it gets assigned Actual Result : : Events : Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 26s ( x7 over 26s ) default-scheduler pod has unbound immediate PersistentVolumeClaims Normal Scheduled 26s default-scheduler Successfully assigned testing/cassandra-0 to minikube Normal Pulling 19s ( x2 over 25s ) kubelet , minikube pulling image ' gcr.io/google-samples/cassandra:v13 ' Normal Pulled 19s ( x2 over 25s ) kubelet , minikube Successfully pulled image ' gcr.io/google-samples/cassandra:v13 ' Normal Created 19s ( x2 over 25s ) kubelet , minikube Created container Normal Started 19s ( x2 over 25s ) kubelet , minikube Started container Warning BackOff 14s kubelet , minikube Back-off restarting failed container .	2	0
0 0 0.6 0.0 0.8 0.5 1.2 1.0 1.0 1.0 1.1774193548387097 62 1.0 3 10 21 55	handle more error type for oci . when docker is restarting ... and minikube starts we error this way : ' Error response from daemon : dial unix docker . raw . sock : connect : connection refused ' : make : `out/minikube ' is up to date . 棣冩 minikube v 1.7.3 on Darwin 10.13.6 閴?Using the docker ( experimental ) driver based on user configuration 閳跨媴绗?' docker ' driver reported an issue : exit status 1 棣冩寱 Suggestion : Docker is not running or is responding too slow . Try : restarting docker desktop . 棣冩暉 Creating Kubernetes in docker container with ( CPUs = 2 ) ( 0 available ) , Memory = 2000MB ( 0MB available ) ... 棣冩寴 Unable to start VM . Please investigate and run ' minikube delete ' if possible : creating host : create : creating : create kic node : creating volume for minikube container : output Error response from daemon : dial unix docker . raw . sock : connect : connection refused : exit status 1 .	0	0
2 2 1.2 1.0 1.1 1.0 1.0 1.0 1.0 1.0 0.0 0 0.0 4 8 8 25	Mac M1 : hyperkit not supported . . Mac M1 Steps to reproduce the issue : arch -x86_64 brew install hyperkit minikube start -- vm = true minikube start -- driver = hyperkit Full output of failed command : $ minikube start -- driver = hyperkit 棣冩 minikube v 1.21.0 on Darwin 11.4 ( arm64 ) 閴?Using the hyperkit driver based on user configuration 閴?Exiting due to DRV_UNSUPPORTED_OS : The driver ' hyperkit ' is not supported on darwin/arm64	2	1
2 2 1.8 2.0 1.4 2.0 1.2 1.5 1.6666666666666667 2.0 0.75 4 0.5 0 11 19 55	[ Bump-up ] VolumeSnapshot from v1beta1 to v1(GA ) . /area addons Minikube can use VolumeSnapshot as an addon . Its version is v1beta1 api version . < URL > In Kubernetes v 1.20 , the VolumeSnahot moved to GA(v1 ) . < URL > We need to upgrade volumesnapshot versions .	0	1
0 0 0.6 0.0 1.0 1.0 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 0 0 1	Support For Windows Containers . Feature Request Currently , I know that Minikube is a single node cluster , and that node is Linux so it wont be able to run Windows Containers , but it would be better if Minikube is able to run Windows Containers also by being able to add a Windows node .	2	1
0 0 1.2 1.0 1.0 1.0 1.0 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 2 2 15	minikube start -- image-repository with port does not work without scheme . Related to PR < URL > which was release on v 1.23.0 Steps to reproduce the issue : 1 . Run command : minikube start -- image-repository < repository > : < port > . for example : : minkube start -- image-repository repository : 8080 . 2 . Observe the output : The -- image-repository flag your provided contains Scheme : repository , which will be removed automatically . and on the next line `Using image repository ' without the repository . If I run : minkube start -- image-repository < URL > . it complains about the scheme but trims it out and works with : repository : 8080 . . Running minikube without port also works without the schema . I'll try to have by the EOD a PR to address this issue .	0	0
0 0 0.6 0.0 0.9 1.0 0.9 1.0 0.3333333333333333 0.0 1.1627906976744187 43 1.0 3 8 19 80	Upgrade Buildroot to 2020.02 LTS . The current release is nearing EOL . We should upgrade to the next LTS . Notice that the 2019.02 . x series is now end of life unless somebody steps up to take over maintenance . Please migrate to the 2020.02 series instead which will be supported until April 2021 . ' Buildroot 2020.02 released ' < URL > ' Buildroot 2019.02.11 released , 2019.02 . x is EOL ' < URL > This will feature upgrade Linux to 5.4 ( ~ 4.19 ~ ) and systemd to 244 ( ~ 240 ~ ) Also upgrades Binutils to 2.32 ( ~ 2.30 ~ ) and GCC to 8.4.0 ( ~ 7.5.0 ~ ) and the system Go compiler is bumped from 1.11.13 to 1.13.14 Also need to upgrade VirtualBox , to get guest additions support for Linux 5.4	0	1
1 1 0.8 1.0 0.6 0.0 0.65 0.0 0.3333333333333333 0.0 1.0 2 1.0 0 1 4 35	Minikube status shows paused cluster as running . Steps to reproduce the issue : Pause the cluster . run the following commands in the following versions . : 閴?Downloads . /minikube12-2 status -- layout cluster -- output json {' Name ' : ' minikube ' , ' StatusCode ' : 418 , ' StatusName ' : ' Paused ' , ' BinaryVersion ' : ' v 1.12.2 ' , ' Components ' :{ ' kubeconfig ' :{ ' Name ' : ' kubeconfig ' , ' StatusCode ' : 200 , ' StatusName ' :'}} , ' Nodes ' :[{ ' Name ' : ' minikube ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' , ' Components ' :{ ' apiserver ' :{ ' Name ' : ' apiserver ' , ' StatusCode ' : 418 , ' StatusName ' : ' Paused ' } , ' kubelet ' :{ ' Name ' : ' kubelet ' , ' StatusCode ' : 405 , ' StatusName ' : ' Stopped ' }}}]} 閴?Downloads . /minikube13-1 status -- layout cluster -- output json {' Name ' : ' minikube ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' , ' BinaryVersion ' : ' v 1.13.1 ' , ' Components ' :{ ' kubeconfig ' :{ ' Name ' : ' kubeconfig ' , ' StatusCode ' : 200 , ' StatusName ' :'}} , ' Nodes ' :[{ ' Name ' : ' minikube ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' , ' Components ' :{ ' apiserver ' :{ ' Name ' : ' apiserver ' , ' StatusCode ' : 418 , ' StatusName ' : ' Paused ' } , ' kubelet ' :{ ' Name ' : ' kubelet ' , ' StatusCode ' : 405 , ' StatusName ' : ' Stopped ' }}}]} . As you can see , the most recent version does not correctly output the status as paused .	0	0
0 0 0.6 0.0 0.6 0.0 0.6 0.0 0.3333333333333333 0.0 0.0 0 0.0 0 4 7 30	UI progress bars stepping on each other . I noticed that : preloaded-images-k8s . and : index.docker.io/kicbase . progress bars step on each other when download is in progress and it is confusing for the user and might be interpreted as a glitch . When the download is finished , they are displayed in separate lines : : minikube start -- driver = docker 棣冩 minikube v 1.19.0 on Darwin 10.15.7 閴?Using the docker driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩 Pulling base image ... 棣冩崙 Downloading Kubernetes v 1.20.2 preload ... > gcr.io/k8s-minikube/kicbase ... : 357.67 MiB / 357.67 MiB 100.00% 396.74 K > preloaded-images-k8s-v10-v1 ... : 491.71 MiB / 491.71 MiB 100.00% 411.47 K > index.docker.io/kicbase/sta ... : 357.67 MiB / 357.67 MiB 100.00% 417.22 K .	2	0
0 0 0.4 0.0 1.0 1.0 1.2 1.5 0.3333333333333333 0.0 1.1041666666666667 96 1.0 5 8 20 69	feature : add doc URL to the addon list table . while reviwing this PR : < URL > since the URL will be different based on driver , it would be nice that if we have add the URL of the addon to the list , so they dont have to dig into the documentaiton and pain to find the URL for the regsiry : medmac@ ~ /Desktop/md1 $ . /minikube-darwin-amd64 addons list | -----------------------------|---------- | -------------- | | ADDON NAME | PROFILE | STATUS | | -----------------------------|---------- | -------------- | | dashboard | minikube | disabled | | default-storageclass | minikube | enabled 閴?| | efk | minikube | disabled | | freshpod | minikube | disabled | | gvisor | minikube | disabled | | helm-tiller | minikube | disabled | | ingress | minikube | disabled | | ingress-dns | minikube | disabled | | istio | minikube | disabled | | istio-provisioner | minikube | disabled | | logviewer | minikube | disabled | | metrics-server | minikube | disabled | | nvidia-driver-installer | minikube | disabled | | nvidia-gpu-device-plugin | minikube | disabled | | registry | minikube | enabled 閴?| | registry-aliases | minikube | disabled | | registry-creds | minikube | disabled | | storage-provisioner | minikube | enabled 閴?| | storage-provisioner-gluster | minikube | disabled | | -----------------------------|---------- | -------------- | .  	2	2
2 2 0.8 0.0 1.3 2.0 0.95 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 5 10 31	Minikube keeps on reporting ' Unable to fetch image ' from remote server while I have the image set up locally . Steps to reproduce the issue : I'm testing with this repo : < URL > And what I did was : Configure docker to use minkube's registry by : : eval $(minikube -p minikube docker-env ) . Build the image with : docker build -t docker.io/test/helloworld-python . . Now if I run : : minikube image list . I will see : docker.io/test/helloworld-python:latest in the list . I have to update the yaml file accordingly by making the lines : : - image : docker.io/test/helloworld-python:latest imagePullPolicy : Never . But when I deploy the service with : : kubectl create -f service . yaml . I keep on getting the same error from the log : : Unable to fetch image ' docker.io/test/helloworld-python:latest ' : failed to resolve image to digest : HEAD < URL > unexpected status code 401 Unauthorized ( HEAD responses have no body , use GET for details ) . Is there a document somewhere that details how local registery works with minkube ? There seems to be scattered information all over the interent but not sure about the official steps . Below is the version I use : : minikube version : v 1.19.0 commit : 15cede53bdc5fe242228853e737333b09d4336b5 . And I'm using Ubuntu 20.04 kernel 5.4.0 -72-generic Full output of : minikube logs . command : Full output of failed command :	2	0
0 0 1.4 2.0 1.5 2.0 1.0 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 0 5 28	minikube doesn't start in VM with no virtualisation support . I'm not sure if this is a bug or a feature request . We have available corporate VMs which have virtualisation support in the ( virtual ) CPU disabled . I have tried for a week to get minikube with docker running , and each time I fix one blocker , there's another waiting . This should be easy . :-) But it's not . Please include this scenario in the test sets for minikube . I'm working on Ubuntu 16.04 , with the latest ( as of yesterday ) minikube .  	2	2
0 0 1.0 1.0 1.2 1.5 1.25 1.5 0.6666666666666666 0.0 1.0 7 1.0 0 0 0 11	hyperkit : `minikube ip` shows an IP even if VM is stopped . As reported on #minikube by @dhs227 . It's noteworthy that : status . shows that minikube is stopped : : [ ~ /c/m/hellonode ] : minikube status 10:40:28 PM minikube : Stopped cluster : kubectl : .	2	0
0 0 1.0 1.0 1.0 1.0 0.95 1.0 1.0 1.0 0.0 0 0.0 2 3 5 22	Documentation : change registries title from ' docker on macOS ' to ' docker on Linux/macOS ' . Isn't it worth mentioning something about Linux ? I managed to make the private registry work on my Ubuntu 20.04 machine by following the steps listed in ' docker on macOS ' . Maybe it would be interesting to change that title to ' Docker on Linux/macOS ' . That said , I haven't followed the whole handbook . I just read the ' Registries ' page .  	2	2
2 2 1.6 2.0 1.4 1.5 1.4 2.0 1.6666666666666667 2.0 0.9807692307692307 52 1.0 1 3 7 31	Add -- dry-run flag to surface configuration warnings . For embedding within an API , it would be handy to have a dry run mode to surface issues a user may have with the current configuration . For example : Warning if the user will need sudo permissions : minikube start -- dry-run . Resource warnings ( RAM , CPU ) Basically , anything that would normally cause : minikube start . to exit due to an invalid configuration should do so and output why - and do so quickly .   	1	1
0 0 1.2 2.0 1.2 2.0 1.15 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 0 1 5	Unable to enable AppArmor in minikube ( works in minishift ) . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : Both Mac and Ubuntu What happened : I tried from minikube 0.28 - 0.23 , they all don't work After I : minikube start . a kubernetes cluster . the apparmor should be enabled by default . But after I : minikube ssh . to the node and check , the apparmor is not installed and enabled . And when I deploy any kube app , then I : kube exec . to access the pod of the kube app , I also cannot see the apparmor is enabled in the pod . But I tried minishift today , they enable by default . I also found we can enable it like this : < URL > But I tried : : minikube start -- feature-gates = AppArmor = true . and : minikube start -- feature-gates AppArmor = true . , they all don't work . What you expected to happen : after : minikube start . , the apparmor should be enabled by default . Or can you please tell me how to enable : apparmor . for my minikube ? Thanks a lot !	2	0
1 1 1.8 2.0 1.8 2.0 1.45 2.0 1.6666666666666667 2.0 1.0 1 1.0 1 3 8 30	Feature request : Allow customizing host path used for dynamically provisioned PersistentVolumes . Dynamic persistent volume provisioning works great , but uses paths like : /tmp/hostpath-provisioner/pvc-433c40c9-7cf1-40a8-91ab-f9210dceec50 . which are not easy to remember and type . For development and testing , it would be much more convenient if the path could be customized to something along the lines of : : /data/ < claim > . This would make a PVC named : mysqldb . resolve to an automatically generated PV that stores the data in : /data/mysqldb . . Much easier to remember and type ! :)	2	1
0 0 0.2 0.0 0.9 0.5 0.75 0.0 0.3333333333333333 0.0 0.9032258064516129 186 1.0 3 14 18 42	support ancient versions of docker again ( template parsing issue ) . we broke it since 16.01 the ancient verions of docker 18.09.7 fails at template parse : cmd : = exec . Command(Docker , ' network ' , ' inspect ' , name , ' -- format ' , `{'Name ' : ' {{ . Name }}' , ' Driver ' : ' {{ . Driver }}' , ' Subnet ' : ' {{ range . IPAM . Config }}{{ . Subnet}}{{end }}' , ' Gateway ' : ' {{ range . IPAM . Config }}{{ . Gateway}}{{end }}' , ' MTU ' : {{ if ( index . Options ' com . docker . network . driver . mtu')}}{{(index . Options ' com . docker . network . driver . mtu')}}{{else}}0{{end }} , {{$first : = true }} ' ContainerIPs ' : [{{ range $k , $v : = . Containers }}{{ if $first}}{{$first = false}}{{else }} , {{ end}}'{{$v . IPv4Address}}'{{end}}]}`) . errors out : : Template parsing error : template : : 1 : unexpected ' = ' in operand .	0	0
2 2 1.6 2.0 1.1 1.0 1.05 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 1 2 26	MountVolume . SetUp failed for volume : couldn't propagate object cache : timed out waiting for the condition . < I have a problem after exposing an image in kubernetes . As soon as I checked the minikube dashboard an error occurred . Later on when I back to minikube dashboard later it was working fine without error . > kubectl expose deployment hello-node -- type = LoadBalancer -- port = 8080 : from minikube dashboard this error was found on the pod for the image I exposed : MountVolume . SetUp failed for volume ' defaul t-t oken-kd9zk ' : couldn't propagate object cache : timed out waiting for the condition minikube version : v 1.2.0 ubuntu 18.04	2	0
1 1 1.4 2.0 1.1 1.5 1.0 1.0 1.6666666666666667 2.0 1.1111111111111112 9 1.0 2 11 19 57	Minikube ( re)start on docker driver breaks docker-env . : minikube start -p docker3 -- container-runtime docker -- vm-driver = docker . Restarting caused some issue with the IP ( minikube docker-env command ) that was working before - this is a skaffold log output : : ' docker load ' command failed with error : error during connect : Post < URL > EOF . : minikube version 6.7 s 椤?Thu Feb 27 10:59:00 2020 minikube version : v 1.7.3 commit : 436667c819c324e35d7e839f8116b968a2d0a3ff . Originally posted by @balopat in < URL >	0	0
2 2 1.6 2.0 1.5 2.0 1.25 1.5 2.0 2.0 0.0 0 0.0 1 1 2 17	CRI-O config (/etc/crio/crio . conf ) should not be hardcoded ( nor overwritten without a prompt ) . With : minikube start -- vm-driver = none -- container-runtime = crio . , minikube attempts to overwrite any pre-existing CRI-O configuration at : /etc/crio/crio . conf . with a hardcoded template . First , this has a potential for user data loss ( if the user has a nontrivial configuration in place ) . Second , this makes minikube/crio unusable on any systems that do not adhere to the hardcoded paths ( e . g . Arch Linux , which has : /usr/bin/conmon . and : /usr/lib/cni . instead of : /usr/libexec/crio/conmon . and : /opt/cni/bin . ) . Finally , this makes it impossible to use any container runtime other than : crun . ( which is often the motivation to use CRI-O in the first place ) .  	1	0
0 0 1.0 1.0 1.4 2.0 1.35 2.0 0.3333333333333333 0.0 1.15 40 1.0 4 12 17 50	The podman driver should not require sudo or root . Even though podman is run with sudo ( docker uses a group instead ) , the driver should not . : 閴?Using the podman ( experimental ) driver based on user configuration 棣冩寴 The ' podman ' driver requires root privileges . Please run minikube using ' sudo minikube -- driver = podman ' . . This will lead to the same kind of ownership and path issues that is plaguing the none driver ... Instead only the : podman . commands should be wrapped in : sudo . ( to not try to run as rootless )  	1	0
2 2 0.8 0.0 0.9 0.5 0.9 0.5 1.3333333333333333 2.0 0.0 2 0.0 0 0 5 24	docker-machine-driver-kvm2_ 1.22.0 -0_arm64 . deb has wrong package architecture . Steps to reproduce the issue : Download : docker-machine-driver-kvm2_ 1.22.0 -0_arm64 . deb . from the v 1.22.0 release Try to install it on an arm64 system , or ( my initial case ) try to add it to an apt repo that hosts arm64 packages Errors out because the package internally says that it's : aarch64 . when it should be : arm64 . The : minikube . package correctly has the : arm64 . architecture . Full output of : minikube logs . command : N/A Full output of failed command : : Error looking at ' docker-machine-driver-kvm2_ 1.22.0 -0_arm64 . deb ' : ' aarch64 ' is not one of the valid architectures : ' i386 amd64 armhf arm64 source ' . : new Debian package , version 2.0 . size 4649436 bytes : control archive = 423 bytes . 446 bytes , 12 lines control Package : docker-machine-driver-kvm2 Version : 1.22.0 Section : base Priority : optional Architecture : aarch64 Depends : libvirt0 (>= 1.3.1 ) Recommends : minikube Maintainer : Thomas Str鏋歮berg < t+minikube@stromberg.org > Description : Machine driver for KVM minikube uses Docker Machine to manage the Kubernetes VM so it benefits from the driver plugin architecture that Docker Machine uses to provide a consistent way to manage various VM providers . .	0	0
1 1 1.6 2.0 1.2 1.0 1.0 1.0 1.6666666666666667 2.0 0.0 0 0.0 1 3 9 35	Minikube image zsh completion broken . The : minikube image . tab completion on ZSH doesn't work : : $ minikube image < tab here > _minikube_image : 10 : bad math expression : operand expected at `'ls''tion-name: zsh : do you wish to see all 195 possibilities ( 98 lines ) ? . For some reason , part of the previously added image name is part of the error message ( : application-name . )  	1	0
2 2 1.6 2.0 1.6 2.0 1.4 2.0 1.3333333333333333 2.0 0.0 0 0.0 3 5 6 33	VM starts but the download of `preloaded-images-k8s` was not completed . ( This is not a regular issue , but a support request ) Steps to reproduce the issue : : minikube start minikube delete minikube start . 棣冩 minikube v 1.19.0 on Darwin 10.14.6 閳?MINIKUBE_ACTIVE_DOCKERD = minikube 閴?Using the virtualbox driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩崙 Downloading Kubernetes v 1.20.2 preload ... > preloaded-images-k8s-v10-v1 ... : 308.70 MiB / 491.71 MiB 62.78% 84.05 KiB 棣冩暉 Creating virtualbox VM ( CPUs =3 , Memory = 5120MB , Disk = 20000MB ) ... ... Questions : Does Minikube has any way to avoid re-downloading the : preloaded-images-k8s . , every time that a VM is created ? Where does Minikube stores the : preloaded-images-k8s . ? / So I can manually copy it before : delete . and restore it before : start . . I am having connectivity problems and I would like to reuse the images that were already downloaded on the first execution of : minikube start . , when re-creating the Virtual Machine . Is there any way to achieve this ?	2	0
0 0 1.4 2.0 1.2 1.5 0.95 1.0 1.0 1.0 0.0 1 0.0 1 2 4 18	  Broken hyperlinks at < URL > . < URL > lists methods for uploading images : 1 . docker-env command which is a hyperlink to < URL > 2 . podman-env command , which is a hyperlink to < URL > . and so on These and the following hyperlinks do not work . The correct hyperlinks would have been < URL > < URL > < URL >	0	2
2 2 0.8 0.0 1.0 1.0 0.9 1.0 1.3333333333333333 2.0 1.2105263157894737 38 1.0 4 8 11 47	update-context with missing entry : panic : runtime error : invalid memory address or nil pointer dereference . minikube 1.9.2 , ubuntu 16.04 , virtualbox : $ minikube status E0405 16:02:32 . 312506 22446 status . go : 233 ] kubeconfig endpoint : extract IP : ' minikube ' does not appear in /home/anders/ . kube/config m01 host : Running kubelet : Running apiserver : Running kubeconfig : Misconfigured WARNING : Your kubectl is pointing to stale minikube-vm . To fix the kubectl context , run `minikube update-context` $ minikube update-context panic : runtime error : invalid memory address or nil pointer dereference [ signal SIGSEGV : segmentation violation code = 0x1 addr = 0x18 pc = 0x148ecb0 ] goroutine 1 [ running ]: k8s.io/minikube/pkg/minikube/kubeconfig.UpdateEndpoint(0x1a823d1 , 0x8 , 0xc00050a1b0 , 0xe , 0x20fb , 0xc0000437e0 , 0x19 , 0x1d82160 , 0xc00012c060 , 0xc00050a1b0 ) /app/pkg/minikube/kubeconfig/kubeconfig . go : 122 +0x290 k8s.io/minikube/cmd/minikube/cmd.glob .. func30(0x2a6e180 , 0x2aa53b0 , 0x0 , 0x0 ) /app/cmd/minikube/cmd/update-context . go : 37 +0x10a github.com/spf13/cobra.(* Command ) . execute(0x2a6e180 , 0x2aa53b0 , 0x0 , 0x0 , 0x2a6e180 , 0x2aa53b0 ) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:830 +0x2aa github.com/spf13/cobra.(* Command ) . ExecuteC(0x2a6c380 , 0x0 , 0x1 , 0xc00040f500 ) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:914 +0x2fb github.com/spf13/cobra.(* Command ) . Execute (...) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:864 k8s.io/minikube/cmd/minikube/cmd.Execute () /app/cmd/minikube/cmd/root . go : 108 +0x6a4 main . main () /app/cmd/minikube/main . go : 66 +0xea .	0	0
0 0 0.4 0.0 0.8 0.5 0.95 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 3 13	multi-nodes-minikube loss node label after restart . Steps to reproduce the issue : minikube start -n 2 kubectl label node minikube-m02 nsid_public = open -- overwrite minikube stop minikube start kubectl get node -A -o wide -- show-labels : kubectl get node -A -o wide -- show-labels NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME LABELS minikube Ready control-plane , master 29d v 1.20.2 192.168.49.2 < none > Ubuntu 20.04.2 LTS 4.14.81 . bm . 26-amd64 docker :/ / 20.10.6 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=c61663e942ec43b20e8e70839dcca52e44cd85ae,minikube.k8s.io/name=minikube,minikube.k8s.io/updated_at=2021_05_20T13_05_08_0700,minikube.k8s.io/version=v1.20.0,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master= minikube-m02 Ready < none > 9d v 1.20.2 192.168.49.3 < none > Ubuntu 20.04.2 LTS 4.14.81 . bm . 26-amd64 docker :/ / 20.10.6 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube-m02,kubernetes.io/os=linux .  	1	0
0 0 1.0 1.0 1.1 1.0 0.85 1.0 0.6666666666666666 0.0 1.2432432432432432 37 1.0 3 4 6 43	Not possible to translate new start message . The string ' control plane ' is hard-coded : : cp : = ' if apiServer { cp = ' control plane ' } out . T(out . ThumbsUp , ' Starting {{ . controlPlane}}node {{ . name }} in cluster {{ . cluster }}' , out . V{'controlPlane ' : cp , ' name ' : n . Name , ' cluster ' : cc . Name }) . So it comes out as a mix with english : : 棣冩啢 Startar control plane nod m01 i kluster minikube . ( on a side note , maybe the names ' m01 ' and ' minikube ' should be in quotes ? ) Unfortunately , the translation framework strips out any trailing spaces in strings . So if you try to wrap it in : translate . T . , the string comes out as : ' control plane ' . Probably the best is to use two separate strings , even if it is a little redundant . : if apiServer { out . T(out . ThumbsUp , ' Starting control plane node {{ . name }} in cluster {{ . cluster }}' , out . V{'name ' : n . Name , ' cluster ' : cc . Name }) } else { out . T(out . ThumbsUp , ' Starting node {{ . name }} in cluster {{ . cluster }}' , out . V{'name ' : n . Name , ' cluster ' : cc . Name }) } .	0	0
0 0 1.0 1.0 1.2 1.5 1.0 1.0 0.6666666666666666 0.0 0.0 2 0.0 0 1 5 19	Implement -- force-systemd into cri-o . Minikube version 1.23.1 does not START work with crio and podman , because crio does not switch to systemd cgroup manager . According to < URL > : minikube start -- force-systemd . and : minikube start -- force-systemd = true . is supposed to enforce systemd閳ユ獨 cgroup manager and be workaround for < URL > However , : minikube start -- force-systemd = true . does not start , which means that cgroup is not enforced .	0	1
0 0 1.0 1.0 0.8 0.5 0.9 1.0 1.3333333333333333 2.0 0.0 0 0.0 5 8 18 67	Podman is OOM-killed when loading images . Steps to reproduce the issue : : minikube start -- container-runtime = crio -- disk-size = 100GB -- memory = 3900m -- driver = kvm2 . ( on minikube VM ) : watch -d ' df -h ' . : podman save < image > | minikube ssh ' sudo podman load ' . Repeat , tmpfs mounted on / fills , eventually podman will be killed inside the minikube VM Maybe I'm misunderstanding how the CRI-O driver works , but it seems like : podman . is using : /var/lib/containers/storage . as the GraphRoot instead of : /mnt/vda1/var/lib/containers/storage . . Full output of : minikube start . command used , if not already included : : 棣冩 minikube v 1.9.2 on Ubuntu 18.04 閴?Using the kvm2 driver based on user configuration 棣冩啢 Starting control plane node m01 in cluster minikube 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 3900MB , Disk = 102400MB ) ... 棣冨返 Preparing Kubernetes v 1.18.0 on CRI-O 1.17.1 ... 棣冨皞 Enabling addons : default-storageclass , storage-provisioner 棣冨及 Done ! kubectl is now configured to use ' minikube ' . dmesg output : : [ 340.592651 ] Out of memory : Kill process 10107 ( podman ) score 9 or sacrifice child [ 340.592696 ] Killed process 10107 ( podman ) total-vm : 830324kB , anon-rss : 7212kB , file-rss : 4kB , shmem-rss : 28888kB [ 340.594962 ] oom_reaper : reaped process 10107 ( podman ) , now anon-rss : 0kB , file-rss : 0kB , shmem-rss : 0kB .   	1	0
1 1 1.4 2.0 1.3 1.5 1.2 1.0 1.0 1.0 0.0 0 0.0 1 3 10 56	Using a proxy with containerd runtime . Now I use the default container runtime ( : docker . ) in minikube . And at the start , I specify the option to use a proxy : : minikube start -- docker-env HTTP_PROXY = < my proxy > . But what about using : containerd . ? How can I specify the environment setting in this case ? : minikube start -- container-runtime = containerd . In production we are already using a containerd . Therefore , I need to use it in minikube too . I only found < URL > when it is used without minikube .	2	1
0 0 0.6 1.0 1.0 1.0 1.15 1.0 0.6666666666666666 1.0 1.0 2 1.0 0 0 0 2	Create installer to remove the necessity for users to download additional binaries . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Feature Request The current installation process for minikube is : Download and install a hypervisor driver ( kvm , xhyve , hyper-v ) Set up hypervisor driver ( setup qemu-kvm , root own xhyve driver , set switches on hyper-v ) Download and install minikube We've tried in the past to vendor the drivers directly - unfortunately there were two issues : xhyve driver - must have root permissions , calls the hyperkit CLI which os . Exits kvm driver - dynamically linked against libvirt.so, which can't be statically linked The feature request is that we change the installation process to include the non-virtualbox drivers for each OS , xhyve and kvm . The extraction script will extract and set up the drivers correctly . Then , the installation process would be Download and extract minikube . tar Run script to set up hypervisor driver Of course , package managers could automate this process .  	1	1
2 2 1.6 2.0 1.4 2.0 1.45 2.0 2.0 2.0 0.0 0 0.0 0 0 0 1	Proposal : Suggest NFS as default filesystem for local dev . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Please provide the following details : Environment : Minikube version : v 0.21.0 - OS : OS X - VM Driver : Virtualbox - ISO version : minikube-v 0.23.0 . iso - Install tools : brew - Others : What happened : We noticed our application ran significantly slower in k8s/minikube compared to vagrant . We changed our local files to mount via NFS instead of 9p and our dev system is now much faster than vagrant . What you expected to happen : We should update the docs to recommend using NFS for local dev instead of 9p , at least for OS X , but probably makes sense on linux , too . We're going to test on Windows shortly to see if it has the same effect which I assume it will . How to reproduce it ( as minimally and precisely as possible ): Set up a persistent volume store using NFS and use it instead of 9p : 1 ) Update your : /etc/exports . to allow minikube to access it : : $ echo ' /Users -alldirs -mapall='$(id -u )': ' $(id -g )' 192.168.99.100 ' | sudo tee -a /etc/exports . : apiVersion : v1 kind : PersistentVolume metadata : name : nfs-volume spec : capacity : storage : 15Gi accessModes : - ReadWriteMany persistentVolumeReclaimPolicy : Retain storageClassName : standard nfs : server : 192.168.99.1 path : /Users/Shared/Sites/ --- kind : PersistentVolumeClaim apiVersion : v1 metadata : name : cr-volume spec : storageClassName : standard accessModes : - ReadWriteMany resources : requests : storage : 15Gi . Anything else do we need to know : If you think this is a good idea , I'll make a PR for the update docs . It's been life changing for our devs .  	2	2
0 0 0.6 0.0 1.1 1.0 1.2 1.5 0.6666666666666666 0.0 0.0 0 0.0 0 1 15 45	minikube kubect : permission denied . After upgrading to minikube v . 1.7.2 I'm not longer able to use : minikube kubectl . . The exact command to reproduce the issue : : rm -rf ~ / . minikube minikube start minikube kubectl . The full output of the command that failed : : Error running /Users/ < USER > / . minikube/cache/v 1.17.2 /kubectl : fork/exec /Users/ < USER > / . minikube/cache/v 1.17.2 /kubectl : permission denied . Contents of the : ~ / . minikube/cache/v 1.17.2 /kubectl . : : ls -l ~ / . minikube/cache/v 1.17.2 -rw ------- 1 < USER > staff 39342080 11 lut 10:27 kubeadm -rw ------- 1 < USER > staff 43491328 11 lut 10:27 kubectl -rw ------- 1 < USER > staff 111568408 11 lut 10:27 kubelet . : file ~ / . minikube/cache/v 1.17.2 /kubectl /Users/ < USER > / . minikube/cache/v 1.17.2 /kubectl : ELF 64-bit LSB executable , x86-64 , version 1 ( SYSV ) , statically linked , Go BuildID = itoADCJwxOm7z3ymMN2Z/yoeLs_MYMz8Q5D0z_bWX/K9sqErNfMu6XJcqYVzr4/nHZFbmivkI7eipSyzj5i , stripped . The operating system version : macOS Catalina 10.15.3 ( 19D76 ) It looks like minikube downloaded executables for Linux operating system , where it should download executable to macOS . Currently the workaround is to delete files from : ~ / . minikube/cache/v 1.17.2 . folder and execute : minikube kubectl . which will download correct executables for macOS .	0	0
1 1 1.2 1.0 1.3 1.5 1.3 1.5 1.3333333333333333 1.0 0.0 0 0.0 2 5 9 30	none : reusing node : detecting provisioner : Too many retries waiting for SSH to be available . Environment : minikube version : : v 1.0.0 . OS : : Ubuntu 16.04 LTS ( Xenial Xerus ) . VM Driver : : none . What happened : ``` Created a VM with none driver , stopped it , then started it again . The VM failed to start and minikube reported that it crashed . : What I expected to happen : the VM created by the first minikube start command is started . Output from the second minikube start command : 棣冩 minikube v 1.0.0 on linux ( amd64 ) 棣冦仚 Downloading Kubernetes v 1.14.0 images in the background ... 棣冩寱 Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Restarting existing none VM for ' minikube ' ... 閳?Waiting for SSH access ... 棣冩寴 Unable to start VM : detecting provisioner : Too many retries waiting for SSH to be available . Last error : Maximum number of retries ( 60 ) exceeded 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > Output from ' sudo minikube start -- alsologtostderr -v = 8 -- vm-driver = none ' : 閳?Waiting for SSH access ... Waiting for SSH to be available ... Getting to WaitForSSH function ... Error getting ssh command ' exit 0 ' : driver does not support ssh commands . To reproduce : sudo minikube start -- vm-driver = none sudo minikube stop sudo minikube start -- vm-driver = none Starting a stopped VM was working in minikube v 0.28 .  	1	0
2 2 1.4 2.0 1.1 1.0 1.0 1.0 1.3333333333333333 2.0 1.1333333333333333 120 1.0 1 3 5 32	  Provide links back to ' Production environment ' , when finished with ' Learning environment ' . Currently the Kubernetes documentation is divided into : < URL > Learning environment < URL > kubectl kind minikube + < URL > ( Katacoda ) kubeadm Production environment < URL > kubeadm kops ( AWS ) kubespray ( Ansible ) cloud We should provide some exit links , how to proceed to production ... Note : these are external resources , outside minikube Too many to mention : < URL > This would be an improvement over trying to run minikube in production . < URL > ( also ~ ~ #10097 ~~) Non-Goals Simplifying Kubernetes production deployment experience Linking back to the regular Kubernetes documentation should be enough .   	1	2
0 0 1.4 2.0 1.5 2.0 1.4 2.0 1.0 1.0 0.0 0 0.0 0 1 1 9	Failed to create symbolic link on mount : Operation not permitted . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : OSX Minikube version ( use : minikube version . ): v 0.28.2 - OS ( e.g. from /etc/os-release ): Mac OS X High Sierra - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): hyperkit - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): . minikube/cache/iso/minikube-v 0.28.1 . iso - Install tools : What happened : ln : failed to create symbolic link ' test1 ' : Operation not permitted What you expected to happen : I would expect the symbolic link to create without a problem . How to reproduce it ( as minimally and precisely as possible ): : docker run -- rm -it / -- volume ${PWD }: /usr/src/test / -- workdir /usr/src/test / node : latest / bash . : root @deca1a6f4779 : /usr/src/test# touch test root @deca1a6f4779 : /usr/src/test# ln -s test test1 ln : failed to create symbolic link ' test1 ' : Operation not permitted . Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	0
2 2 0.8 1.0 1.2 1.0 1.25 1.0 1.0 1.0 0.9548872180451128 133 1.0 2 4 13 26	DEB_REVISION is not propagated into . deb metadata . Are you mentioning the versions reported by : apt . / : dpkg . , or the one reported by : minikube version . ? The former -- the version info in the : . deb . package metadata . A quick check identifies the issue here : < URL > That should be : sed -E -i ' s/--VERSION--/'$(DEB_VERSION)-$(DEB_REVISION)'/g ' ... . Originally posted by @mgabeler -lee-6rs in < URL >	0	0
2 2 1.0 1.0 0.8 1.0 1.25 1.5 1.0 1.0 0.9520958083832335 167 1.0 3 9 12 43	make perfomrance pr-bot to use json output . for more information ask @priyawadhwa  	1	1
2 2 1.4 2.0 1.2 1.5 1.35 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 1 23	Exit with error when Docker version is too old . 10369 dropped the error in the Docker version validation to support old Docker versions . With this change , the users get a confusing : docker info -- format : exit status 2 . error when they're using ancient Docker versions ( < 1.13.0 ) that don't even support : docker info -- version . , see 11480 11538 Should we exit with an error and tell users to upgrade their Docker version when it's way too old ? One option is to define two constants : : minDockerVer . ( e.g. 1.13.0 ) and : recommendedDockerVer . ( 18.09.0 ): 1 . If user Docker version < : minDockerVer . : Exit with error message 2 . If user Docker version < : recommendedDockerVer . : Only show suggestions to upgrade Docker 3 . If user Docker version > = : recommendedDockerVer . : Do nothing  	1	1
1 1 1.0 1.0 1.5 2.0 1.3 1.5 0.6666666666666666 1.0 1.2222222222222223 9 2.0 2 3 3 11	macOS Install User Journey .  	2	2
0 0 1.4 2.0 1.3 2.0 1.2 2.0 1.0 1.0 2.0 1 2.0 1 1 2 6	Retain folder structure when copying files from ~ / . minikube/files . Environment : Minikube version : v 0.26.1 OS : Ubuntu 16.04 VM Driver : virtualbox ISO version : v 0.26.0 What happened : When Minikube copies files onto the VM from ~ / . minikube/files , the folder structure is not retained . What you expected to happen : On the host : : ~ / . minikube/files$ find . | sort . . /abc . /abc/1 . txt . /def . /def/2 . txt . /def/ghi . /def/ghi/3 . txt . However , when the VM is provisioned , all the files are put onto the first directory with content . : $ minikube ssh find /abc /abc /abc/3 . txt /abc/2 . txt /abc/1 . txt . I expect the copied files to match the structure on my host . How to reproduce it : 1 . Create a nested folder structure in : ~ / . minikube/files . with some files . : cd ~ / . minikube/files mkdir -p { abc , def/ghi } touch abc/1 . txt touch def/2 . txt touch def/ghi/3 . txt . Start Minikube with : minikube start -- kubernetes-version v 1.10.0 -- bootstrapper = kubeadm . . After completion , check the VM for the files with : minikube ssh find /abc . .	2	1
2 2 1.4 2.0 1.4 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 9 24	-- image-mirror-country='cn ' still fetches binary SHA's from Google . Already used : -- image-mirror-country='cn ' . or : -- image-repository . to start minikube , but still failed in the preparing on Docker stage due to fetching sha files from google ap	2	0
2 2 1.6 2.0 1.6 2.0 1.4 2.0 1.3333333333333333 2.0 0.0 0 0.0 2 2 5 42	Add command for a ' Getting Started ' workshop using eduk8s as addon . /kind feature Hello , I was thinking that it would be nice having a katacoda-like tutorial for beginners for local Kubernetes with Minikube , available from cli command and implemented as addon . One good candidate for this could be creating an addon for < URL > , a workshop framework installable via Operator and providing CR for creating interactive content , containing the < URL > 1h workshop ( or creating a new one ) . The flow would be similar to : minikube dashboard . command which activates dashboad addon , in this case it would be : : $ minikube getstarted . or : $ minikube tutorial . activating : eduk8s . addon or : getstarted . addon ( containing eduk8s + workshop ) , opening the browser pointing to eduk8s ingress . This would help people that'd love to learn Kubernetes doing it from their workstation with Minikube , having so a local persistent env were to execute exercises , and taking benefit from storage and ingress components already available , needed by the workshop framework in this case . How does that sound ?	2	1
0 0 1.0 1.0 1.0 1.0 1.25 1.5 0.3333333333333333 0.0 0.45454545454545453 22 0.0 3 5 9 23	Add validatePodmanEnv subtest for crio runtime to TestFunctional . This goes hand in hand with validateDockerEnv which currently runs for the docker runtime  	1	1
2 2 1.0 1.0 1.0 1.0 1.15 1.0 1.6666666666666667 2.0 2.0 1 2.0 2 6 8 40	unknown runtime type : ' nvidia ' . I have installed the nvidia-driver and nvidia-docker2 ( < URL > I modify the default-runtime in /etc/docker/daemon . json , : [ root @localhost ~ ]# cat /etc/docker/daemon . json { ' default-runtime ' : ' nvidia ' , ' runtimes ' : { ' nvidia ' : { ' path ' : ' nvidia-container-runtime ' , ' runtimeArgs ' : [] } } } . I want start minikube as runtime using nvidia , but it occurs an error : [ root @localhost ~ ]# minikube start -- container-runtime = nvidia o minikube v 0.35.0 on linux ( amd64 ) ! Failed to generate config : unknown runtime type : ' nvidia ' * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL > .	2	2
1 1 0.8 1.0 1.1 1.0 1.3 1.5 0.6666666666666666 1.0 1.2058823529411764 34 1.0 2 6 15 47	Default to bash shell when $SHELL is missing . The current message is somewhat confused : : $ unset SHELL $ minikube docker-env The default lines below are for a sh/bash shell , you can specify the shell you're using , with the -- shell flag . 棣冩寴 Error detecting shell : Error : Unknown shell 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > .	2	0
2 2 1.4 1.0 1.2 1.0 1.35 1.5 1.3333333333333333 1.0 0.0 0 0.0 2 7 17 37	open < user > . minikube/machines/minikube/config . json : The system cannot find the file specified . . Command : : minikube start . Returns o minikube v 0.35.0 on windows ( amd64 ) ! Unable to start VM : Error loading existing host . Please try running [ minikube delete ] , then run [ minikube start ] again . : filestore : open C : /Users/goodw . minikube/machines/minikube/config . json : The system cannot find the file specified . Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL > minikube delete ! Failed to delete cluster : open C : /Users/goodw . minikube/machines/minikube/config . json : The system cannot find the file specified . Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL > I've tried deleting the . minikube file manually and no luck . Says I require permission from DESKTOP-UTI3DC7/username to make changes to this folder . I'm assuming that's the VM ? It seems like minikube is in some half installed state and I would like to just start from scratch again . : minikube logs . returns ! api load : filestore : open C : /Users/goodw . minikube/machines/minikube/config . json : The system cannot find the file specified . Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL > Using windows 10 pro . I tried to start minikubes originally with : minikube start vm-driver = hyperv hyper v-v irtual-switch = my_network_switch . It seemed to have made the cluster but then hung . And then I couldn't run any minikube commands . Let me know if there is any other information I can provide .  	1	0
0 0 1.0 1.0 1.2 1.5 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 6 9 33	Minikube - Enable colors error message . Steps to reproduce the issue : Run ' minikube status ' The message ' Enable colors error ' shoud not be displayed in output . This issue occurs in releases 1.20 and 1.19 . Full output of : minikube logs . command : Full output of failed command :	2	0
2 2 1.4 1.0 1.1 1.0 1.2 1.0 1.6666666666666667 2.0 0.9416058394160584 137 1.0 3 4 16 55	logs command outputs lines slowly , takes > 45 seconds to run . I realized that on macOS , with minikube v 1.18.1 , the logs command runs so slowly that I can literally see the lines drawn individually . A ran it with the : time . command , and it can take over a minute to run . Here is output from my last , slightly faster run : : ________________________________________________________ Executed in 53.95 secs fish external usr time 0.99 secs 101.00 micros 0.99 secs sys time 1.08 secs 1410.00 micros 1.08 secs . It feels like the buffer is flushed between each line , but I don't see the CPU load to indicate that is in fact the case . We should throw a profiler at the : logs . command to better understand why it's taking so much time . To duplicate : : minikube start -- container-runtime = containerd . : minikube logs . As a comparison , this command runs in 1.2 seconds : : time minikube ssh ' sudo journalctl -u kubelet -- no-pager ' > /tmp/x .   	1	0
2 2 1.6 2.0 1.5 2.0 1.45 2.0 2.0 2.0 0.8193832599118943 227 1.0 0 3 3 22	add detailed info about what each integration test is doing to the site . currently our : make generate-docs . creates this page < URL > which generates the site based on the Comments on the Test and validate the func , I would like these descriptions to exaclty say what it is doing instead of a < URL > : validateBuildImage makes sures that minikube image build works as expected . it should say : validateBuildImage makes sures that `minikube image build` works as expected builds an image using `minikube image build` and the Dockerfile located in test/integration/testdata/build/Dockerfile which adds a relative file into a busybox container and then ensures the image was build using image inspect inside minikbue special case : in case of contained it starts the buildkit inside minikube currently skipped on None Driver and Github Actions on MacOs , .    	1	2
1 1 1.6 2.0 1.2 1.5 1.15 1.5 1.6666666666666667 2.0 1.1595744680851063 94 1.0 1 4 8 38	Memory requirements do not match kubeadm any longer ( since 1.20 ) . Minikube still has : : minUsableMem = 953 // 1GB In MiB : Kubernetes will not start with less minRecommendedMem = 1907 // 2GB In MiB : Warn at no lower than existing configurations . Kubeadm now has : : // Below that amount of RAM running a stable control plane would be difficult . ControlPlaneMem = 1700 . Since < URL > ( and < URL > from 2048M to 1700M ) : v 1.20.0 -alpha . 0-252-ge5bb66f8990 . : v 1.20.0 -alpha . 0-253-gc6975a77508 . Problem seen in #10014 #10071 : [ ERROR Mem ]: the system RAM ( 953 MB ) is less than the minimum 1700 MB . So we now need to require 2G more strict , and maybe suggest 4G ? Like suggestd in #8050 Alternatively try to run with : -- ignore-preflight-errors = Mem . , like we currently do for Swap ?  	1	0
2 2 1.4 2.0 1.6 2.0 1.3 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 0 3 15	sch_netem kernel module missing for network emulation ( tc netem ) . The exact command to reproduce the issue : Application based : Run a container like debian in priviledged mode in a pod and one pod that uses the < URL > network emulation linux tool , in my case < URL > . Pumba has no effect trying to mess with the network of the debian container . Pumba uses netem . modprobe : Try to run : minikube ssh . and then : modprobe -n sch_netem . , which says netem module is missing . With netem on a priviledged debian container in minikube : Get a bash in the debian container and try to modify the eth0 virtual interface by changing : qdisc . to : netem loss . ( which is essentially what pumba does ) . The command is : tc qdisc add dev eth0 root netem loss random 100% . . A related issue , and the environment in which I found out this is not working , is described < URL > . This is a minikube specific issue , as it works with other kubeadm based clusters and GKE and others . It also works locally on a laptop just with docker images . The full output of the command that failed : * : modprobe -n sch_netem . -> : modprobe : FATAL : Module sch_netem not found in directory /lib/modules/ 4.19.76 . * : tc qdisc add dev eth0 root netem loss random 100 . -> : invalid qdisc name . The operating system version : * 5.4.1 -2-MANJARO * minikube version : v 1.5.2 commit : 792dbf92a1de583fcee76f8791cff12e0c9440ad Additional info I just wanted to let you know this issue exists . Maybe we can do something about it , as minikube is a great environment for starting with chaos engineering , and not being able to mess with the network ( in this way ) is a big drawback . Also it took me quite some time to figure this out , as I had no access to other clusters :D .	2	1
2 2 1.8 2.0 1.6 2.0 1.35 2.0 1.6666666666666667 2.0 0.5 2 0.5 3 4 15 57	kic : custom kic/docker/osx message getting displayed for all drivers/runtimes . On linux using service : : minikube service hello-minikube1 | -----------|----------------- | -------------|------------------------- | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|----------------- | -------------|------------------------- | | default | hello-minikube1 | | < URL > | | -----------|----------------- | -------------|------------------------- | 棣冨竴 Opening service default/hello-minikube1 in default browser ... 閳跨媴绗?Because you are using docker driver on Mac , the terminal needs to be open to run it . . The warn message should be specific for kic/docker/osx .	0	0
1 1 1.4 1.0 1.6 2.0 1.1 1.0 1.3333333333333333 1.0 1.3333333333333333 3 1.0 2 5 7 19	Add start option , to download only . Make it possible to download files ahead-of-time , without actually install and loading them . : $ . minikube start -- cache-images -- download-only 棣冩 minikube v 0.34.1 on linux ( amd64 ) 棣冦仚 Caching images in the background ... 棣冩崚 Downloading Minikube ISO ... 184.30 MB / 184.30 MB [= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ==] 100.00% 0s 棣冩崙 Downloading kubeadm v 1.13.3 棣冩崙 Downloading kubelet v 1.13.3 閳?Waiting for image caching to complete ... 閴佹棑绗?Download complete and in download only mode . This will save the files for later in the local minikube cache , before actually starting the host .	2	1
2 2 1.4 2.0 1.1 1.5 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 3 4 19	Add option to skip VM driver PreCreateCheck . Environment : Windows 10 Version 10.0.16299.904 Minikube : v 0.34.1 The supported drivers provide a : PreCreateCheck () . method that is < URL > by minikube in order to ensure the driver works properly and its prerequisites are fulfiled . Unfortunately those prerequisites are not without flaws . The one implemented in the < URL > invokes some < URL > as part of its checks . The one listed before checks if the user is part of the Hyper-V Administrators group . While this check is a good idea , it does not work if the computer joined a domain and cannot reach its Domain Controller . Therefore , when e.g. working in Home Office or in the train without network connectivity at all , I cannot use minikube in this setup . Please add an option to skip ( or ignore failing ) : PreCreateCheck . s . The minishift project is doing < URL > specific to hyperv , but I'd propose to just add a general : I-know-what-I'm-doing . -option to continue when those checks failed .	2	1
0 0 0.6 0.0 1.2 1.5 0.9 1.0 0.0 0.0 0.7333333333333333 15 0.0 1 1 2 28	Allow minikube to run in a non-default docker host . : docker-machine create -- driver = virtualbox default eval $(docker-machine env default . This is will create a docker host inside of vbox VM , which currently causes minikube to fail , if there is no docker daemon running directly on a user's machine .  	1	1
2 2 1.2 1.0 1.6 2.0 1.45 2.0 1.0 1.0 0.0 0 0.0 1 9 12 57	add support for rootless docker . Hello , I am trying to run minikube using docker driver in rootless mode . But minikube is not able to detect the docker daemon . ' docker ' driver reported an issue : ' docker version -- format {{ . Server . Os}}-{{ . Server . Version }}' exit status 1 : Cannot connect to the Docker daemon at unix :// /var/run/docker . sock . Is the docker daemon running ? The docker host env is set to export DOCKER_HOST = unix :// /run/user/1000/docker . sock	2	1
0 0 0.4 0.0 0.8 0.5 0.85 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 4 17 45	minikube install uses winget on Windows . The fact that Windows Package Manager ( winget ) is still in preview , should be mentioned on the installation tab for Windows . Maybe hence winget also shouldn't be named as the first option to install minikube .  	2	2
1 1 0.8 1.0 0.6 0.0 0.85 1.0 0.6666666666666666 1.0 0.984375 64 1.0 7 10 12 31	Remove inaccurate documentation from kubernetes.io/ .  	2	2
0 0 0.8 0.0 0.7 0.0 0.95 1.0 0.0 0.0 0.0 0 0.0 2 2 4 30	rootless podman driver . Steps to reproduce the issue : : minikube start -- driver = podman . Full output of failed command : : 棣冩 minikube v 1.11.0 on Arch rolling 閳?KUBECONFIG = /path-to-my-config 閴?Using the podman ( experimental ) driver based on user configuration 閴?' podman ' driver reported an issue : ' sudo -k -n podman version -- format {{ . Version }}' exit status 1 : sudo : a password is required 棣冩寱 Suggestion : Add your user to the ' sudoers ' file : ' ejiek ALL =( ALL ) NOPASSWD : /usr/bin/podman ' 棣冩憣 Documentation : < URL > 棣冩寴 Failed to validate ' podman ' driver . This means that with podman driver user is obligated to have root access without password wich is not minimal provilage . < URL > to run podman rootless . < URL > is a bit of Arch wiki on configuring podman not to require root permissions . I suggest having a flag for this driver to either use sudo or not . There is a good chance that rootless podman is not suitable for minikube yet due to it's < URL > ( this is a linkg to a particular version of podman )	2	1
1 1 1.0 1.0 0.9 1.0 1.05 1.0 1.3333333333333333 1.0 0.0 0 0.0 2 2 5 21	VBOX_VERR_VMX_NO_VMX : Point users to better documentation . The exact command to reproduce the issue : The full output of the command that failed : The output of the : minikube logs . command : The operating system version :  	2	2
2 2 1.6 2.0 1.5 2.0 1.45 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 1 1 6	minikube start -- allow additional arguments to -- mount-string . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): I'm leaning towards bug report as I would expect : -- mount-string . to mimic : minikube mount . Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.32.0 - OS ( e.g. from /etc/os-release ): Windows 10 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): hyperv - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.32.0 - Install tools : What happened : Additional : mount . arguments passed into : -- mount-string . are ignored Ran : minikube start -- mount -- mount-string='C:/mydrive:/mydrive -- uid 911 -- gid 911 -- 9p-version = 9p2000 . L ' minikube ssh ls -l /mydrive . What you expected to happen : expected it to properly set ownership of : /mydrive . to 911 as a workaround to #2290 How to reproduce it ( as minimally and precisely as possible ): see what happened	2	0
0 0 0.4 0.0 0.8 0.5 0.95 1.0 0.0 0.0 0.0 0 0.0 0 5 12 40	Ability to add disks to the minikube VM on Hyperkit ? . While adding extra disks with the kvm2 driver is easy , I couldn't find anything to add disks on : hyperkit . . Anyone already doing this perhaps ? Thanks !	2	1
1 1 0.8 1.0 1.1 1.0 1.25 2.0 1.0 1.0 1.5 2 1.5 3 4 9 39	Provide more descriptive error codes for GUEST_START . GUEST_START serves as a catch-all error for cluster node failing to start . Can minikube provide more descriptive error codes ( is the failure owing to memory/cpu/disk constraints ) ?	0	1
1 1 0.8 1.0 0.9 1.0 1.0 1.0 1.0 1.0 0.6 15 0.0 1 4 5 19	Warn when /var is almost at memory capacity for VM drivers . Related to < URL >  	1	1
2 2 1.4 2.0 1.4 2.0 1.2 1.0 2.0 2.0 0.0 0 0.0 6 11 12 32	NO_PROXY doesn't respect addr/range - contrary to documentation . For whatever reason , NO_PROXY does not respect an addr/range argument in Ubuntu 16.04 . Therefore , 192.168.99.1 /24 will not make a no proxy exception for an address in that subnet In this case , what I found works is to replace it with a specific ip address of the minikube vm in NO_PROXY , which in my case was 192.168.99.100 . Please add this to the documentation , thanks !  	2	2
2 2 1.6 2.0 1.5 2.0 1.1 1.0 1.3333333333333333 2.0 1.1428571428571428 70 1.0 3 7 15 56	clarify ' m01 ' stopped in stop command . per slack conversation , < URL > the stop command shows . ' m01 ' stopped this could be confusing for the users . we could clarify . node ' m01 ' stopped	0	1
2 2 1.2 1.0 1.1 1.0 1.05 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 2 5 23	Question : no node-provided-ip annotation on node . Regarding to < URL > and < URL > I'm expecting to find the minikube external IP address in node annotation . : . kubernetes.io/provided-node-ip . Is it a bug ? or there is an option how to get it there ? The exact command to reproduce the issue : : kubectl describe node . The full output of the command that failed : : 閴?helm kubectl describe node Name : minikube Roles : master Labels : beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=minikube kubernetes.io/os=linux node-role.kubernetes.io/master= Annotations : kubeadm.alpha.kubernetes.io/cri-socket : /var/run/dockershim . sock node.alpha.kubernetes.io/ttl : 0 volumes.kubernetes.io/controller-managed-attach-detach : true CreationTimestamp : Fri , 05 Jul 2019 11:19:46 +0300 . The operating system version : OSX 閴?helm minikube version minikube version : v 1.2.0 OSX : 10.14.5 ( 18F132 )	2	1
0 0 0.8 0.0 0.8 0.5 1.1 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 0 5 8	Allow set `ImageRepository` for kubeadm init . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Please provide the following details : Environment : : minikube version : v 0.28.2 OS : cat : /etc/os-release : No such file or directory VM driver : ' DriverName ' : ' hyperkit ' , ISO version ' Boot2DockerURL ' : ' file :// /path/to/ . minikube/cache/iso/minikube-v 0.28.1 . iso ' , . Can the kubeadm bootstrapper adds a configuration option support for < URL > , which enables setting different image base repository . This feature will help developers who are having difficulties to connect to k8s.gcr.io ( namely , China ) , as long as they can set the repository prefix to self managed proxies .  	1	1
2 2 1.4 2.0 1.3 1.5 1.35 2.0 2.0 2.0 0.0 0 0.0 0 1 4 22	minikube config emits no such file config . json when setting container-runtime . The exact command to reproduce the issue : Below , setting configuration options for my minikube environment . Minikube is not running when performing these commands . ( tensor ) ~ > minikube config set cpus 6 閳跨媴绗?These changes will take effect upon a minikube delete and then a minikube start ( tensor ) ~ > minikube config set memory 8192 閳跨媴绗?These changes will take effect upon a minikube delete and then a minikube start ( tensor ) ~ > minikube config set container-runtime crio 棣冩寴 Set failed : [ config . Load : stat /Users/mack/ . minikube/profiles/minikube/config . json : no such file or directory ] 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The full output of the command that failed : 棣冩寴 Set failed : [ config . Load : stat /Users/mack/ . minikube/profiles/minikube/config . json : no such file or directory ] 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The output of the : minikube logs . command : ( tensor ) ~ > minikube logs 棣冩寴 Error getting config : stat /Users/mack/ . minikube/profiles/minikube/config . json : no such file or directory 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The operating system version : macOS 10.15.1 ( Catalina )	0	0
0 0 0.6 0.0 0.5 0.0 0.95 1.0 0.0 0.0 1.2857142857142858 7 1.0 1 2 6 29	Upgrade Docker to 18.09 ( and containerd ) . While Kubernetes still recommends * using 18.06 , some people are using the minikube Docker daemon for other purposes ( such as building ) and could benefit from upgrading to 18.09 . < URL > Version 18.06.2 is recommended , but 1.11 , 1.12 , 1.13 , 17.03 and 18.09 are known to work as well . Keep track of the latest verified Docker version in the Kubernetes release notes . < URL > The list of validated docker versions has changed . 1.11.1 and 1.12.1 have been removed . The current list is 1.13.1 , 17.03 , 17.06 , 17.09 , 18.06 , 18.09 . As a part of this upgrade , we can stop using : docker-runc . and : docker-containerd . / : docker-containerd-ctr . and instead use the regular : runc . and : containerd . / : ctr . . 18.06.3 -ce : docker/ docker/docker docker/docker-containerd docker/docker-containerd-ctr docker/docker-containerd-shim docker/dockerd docker/docker-init docker/docker-proxy docker/docker-run . 18.09.5 : docker/ docker/containerd docker/containerd-shim docker/ctr docker/docker docker/dockerd docker/docker-init docker/docker-proxy docker/runc .  	1	1
0 0 1.0 1.0 1.2 1.5 1.15 1.5 0.6666666666666666 0.0 0.47619047619047616 21 0.0 2 3 4 24	Add gopogh to scheduled stop windows github actions test . Currently gopogh doesn't work with these tests because of < URL > Once that is resolved , add gopogh back to those tests . Reference #10132	0	1
2 2 2.0 2.0 1.7 2.0 1.4 2.0 2.0 2.0 0.0 0 0.0 2 3 6 17	libvirt group check causes false positives . PR < URL > checks whether the user is in the libvirt group . However , there's many ways to give users permissions to use system libvirt that do not involve this group . It may be named differently , may be pointing to a remote libvirt instance , or a polkit rule like this one may be used : : polkit . addRule(function(action , subject ) { if ( action.id = = ' org . libvirt . unix . manage ' && subject . isInGroup('wheel ' )) { return polkit . Result . YES ; } }); . Minikube should first check whether it can actually access libvirt , and only then check for the missing group . Using : -- force . is not an appropriate workaround since that would skip other safety checks , too . Related : < URL > < URL >  	1	0
2 2 1.0 1.0 1.3 1.5 1.15 1.0 1.0 1.0 1.0 49 1.0 0 0 4 30	( meta ) Make Windows/Hyper-V a first-class solution . Here are the issues to be resolved : Hyper-V : Use the ' Default Switch ' ( NAT based ) by default - #4079 Hyper-V : Stable integration testing #3591 Hyper-V : minikube mount fails with windows 10 default switch - #3591 [ NON_C_DRIVE ] when run from C :/ but user config is on different drive - #4079 Please feel free to comment with any others that cause a large user experience gap for users of Windows + Hyper-V .	0	0
0 0 1.2 1.0 1.3 1.0 1.3 1.5 1.0 1.0 1.25 4 1.5 2 5 8 25	Change the error message when Config file is not available . The exact command to reproduce the issue : minikube-windows-amd64 . exe start -- alsologtostderr -- v = 8 The full output of the command that failed : The full output is correct but this first line kind of pops out - : Error reading config file at C : /Users/Pranav . Jituri/ . minikube/config/config . json : open C : /Users/Pranav . Jituri/ . minikube/config/config . json : The system cannot find the file specified . . The operating system version : Windows 10 Enterprise . Instead of showing it as an error , this needs a better solution message . Earlier , I used to see this and get confused as to what was wrong . Later on figured that this is rather the external custom config which is used .	2	1
2 2 1.6 2.0 1.3 1.5 0.95 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 3 17	minikube tunnel hard coded docker . Steps to reproduce the issue : minikube start -- embed-certs -- kubernetes-version 1.20.10 -- addons registry-creds -- driver podman minikube tunnel E0927 19:02:29 . 077832 374498 out . go : 451 ] unable to execute get port 22 for ' minikube ' : docker container inspect -f '' {{( index ( index . NetworkSettings . Ports ' 22/tcp ' ) 0 ) . HostPort }}'' minikube : exec : ' docker ' : executable file not found in $PATH  	1	0
2 2 1.8 2.0 1.3 1.0 1.15 1.0 1.6666666666666667 2.0 1.1466666666666667 75 1.0 1 2 5 24	Fork the machine driver ' generic ' to minikube , prepare for renaming it . Previously we have forked the docker-machine ( libmachine ) driver called ' none ' : < URL > < URL > We want to do the same thing for the driver called ' generic ' , as used in PR #9545 < URL > < URL > Once it is in our ' drivers ' code base , we can rename it as see fit ... Possibly to something like ' remote ' , to be decided ( see #7772 )  	1	1
