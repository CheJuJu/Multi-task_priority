1 1 0.8 1.0 1.1 1.0 1.3 2.0 1.0 1.0 1.0462962962962963 108 1.0 2 6 17 79	Tunnel : needs clean after delete router : conflicting route : 10.96.0.0 /12 via 172.17.0.3 dev docker0 . after deleting minikube , and starting it again and deploying my service I saw : : ~ $ minikube tunnel Status : machine : minikube pid : 764782 route : 10.96.0.0 /12 -> 172.17.0.2 minikube : Running services : [] errors : minikube : no errors router : conflicting route : 10.96.0.0 /12 via 172.17.0.3 dev docker0 loadbalancer emulator : no errors Status : machine : minikube pid : 764782 route : 10.96.0.0 /12 -> 172.17.0.2 minikube : Running services : [] errors : minikube : no errors router : conflicting route : 10.96.0.0 /12 via 172.17.0.3 dev docker0 loadbalancer emulator : no errors . I was able to fix it by running : minikube tunnel -c . minikube should do an auto-clean up , so the user doesnt have to do a clean up after each minikube run or delete .  	1	0
0 0 1.4 2.0 1.4 2.0 1.5 2.0 1.3333333333333333 2.0 0.8571428571428571 203 1.0 0 1 15 53	gcp auth avoid mounting empty credentials .	0	0
0 0 0.2 0.0 0.8 1.0 1.15 1.0 0.3333333333333333 0.0 1.0263157894736843 76 1.0 1 1 3 24	delete with kic : stuck on ' docker inspect -f {{ . State . Status }} minikube ' . : . /out/minikube delete -- alsologtostderr -v =3 . It doesn't show any output , but I can see it's blocked on inspect : : 19267 7810 6099 0 7:57 AM ttys001 0:00 . 06 . /out/minikube delete -- alsologtostderr -v =3 19267 7811 7810 0 7:57 AM ttys001 0:00 . 06 docker inspect -f {{ . State . Status }} minikube . Related to #6305	0	0
0 0 0.8 1.0 1.0 1.0 0.9 1.0 0.6666666666666666 0.0 1.0 47 1.0 1 2 6 29	Choose smarter defaults for -- memory . While working with @priyawadhwa on a demo , I noticed that minikube was locking up . Increasing the memory size fixed the problem . Also , the first thing our documentation states after installation is to increase the memory size . What if we were able to improve the first-start experience for most users by default , by dynamically selecting a more appropriate option for : -- memory . ? My recommendation is to change the default 2GB setting to 37.5% of available memory by default : but never less than 2.25 GB or more than 8GB . For instance : 4GB host -> 2.25 GB VM 6GB host -> 2.25 GB VM 8GB host -> 3GB VM 16GB host -> 6GB VM 32GB host -> 8GB VM 64GB host -> 8GB VM The balance can be fine tuned , but my main argument is that a developer on a 16GB machine can probably live with a 6GB VM .	0	1
2 2 0.4 0.0 0.4 0.0 0.65 0.0 0.6666666666666666 0.0 1.1066666666666667 75 1.0 4 5 12 54	minikube not respecting vm-driver flag if existing profile . this is on 1.8.2 I am not sure if this has been fixed on HEAD but I create a bug so I dont forget about it : medya@ ~ $ minikube start -- vm-driver = docker 棣冩 minikube v 1.8.2 on Darwin 10.15.3 閴?Using the hyperkit driver based on existing profile 棣冩崙 Downloading driver docker-machine-driver-hyperkit : > docker-machine-driver-hyperkit . sha256 : 65 B / 65 B [ --- ] 100.00% ? p/s 0s > docker-machine-driver-hyperkit : 6.28 MiB / 10.90 MiB 57.61% 10.26 MiB p/^C .	0	0
1 1 1.4 2.0 1.2 1.5 1.25 1.5 1.6666666666666667 2.0 0.36363636363636365 11 0.0 1 4 6 22	Try running minikube on Windows 11 beta . Windows 11 is coming out later this year , we should check if minikube works on the beta .	0	1
1 1 1.4 2.0 1.2 1.5 1.4 2.0 1.6666666666666667 2.0 1.0921052631578947 76 1.0 0 4 11 45	Define imaginary user character for minikube website . How about we define a few characters with names that who are the ppl the we create minikube for ? So when we redesign our document website we find a real user from that audience to try the website and tell us their frictions . Here is a quick example . User A Wants to try kubernetes for first time and wants to install minikube User B A software developer who wants to use minikube for local development and wants learn how to deploy their app to minikube . ( Build image , push image and deploy to minikube and then hit the url of their app ) User C A software engineer working at a corp with vpn and proxy and corp certs . They wanna use minikube on corp machines . Wants to figure out how to add corp certs to minikube . User D: Wants to explore and try different things that minikube is capable of . And try different features with examples User E : Wants to troubleshoot their not working minikuke . User F : Wants to contribute to minikube and wants to learn how to build or add something to minikube . User G : Wants to create an addon for minikube . So their cool service be easily installed on minikube for other users  	2	2
0 0 0.6 0.0 0.8 0.5 0.7 0.5 0.0 0.0 0.8490566037735849 212 1.0 0 0 4 52	minikube status : show timeToStop only if it exists . $ minikube status minikube type : Control Plane host : Running kubelet : Running apiserver : Running kubeconfig : Configured timeToStop : Nonexistent	0	1
0 0 1.2 2.0 0.9 0.5 1.0 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 2 14 34	Make MAC address settable . I use the : kvm2 . driver on debian linux but when I create a new minikube instance I do not get the ability to specify the IP or macaddress for the : -- kvm-network . interface . This is annoying because I need to take the extra step of : minikube ssh . and check the IP address of : eth0 . . It would be nice to be able to supply a static address of some sort so I can have a DHCP reservation or a well-known address . One other option would be to offer the IP address of : eth0 . as an option for : minikube ip . . Thanks for minikube , its very nice for small deployments who do not want to fight the battle of the entire stack .	2	1
2 2 1.6 2.0 1.3 1.5 1.15 1.0 1.6666666666666667 2.0 0.0 0 0.0 3 12 16 49	 minikube-linux-ppc64le release binary . Currently < URL > does not offer ppc64le binary . Can we add this platform into the release pipeline too ?  	1	1
1 1 1.6 2.0 1.3 1.5 1.05 1.0 1.3333333333333333 1.0 0.8356164383561644 219 1.0 1 4 9 32	minikube logs output takes a long long time . even when there is no minikube running : $ minikube profile list 棣冦仚 Exiting due to MK_USAGE_NO_PROFILE : No minikube profile was found . 棣冩寱 Suggestion : You can create one using ' minikube start ' . . on my mac it is taking more than a few minutes and sitll running : real 5m 31.022 s user 0m 4.015 s sys 0m 5.576 s .	0	0
2 2 1.4 2.0 1.4 2.0 1.35 2.0 1.0 1.0 0.0 0 0.0 2 7 18 79	The image source from the Hello Minicube tutorial doesn't work . Steps to reproduce the issue : Launch the shell from < URL > tutorial : kubectl create deployment hello-node -- image = gcr.io/hello-minikube-zero-install/hello-node . : kubectl expose deployment hello-node -- type = LoadBalancer -- port = 8080 . : minikube service hello-node . Full output of failed command : : | -----------|------------ | -------------|-------------------------- | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|------------ | -------------|-------------------------- | | default | hello-node | | < URL > | | -----------|------------ | -------------|-------------------------- | Opening service default/hello-node in default browser ... Minikube Dashboard is not supported via the interactive terminal experience . Please click the ' Preview Port 30000 ' link above to access the dashboard . This will now exit . Please continue with the rest of the tutorial . X open url failed : < URL > exit status 1 minikube is exiting due to an error . If the above message is not useful , open an issue : - < URL > . Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : I think the issue comes from revoked permissions to an image gcr.io/hello-minikube-zero-install/hello-node .	0	0
2 2 1.4 2.0 1.3 1.5 1.35 1.5 2.0 2.0 2.0 1 2.0 0 0 3 25	Site : fix casing for tutorials . > the sidebar linkf or telemetry , Using custom TLS certificate with ingress addon etc are not following camel casing which is used throughout the site for links and tags .  	2	2
0 0 1.2 2.0 1.1 1.5 0.95 1.0 1.3333333333333333 2.0 1.1481481481481481 54 1.0 1 4 6 46	Unclear how the container images are updated . The process of updating the minikube . iso is somewhat clear , from the Makefile etc . But for the ' kicbase ' and the ' storage-provisioner ' , things are not so straight-forward They seem to only be building snapshot and latest , but those don't go into release So it would be good to document , how to make changes to the the container images .    	1	2
0 0 1.0 1.0 0.9 1.0 0.85 1.0 0.3333333333333333 0.0 1.096774193548387 93 1.0 1 5 19 64	updated screenshot on minikube docs . the current screenshot does not have the improvements @afbjorklund made . < URL >  	2	2
2 2 0.6 0.0 0.9 0.5 1.2 1.5 0.6666666666666666 0.0 2.0 1 2.0 2 6 9 36	Cannot change apiserver-ips after initial minikube start . Steps to reproduce the issue : : minikube start -- vm-driver = none -- apiserver-ips = 1.1.1.1 ... . : minikube stop . : minikube start -- vm-driver = none -- apiserver-ips = 2.2.2.2 ... . The apiserver certificate has still the initial address in its SAN : : openssl x509 -in /var/lib/minikube/apiserver . crt -text .... X509v3 extensions : ... X509v3 Subject Alternative Name : ... IP Address : 1.1.1.1 , .... . It seems the behavior changed in v 1.10 with < URL > While it is possible to modify : apiserver-names . ( that had also issues but fixed recently with < URL > I cannot change : apiserver-ips . . What was the rationale behind this change ? Is this by design ? Is there any way to work around this regression ? Maybe related to < URL >   	1	0
2 2 0.8 0.0 1.2 1.5 1.4 2.0 1.3333333333333333 2.0 1.144736842105263 76 1.0 0 3 6 22	Add architecture to the binaries and images in the cache . When supporting remote clusters of a different architecture , we need to add the arch in addition to the OS : : ~ / . minikube/cache/linux/v 1.19.3 閳规壕鏀㈤埞鈧?kubeadm 閳规壕鏀㈤埞鈧?kubectl 閳规柡鏀㈤埞鈧?kubelet . Simplest would be to add it after the OS , but we could support both in order to preserve existing user caches ? : ~ / . minikube/cache/linux 閳规壕鏀㈤埞鈧?amd64 閳?閳规柡鏀㈤埞鈧?v 1.19.3 閳?閳规壕鏀㈤埞鈧?kubeadm 閳?閳规壕鏀㈤埞鈧?kubectl 閳?閳规柡鏀㈤埞鈧?kubelet 閳规柡鏀㈤埞鈧?arm64 閳规柡鏀㈤埞鈧?v 1.19.3 閳规壕鏀㈤埞鈧?kubeadm 閳规壕鏀㈤埞鈧?kubectl 閳规柡鏀㈤埞鈧?kubelet . For instance when running an : amd64 . host , towards a : arm64 . machine . Either local hardware or cloud instance . Raspberry Pi AWS Graviton Same thing for the images , if not using the preload ( the preloaded-tarball files already include the arch , though ) : ~ / . minikube/cache/images 閳规壕鏀㈤埞鈧?gcr.io 閳?閳规柡鏀㈤埞鈧?k8s-minikube 閳?閳规柡鏀㈤埞鈧?storage-provisioner_v3 閳规壕鏀㈤埞鈧?k8s.gcr.io 閳?閳规壕鏀㈤埞鈧?coredns_ 1.7.0 閳?閳规壕鏀㈤埞鈧?etcd_ 3.4.13 -0 閳?閳规壕鏀㈤埞鈧?kube-apiserver_v 1.19.2 閳?閳规壕鏀㈤埞鈧?kube-controller-manager_v 1.19.2 閳?閳规壕鏀㈤埞鈧?kube-proxy_v 1.19.2 閳?閳规壕鏀㈤埞鈧?kube-scheduler_v 1.19.2 閳?閳规柡鏀㈤埞鈧?pause_ 3.2 閳规柡鏀㈤埞鈧?kubernetesui 閳规壕鏀㈤埞鈧?dashboard_v 2.0.3 閳规柡鏀㈤埞鈧?metrics-scraper_v 1.0.4 . : ~ / . minikube/cache/images/amd64 ~ / . minikube/cache/images/arm64 .	2	1
2 2 1.2 2.0 1.1 1.5 1.2 2.0 2.0 2.0 0.0 0 0.0 0 0 0 13	  Document uninstall procedures . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Feature Request There doesn't seem to be documentation on how to properly uninstall minikube . #1043 has a bunch of different sets of commands , depending on what OS and shell you're using . I ran into the problem of an incompatibility between a year-old version of minikube and current ( v30 ) . It took a long time to figure out how to clear out all of minikube's old state , before finding I needed to delete ~ / . minikube ( linux ) .	0	2
2 2 2.0 2.0 1.6 2.0 1.15 1.0 2.0 2.0 2.0 1 2.0 2 4 12 48	Add overwrite flag into start command to change values in config files . . Currently it looks we can overwrite flags in config file only when first start fail . < URL > How about add : -- overwrite . flag to intentionally update value ? Then we can solve some problem occurred by machine or nodes internal change ( #10578 )   	1	1
1 1 1.4 2.0 1.2 1.5 1.15 1.0 1.0 1.0 0.0 0 0.0 4 14 19 50	  Document that minikube does not support IPv6 . It would be great if there were instructions for running minikube and kubernetes in IPv6 only mode . Now it could be that that's not even possible from my tests as with the virtualbox driver at least ( which is what I tested ) , the master node has IPv6 disabled in the sysctls ( net . ipv6 . conf . all . disable_ipv6 is set to 1 by default ) - which I wouldn't expect to be default kubernetes behaviour . Also , minikube only has one form of the host-only-cidr config item , which is IPv4 only ( always in vbox driver context ) , as it tries to pass the configured cidr to the ' VBoxManage hostonlyif ipconfig ' command with the ' -- ip ' flag , which is IPv4 only . For Ipv6 Vbox requires the ' -- ipv6 ' flag : VBoxManage hostonlyif Usage : VBoxManage hostonlyif ipconfig [ -- dhcp | -- ip [ -- netmask ( def : 255.255.255.0 )] | -- ipv6 [ -- netmasklengthv6 ( def : 64 )]] create | remove I've stopped trying stuff out here , and maybe I'm doing it wrong , but IF IPv6 is not supported in minikube or minikube and Vbox as driver ( or other drivers ) that should be clearly stated in the handbook .	0	2
1 1 1.6 2.0 1.5 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 3 8 16 39	efk : Discover times out , more info says : ' Error : Request Timeout after 30000ms ' . I'm trying to use EFK on minikube on OSX . Is there any kind of how-to on how this should work ? It seems all the pods come up ( in kube-system namespace ) . Then , I go to kibana-logging endpoint ( minikube-ip : 30003 in this case ) , and I need to set up an index . I do that , then go to discover , and it times out after 30 seconds ? If I expand the ' More info ' at the top , it says Error : Request Timeout after 30000ms at < URL > at < URL >	2	0
1 1 1.0 1.0 0.9 1.0 1.1 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 1 2 16	minikube start -- memory flag is broken in v 1.12.3 . Steps to reproduce the issue : Run a command which sets a custom memory amount for the cluster with minikube start minikube start -- driver = virtualbox -- cpus = 6 -- memory = 15g -- disk-size = 50g Full output of failed command : 棣冩 minikube v 1.12.3 on Fedora 32 閴?Using the virtualbox driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating virtualbox VM ( CPUs = 6 , Memory = 6000MB , Disk = 51200MB ) ... 棣冩儞 Preparing Kubernetes v 1.18.3 on Docker 19.03.12 ... 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : default-storageclass , storage-provisioner 棣冨及 Done ! kubectl is now configured to use ' minikube ' The memory flag in minikube start is ignored and defaults to 6gb . Note the Memory = 6000MB in the command output . This command worked as expected in v 1.12.0 . I recently updated to v 1.12.3 Note : I also tried -- memory='15g ' and obtained the same output .	0	0
2 2 1.4 1.0 1.1 1.0 1.15 1.0 1.6666666666666667 2.0 0.9615384615384616 104 1.0 1 1 4 52	 minikube addons open ' only works for kube-system namespace . This bug has probably always been this way , but I noticed it this morning while working on addons docs .  	1	0
2 2 1.4 2.0 1.1 1.0 1.2 1.0 2.0 2.0 1.1818181818181819 11 1.0 1 2 5 21	The none driver is showing imaginary numbers for the VM . : * Creating none VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... . These are just from the config of the VM that we did not create . It has nothing to do with the VM that we are actually running on ? We should either remove this information , or show the live data . Related implementation : #3574 : github.com/shirou/gopsutil .	0	0
1 1 0.8 1.0 1.1 1.0 0.9 1.0 1.0 1.0 0.9803921568627451 51 1.0 0 2 6 30	Add -- interactive flag to avoid future interactive prompts . We don't have any interactive prompts right now , but we will soon for cases where the user needs to use : sudo . , but does not have rights to do so without interaction . For example : Giving the hyperkit binary special permissions . I suggest allowing users to select : -- interactive = false . , to avoid the complication of weird flag combinations like : -- no-interactive = false . .  	1	1
1 1 1.4 2.0 1.3 1.5 1.15 1.0 1.6666666666666667 2.0 0.9230769230769231 26 1.0 1 2 5 25	Fetch Kubernetes images using tag @hash rather to prevent invalid downloads . Currently , if a user uses -- image-repository , they may end up with images that do not match the official Kubernetes ones . We should use digests or some other mechanism to prevent mirror poisoning .	2	1
0 0 1.2 2.0 0.9 0.5 1.0 1.0 1.3333333333333333 2.0 1.0 2 1.0 2 7 14 50	inner docker systemd service : move ' StartLimitIntervalSec ' param from [ Service ] to [ Unit ] section . repeating errors shown in logs : : Feb 07 01:57:49 functional-20210207015702-1428035 systemd[1 ]: /lib/systemd/system/docker . service : 13 : Unknown key name ' StartLimitIntervalSec ' in section ' Service ' , ignoring . . advising that the StartLimitIntervalSec param is ignored this param was used for resolving issue #9691 in pr #9775 i've been also checking it , but missed the conversation discussing the changes in systemd ( ref : < URL > : ' StartLimitInterval was moved from [ Service ] to [ Unit ] section in 6bf0f408e4833152197fb38fb10a9989c89f3a59 , but the old location was still accepted for compatibility . The new StartLimitIntervalSec name is valid only in [ Unit ] . ' . indeed : < URL > so , I'm creating a pr to move StartLimit * param from [ Service ] to [ Unit ] section and eliminate this error	0	0
2 2 1.8 2.0 1.9 2.0 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 3 4 7 26	[ FEATURE ] insecure-registry via minikube config set ... . I would love to be able to set defaults for -- insecure-registry just like for -- vm-driver etc . ``` $ minikube config set insecure-registry foo 棣冩寴 Set failed : property name ' insecure-registry ' not found ```` ( Similar to #3805 , hopefully same simple & easy fix as #3861 ? )	2	1
0 0 1.0 1.0 1.1 1.5 1.25 2.0 1.3333333333333333 2.0 0.6666666666666666 6 0.5 3 7 17 50	Add number of nodes for cluster in `minikube profile list` . Currently if you have two clusters running and one of them has two nodes , : minikube profile list . looks like : : | ----------|----------- | ---------|------------ | ------|--------- | --------- | | Profile | VM Driver | Runtime | IP | Port | Version | Status | | ----------|----------- | ---------|------------ | ------|--------- | --------- | | minikube | docker | docker | 172.17.0.2 | 8443 | v 1.19.0 | Running | | p1 | docker | docker | 172.17.0.4 | 8443 | v 1.19.2 | Running | | ----------|----------- | ---------|------------ | ------|--------- | --------- | 閴?Found 1 invalid profile(s ) ! p1-m02 棣冩寱 You can delete them using the following command(s ): $ minikube delete -p p1-m02 . We'd like it to look like : : | ----------|----------- | ---------|------------ | ------|--------- | ---------|--------- | | Profile | VM Driver | Runtime | IP | Port | Version | Status | Nodes | | ----------|----------- | ---------|------------ | ------|--------- | ---------|--------- | | minikube | docker | docker | 172.17.0.2 | 8443 | v 1.19.0 | Running | 1 | | p1 | docker | docker | 172.17.0.4 | 8443 | v 1.19.2 | Running | 2 | | ----------|----------- | ---------|------------ | ------|--------- | ---------|--------- | . So , we want to not have the spurious error at the end of the table , and to add the Nodes column with the number of nodes the cluster has .   	1	1
1 1 1.0 1.0 1.3 1.5 1.25 2.0 0.3333333333333333 0.0 0.0 0 0.0 1 3 10 27	docker registry addon changed port missing documentation . : $ minikube addons enable registry . currently outputs : : Registry addon on with docker uses 32781 please use that instead of default 5000 For more information see : < URL > Verifying registry addon ... The ' registry ' addon is enabled . I had to work out the edit required : : $ docker run -- rm -it -- network = host alpine ash -c ' apk add socat && socat TCP-LISTEN : 5000 , reuseaddr , fork TCP :$( minikube ip ): 32781 ' . # <-- port changed at the end . This could do with a note at the suggested URL .  	2	2
1 1 1.0 1.0 1.1 1.0 1.0 1.0 1.0 1.0 1.0 43 1.0 2 2 4 25	Prototype Docker deployment model . Tracking work for initial work toward #4389	0	1
2 2 1.4 2.0 1.3 2.0 1.15 1.5 1.6666666666666667 2.0 0.0 0 0.0 5 5 18 58	windows MK_WRONG_BINARY_WSL even when not in powershell . The offending function below is incorrectly assuming that if : WSLENV . has a non-empty value , that the shell is an WSL shell . According to < URL > post , this environment variable is used for sharing variables into new WSL sessions , and is set within Windows too . < URL > Steps to reproduce the issue : Ensure the : WSLENV . environment variable has a non-empty value . Mine is : WT_SESSION :: WT_PROFILE_ID . Run : minikube . from a regular Windows : cmd . exe . shell Observe the incorrect output : > minikube docker-env 閴?Exiting due to MK_WRONG_BINARY_WSL : You are trying to run windows . exe binary inside WSL , for better integration please use Linux binary instead ( Download at < URL > Otherwise if you still want to do this , you can do it using -- force .	0	0
1 1 0.8 1.0 0.9 1.0 1.1 1.0 0.6666666666666666 1.0 0.0 0 0.0 3 4 9 59	docker fails if -- docker-opt storage-driver = overlay2 is set . Started having this problem after upgrading to 1.8.2 . It disappeared after downgrading to back to 1.7 . x . Could not find anything recent regarding this anywhere else . I've tried deleting the : ~ / . minikube . folder and reinstalling minikube via homebrew . Didn't help . Example of output ( see link to full logs below ) : W0317 10:02:52 . 019784 1773 exit . go : 101 ] Unable to start VM . Please investigate and run ' minikube delete ' if possible : creating host : create : Error creating machine : Error running provisioning : ssh command error : command : sudo systemctl -f restart docker err : Process exited with status 1 output : Job for docker . service failed because the control process exited with error code . See ' systemctl status docker . service ' and ' journalctl -xe ' for details . . The exact command to reproduce the issue : : minikube start -- v 10 -- logtostderr / -- disk-size='20G ' / -- cpus='4 ' / -- docker-opt storage-driver = overlay2 / -- extra-config = apiserver . authorization-mode='RBAC ' / -- kubernetes-version v 1.15.5 / -- memory='5120 ' / -- network-plugin = cni / -- enable-default-cni / -- vm-driver='hyperkit ' . The full output of the command that failed : See here : < URL > The operating system version : macOS 10.15.3  	1	0
2 2 0.8 0.0 1.0 1.0 1.05 1.0 0.6666666666666666 0.0 0.0 0 0.0 10 12 18 55	ssh-user is not used in native mode . Steps to reproduce the issue : minikube start -- driver = ssh -- ssh-ip-address = 192.168.122.171 -- ssh-user = centos -p kubevirt Full output of failed command : 棣冩 [ kubevirt ] minikube v 1.17.1 on Fedora 33 閴?Using the ssh driver based on existing profile 棣冩啢 Starting control plane node kubevirt in cluster kubevirt 棣冨籍 Updating the running ssh ' kubevirt ' bare metal machine ... 棣冦亞 StartHost failed , but will try again : provision : fast detect : OS type not recognized 棣冨籍 Updating the running ssh ' kubevirt ' bare metal machine ... Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : Full of logs in the target host : Feb 25 11:31:24 kubevirt sshd[65242 ]: Connection closed by authenticating user ** root ** 192.168.122.1 port 42952 [ preauth ]   	1	0
1 1 1.0 1.0 1.0 1.0 0.9 1.0 1.3333333333333333 1.0 0.8557213930348259 201 1.0 8 10 16 54	kicbase automation : include the changelog in the PR since last stable kic release . I think we can still the Git logic from the dependbot code example < URL > < URL >  	1	1
2 2 1.4 2.0 1.4 2.0 1.25 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 2 14 33	restart : waiting for k8s-app = kube-proxy : timed out waiting for the condition . When i minikube start , i get error : boby @sok -01 : ~ $ minikube start 棣冩 minikube v 0.35.0 on linux ( amd64 ) 棣冩寱 Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Restarting existing virtualbox VM for ' minikube ' ... 閳?Waiting for SSH access ... 棣冩懕 ' minikube ' IP address is 192.168.99.100 棣冩儞 Configuring Docker as the container runtime ... 閴?Preparing Kubernetes environment ... 棣冩 Pulling images required by Kubernetes v 1.13.4 ... 棣冩敡 Relaunching Kubernetes v 1.13.4 using kubeadm ... 閳?Waiting for pods : apiserver proxy棣冩寴 Error restarting cluster : wait : waiting for k8s-app = kube-proxy : timed out waiting for the condition 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > boby @sok -01 : ~ $ . I am using Ubuntu 18.04 Attached minikube logs < URL >	0	0
2 2 1.8 2.0 1.4 2.0 1.25 2.0 1.6666666666666667 2.0 0.0 1 0.0 0 1 2 27	Addon manager presumes port 8443 -- fails when used with different -- apiserver-port . If minikube is started with the -- apiserver-port directive set to a non-default value , the addon-manager pod fails to connect to the apiserver . The culprit seems to be a hardcoding of port 8443 here : < URL > The fix is a one-liner--I'll submit a pull request .	2	0
2 2 1.6 2.0 1.4 2.0 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 3 8 26	Add -- file flag to minikube logs . . We should have a -- file flag in : minikube logs . so that logs output is sent directly to a file for easy transfer . Before : : $ minikube logs < tons of output > . After : : $ minikube logs -- file out . File ' out ' written full of logs .	0	1
0 0 0.4 0.0 1.0 1.0 1.2 2.0 0.0 0.0 0.7142857142857143 7 1.0 0 5 9 60	docker : service command hangs for docker driver in Windows . The exact command to reproduce the issue : Following the docs tutorial : : minikube start -- vm-driver = docker kubectl create deployment hello-minikube -- image = k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube -- type = NodePort -- port = 8080 minikube service hello-minikube . The full output of the command that failed : : * minikube v 1.7.3 on Microsoft Windows 10 Enterprise 10.0.18362 Build 18362 * Using the docker ( experimental ) driver based on user configuration * Creating Kubernetes in docker container with ( CPUs = 2 ) ( 2 available ) , Memory = 2000MB ( 1989MB available ) ... * Preparing Kubernetes v 1.17.3 on Docker 19.03.2 ... - kubeadm . pod-network-cidr = 10.244.0.0 /16 * Launching Kubernetes ... * Enabling addons : default-storageclass , storage-provisioner * Waiting for cluster to come online ... * Done ! kubectl is now configured to use ' minikube ' deployment . apps/hello-minikube created service/hello-minikube exposed | -----------|---------------- | -------------|------------------------- | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|---------------- | -------------|------------------------- | | default | hello-minikube | | < URL > | | -----------|---------------- | -------------|------------------------- | * Opening service default/hello-minikube in default browser ... . Then the browser times out .	0	0
1 1 1.4 1.0 1.3 1.0 0.95 1.0 1.0 1.0 0.8208333333333333 240 1.0 0 0 2 12	add FAQ how to access service on physical host from inside a VM driver's container .    	1	2
1 1 1.6 2.0 1.5 2.0 1.35 2.0 1.6666666666666667 2.0 0.9818181818181818 55 1.0 2 4 8 22	Add VPN interference check : HTTP fetch before installing Kubernetes . Here's an idea to address #4302 Add a lightweight HTTP service to our VM that unobtrusively reports system stats , such as load average . As soon as we can confirm SSH access to our VM , and the VM appears to be of the correct version , try accessing this service . Port 8443 would be ideal for catching the issue , but lets say a nearby port to avoid conflicts . This would kill two birds with one stone : A lightweight way to gather system metrics without impacting VM load ( compared to ssh+uptime command ) A way to detect connectivity issues separate from Kubernetes . If it fails , we could report a very clear message , such as : : Unable to make HTTP requests to the VM ( < URL > To use minikube , please adjust your firewall or VPN to allow these connections . . We can always add : -- force . to workaround issues with it .	0	1
0 0 0.8 1.0 0.8 1.0 1.05 1.0 0.6666666666666666 0.0 1.1029411764705883 68 1.0 3 4 10 33	Deprecate the old tcp port for docker-env . Currently we are using a standalone : tcp :// . docker daemon , listening on port 2376 . : export DOCKER_TLS_VERIFY='1 ' export DOCKER_HOST='tcp://192 . 168.99.100 : 2376 ' export DOCKER_CERT_PATH='/home/anders/ . minikube/certs ' . We can use : ssh :// . and connect directly to the unix socket instead , simplifying things . This could use either the current ssh shell tunnel , or we could use the regular address ... : export DOCKER_HOST='ssh://docker@192 . 168.99.100 : 22 ' ssh-add /home/anders/ . minikube/machines/minikube/id_rsa . But we wouldn't have to manage all the extra ssl certificates for https , when using ssh . : /home/anders/ . minikube/certs 閳规壕鏀㈤埞鈧?ca-key . pem 閳规壕鏀㈤埞鈧?ca . pem 閳规壕鏀㈤埞鈧?cert . pem 閳规柡鏀㈤埞鈧?key . pem 0 directories , 4 files . And this allows for having the docker-daemon socket-activated ( on-demand ) in the future ... Requirements : Docker 18.09 or later Note that we already support both methods of connecting , so it can be a gradual change . Current config : : /usr/bin/dockerd -H tcp :/ / 0.0.0.0 : 2376 -H unix :// /var/run/docker . sock . < URL > See #9232  	1	1
2 2 2.0 2.0 1.4 2.0 1.45 2.0 2.0 2.0 1.7777777777777777 9 2.0 1 3 5 24	Delete profile should Unset the current profile . Delete profile should Unset the current profile to reproduce : 1- create a minikube with profile p1 and p2 : . /out/minikube start -p p1 -- vm-driver = hyperkit . : . /out/minikube start -p p2 -- vm-driver = hyperkit . 2- set the current profile to p1 : . /out/minikube profile p1 . 3- delete profile p1 : . /out/minikube delete -p p1 棣冩暉 Deleting ' p1 ' in hyperkit ... 棣冩寖 The ' p1 ' cluster has been deleted . . 4- the current profile is gone from profile list ( correctly ) however if u check for current profile it shows the deleted one : : medya@ ~ /workspace/ppl-minikube ( fix-glog-parse-error ) $ . /out/minikube profile list | ----------|----------- | ----------------|----------- | -------------------- | | Profile | VM Driver | NodeIP | Node Port | Kubernetes Version | | ----------|----------- | ----------------|----------- | -------------------- | | minikube | hyperkit | 192.168.64.102 | 8443 | v 1.15.0 | | p3 | hyperkit | 192.168.64.118 | 8443 | v 1.15.0 | | ----------|----------- | ----------------|----------- | -------------------- | . 5- check for current profile : $ . /out/minikube profile p1 . The fix should , after delete unset the current config .	0	0
2 2 1.6 2.0 1.6 2.0 1.55 2.0 2.0 2.0 0.0 0 0.0 0 0 1 8	hyperkit : external DNS resolution fails : conflict with DNS server running on host . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : MacOS High Sierra Minikube version ( use : minikube version . ): 0.25.0 and above - OS ( e.g. from /etc/os-release ): MacOS High Sierra , version 10.13.6 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): hyperkit - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): . minikube/cache/iso/minikube . 25.0.0 . iso - Install tools : curl - #Lo minikube < URL > && chmod +x minikube && sudo mv minikube /usr/local/bin/ - Others : What happened : dns in minikube does not work . DNS cluster can not start , because can not download images from registry . 192.168.64.1 - it is my host IP : nslookup ya.ru Server : 192.168.64.1 Address 1 : 192.168.64.1 nslookup : can't resolve ' ya.ru ' . What you expected to happen : DNS works . If I change DNS server in minikube to 8.8.8.8 , it works . : su root rm -f /etc/resolv . conf && echo nameserver 8.8.8.8 > /etc/resolv . conf nslookup ya.ru Server : 8.8.8.8 Address 1 : 8.8.8.8 google-public-dns-a.google.com Name : ya.ru Address 1 : 87.250.250.242 ya.ru Address 2 : 2a 02:6 b8 :: 2:242 ya.ru . How to reproduce it ( as minimally and precisely as possible ): : minikube stop && minikube delete minikube start -- vm-driver hyperkit . Output of : minikube logs . ( if applicable ) : Anything else do we need to know :  	1	0
2 2 1.6 2.0 1.4 2.0 1.4 2.0 2.0 2.0 0.0 0 0.0 2 4 9 44	add podman support in windows WSL . Steps to reproduce the issue : minikube start -- vm-driver = podman minikube tunnel start a loadbalancer Get an ip that maps to a similar subnet , rather than my local subnet . Results of usual commands are normal . : 閴?minikube service -n kong kong-kong-proxy | -----------|----------------- | --------------------|------------------------ | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|----------------- | --------------------|------------------------ | | kong | kong-kong-proxy | kong-proxy/80 | < URL > | | | | kong-proxy-tls/443 | < URL > | | -----------|----------------- | --------------------|------------------------ | . Returns a good result ! ( from wsl ) : curl -i -H ' Host : api . gengo . dev ' < URL > . : minikube dashboard . gives a good url that I can access on localhost . Some investigation : : sudo nc -l 80 . binds to localhost correctly and the browser can hit it . Am I missing a step ? Thanks !  	1	1
2 2 0.6 0.0 0.8 0.5 0.85 1.0 0.6666666666666666 0.0 2.0 1 2.0 2 4 10 31	xhyve suggests obsolete WantShowDriverDeprecationNotification setting . minikube v 1.0.0 I use : minikube start -- vm-driver xhyve . which tells me : 閳跨媴绗?The xhyve driver is deprecated and support for it will be removed in a future release . Please consider switching to the hyperkit driver , which is intended to replace the xhyve driver . See < URL > for more information . To disable this message , run [ minikube config set WantShowDriverDeprecationNotification false ] . but : $ minikube config set WantShowDriverDeprecationNotification false 棣冩寴 Set failed : Property name WantShowDriverDeprecationNotification not found .	2	0
0 0 1.6 2.0 1.7 2.0 1.4 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 3 14 32	Changing service cidr range breaks DNS resolution . Our company intranet uses large parts of the 10.0.0.1 /8 subnet . Alas , the default service CIDR range used by minikube ( resp . kubeadm ? ) is 10.96.0 . * . To avoid problems , I wanted to change pod and service CIDR range with these two options passed to minikube start : -- service-cluster-ip-range='172 . 20.0.0 /14 ' -- extra-config = kubelet . pod-cidr = 172.16.0.0 /14 The VM seemed to startup fine , but after deploying our application , I noticed that the DNS names of services could not be resolved by our pods , e.g. : nslookup our-service nslookup : can't resolve ' ( null )': Name does not resolve . I checked /etc/resolv . conf inside the pod , and indeed , it still pointed to the default IP of the kubedns service , i.e. , 10.96.0.10 . I would expect that changing the service CIDR range would also take care of that . Looking for a workaround , I tried using extra-config also for the service-cidr range , i.e. , : -- extra-config = kubelet . pod-cidr = 172.16.0.0 /14 -- extra-config = kubelet . service-cidr = 172.20.0.0 /14 . Now the minikube start would fail cause kubelet wouldn't start : : [ ... ] Oct 25 11:40:15 minikube kubelet[24917 ]: F1025 11:40:15 . 308498 24917 server . go : 156 ] unknown flag : -- service-cidr Oct 25 11:40:15 minikube systemd[1 ]: kubelet . service : Main process exited , code = exited , status = 255/n/a [ ... ] . .	2	0
1 1 1.4 1.0 1.2 1.0 1.35 1.5 1.3333333333333333 1.0 2.0 1 2.0 1 1 4 16	Remove polling from the addon manager to reduce overhead . In < URL > I pointed out that the addon manager polls to make sure addon configuration is correct every 5 seconds . This can result in CPU spikes up to ~ 80% of a core every 5 seconds . Every 5 seconds , the addon manager runs the same command : : /usr/local/bin/kubectl apply -f /etc/kubernetes/addons -l kubernetes.io/cluster-service!=true,addonmanager.kubernetes.io/mode=Reconcile -- prune = true -- prune-whitelist core/v1/ConfigMap -- prune-whitelist core/v1/Endpoints -- prune-whitelist core/v1/Namespace -- prune-whitelist core/v1/PersistentVolumeClaim -- prune-whitelist core/v1/PersistentVolume -- prune-whitelist core/v1/Pod -- prune-whitelist core/v1/ReplicationController -- prune-whitelist core/v1/Secret -- prune-whitelist core/v1/Service -- prune-whitelist batch/v1/Job -- prune-whitelist batch/v1beta1/CronJob -- prune-whitelist apps/v1/DaemonSet -- prune-whitelist apps/v1/Deployment -- prune-whitelist apps/v1/ReplicaSet -- prune-whitelist apps/v1/StatefulSet -- prune-whitelist extensions/v1beta1/Ingress -- recursive . Instead of running the addon manager as a pod by default , we can turn the addon manager addon off by default and run the : kubectl apply . command ourselves whenever a user changes the state of addons via : minikube addons enable/disable .	0	1
2 2 0.8 0.0 0.8 0.5 1.25 2.0 0.6666666666666666 0.0 0.0 0 0.0 5 7 10 41	latest storage-provisioner : cannot get resource ' endpoints ' in API group ' in the namespace ' kube-system ' . Not sure , but it seems related to < URL > The exact command to reproduce the issue : : minikube start kubectl create namespace test kubectl apply -f pvc . yaml . The full output of the command that failed : storage provisioner logs : : E0206 10:46:39 . 353949 1 leaderelection . go : 331 ] error retrieving resource lock kube-system/ k8s.io-minikube-hostpath : endpoints ' k8s.io-minikube-hostpath ' is forbidden : User ' system : serviceaccount : kube-system : storage-provisioner ' cannot get resource ' endpoints ' in API group ' in the namespace ' kube-system . The output of the : minikube logs . command : The operating system version : : minikube version minikube version : v 1.5.2 commit : 792dbf92a1de583fcee76f8791cff12e0c9440ad-dirty . : cat /etc/os-release NAME='Linux Mint ' VERSION='19 . 3 ( Tricia )' . : pvc . yaml . : : kind : PersistentVolumeClaim apiVersion : v1 metadata : name : test namespace : test spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi .  	1	0
2 2 1.4 1.0 1.2 1.0 1.25 1.0 1.6666666666666667 2.0 0.0 0 0.0 2 8 11 42	minikube scp . It would be nice to be able to have : minikube scp . like : minikube ssh . , to copy files in the the minikube node . An use case would be : copying Dockerfiles to the node to build custom docker images for Kubernetes . In the meantime , for all of you that want this feature , here's a workaround : : scp -i $(minikube ssh-key ) < file > docker@$(minikube ip ): . edit : you can also use : minikube docker-env . to set up the : docker . client to point to minikube's docker daemon , and build the image locally . However , this still doesn't work for copying files ...	2	1
2 2 0.8 1.0 1.0 1.0 1.15 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 4 6 20	kvm2 : Get < URL > net/http : request canceled while waiting for connection ( Client . Timeout exceeded while awaiting headers ) . The exact command to reproduce the issue : : minikube start -- vm-driver = kvm2 . The full output of the command that failed : 棣冩 minikube v 1.4.0 on Debian rodete 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... 棣冩儞 Preparing Kubernetes v 1.16.0 on Docker 18.09.9 ... 棣冩 Pulling images ... 閴?Unable to pull images , which may be OK : running cmd : sudo env PATH = /var/lib/minikube/binaries/v 1.16.0 : $PATH kubeadm config images pull -- config /var/tmp/minikube/kubeadm . yaml : command failed : sudo env PATH = /var/lib/minikube/binaries/v 1.16.0 : $PATH kubeadm config images pull -- config /var/tmp/minikube/kubeadm . yaml stdout : stderr : failed to pull image ' k8s.gcr.io/kube-apiserver:v1.16.0 ' : output : Error response from daemon : Get < URL > net/http : request canceled while waiting for connection ( Client . Timeout exceeded while awaiting headers ) , error : exit status 1 To see the stack trace of this error execute with -- v = 5 or higher : Process exited with status 1 棣冩畬 Launching Kubernetes ... 閳?Waiting for : apiserver proxy etcd scheduler controller dns 棣冨及 Done ! kubectl is now configured to use ' minikube ' A system restart fixes this , but we should look into what's happening .  	1	0
0 0 0.8 1.0 0.9 1.0 1.1 1.0 1.0 1.0 0.0 0 0.0 0 1 3 21	Feature : setting minikube profile should set k8s current context . . minikube version : : minikube version : v 1.1.1 . problem : when changing profile , it doesn't change the k8s context . reproduce : Create two profiles p1 , p2 : minikube start -p p1 -- vm-driver virtualbox minikube start -p p2 -- vm-driver virtualbox . set the profile to p1 : : minikube profile p1 . : $ minikube profile p1 -- alsologtostderr -v = 8 I0615 16:11:49 . 939954 94839 notify . go : 128 ] Checking for updates ... 閴?minikube profile was successfully set to p1 . the kubectl context : : $ kubectl config current-context p2 . expect the context to be p1 but it is still p2 . ( it only gets set by start cmd ) os : mac os high sierra 10.13.6	2	1
2 2 1.4 2.0 1.2 1.5 1.5 2.0 1.3333333333333333 2.0 1.3043478260869565 23 1.0 0 0 9 25	sudo crictl : not found if installed to a directory outside of sudoers secure_path . We have some things breaking on CentOS , which does not have : /usr/local/bin . in the PATH . That is : users do have it in their PATH , but root does not . And : sudo . runs with the root PATH . So if you are trying to run something that is not from a package , you will get a ' command not found ' This goes both for ' crio version ' ( when run as root ) and for ' sudo crictl ' , only work with : /usr/bin . . One workaround is to move everything to : /usr/bin . , which is violating the < URL > but whatever . It would be nice if minikube was able to find : crio . and : crictl . - also in their default location ? This would have worked better if crio and cri-tools would be available in the default distribution . But it doesn't ( at the moment ) , and when you try to install it locally you run into these issues ...  	1	1
0 0 0.6 0.0 1.1 1.5 0.8 0.5 0.6666666666666666 0.0 0.0 0 0.0 2 8 15 40	-- apiserver-name has no effect on ~ / . kube/config . To reproduce : : powtest @chlorine : ~ $ minikube version minikube version : v 0.35.0 ````console powtest @chlorine : ~ $ sudo -E minikube start -- vm-driver = none -- apiserver-name = chlorine.acdlab	2	0
2 2 0.8 0.0 0.9 0.5 1.1 1.5 1.3333333333333333 2.0 0.9854014598540146 137 1.0 6 7 10 24	docker : minikube delete should delete the network . I notice minikube delete doesn't remoev the network after itself . created this issue . : medya @medya : ~ /workspace/minikube ( master)$ . /out/minikube delete -- all medya @medya : ~ /workspace/minikube ( master)$ docker network ls NETWORK ID NAME DRIVER SCOPE fdc833b38ce5 bridge bridge local 43cf4c7ad471 host host local 0e6c505503f2 minikube bridge local 2d7623149fc1 none null local .	0	0
1 1 1.2 1.0 1.2 1.0 1.15 1.0 1.3333333333333333 1.0 1.1538461538461537 117 1.0 1 5 7 24	Using the none driver sets the hostname to ' control-plane ' . This is not desired , the hostname should stay the way it is . For multi-node VMs that we create ourselves , then sure . But existing machines get to keep their current hostnames . Seen in #11174 with CentOS 8 ( systemd 239 )  	1	0
0 0 0.8 0.0 1.1 1.5 1.1 1.0 0.6666666666666666 0.0 1.588235294117647 17 2.0 0 0 3 23	Improve error message for service not in default namespace . . 1- create a NodePort service in not default namespace : : $ kubectlget svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S ) AGE default kubernetes ClusterIP 10.96.0.1 < none > 443/TCP 15d dgeek-v1 dgeek-helm-dgeek-helm NodePort 10.103.186.237 < none > 80:32316 /TCP 3m40s . 2- try to open the service url : : $ minikube service dgeek-helm-dgeek-helm 棣冩寴 Error opening service 閴?Error : [ SERVICE_NOT_FOUND ] Could not find finalized endpoint being pointed to by dgeek-helm-dgeek-helm : Temporary Error : Error getting service dgeek-helm-dgeek-helm : services ' dgeek-helm-dgeek-helm ' not found Temporary Error : Error getting service dgeek-helm-dgeek-helm : services ' dgeek-helm-dgeek-helm ' not found Temporary Error : Error getting service dgeek-helm-dgeek-helm : services ' dgeek-helm-dgeek-helm ' not found Temporary Error : Error getting service dgeek-helm-dgeek-helm : services ' dgeek-helm-dgeek-helm ' not found 棣冩寱 Suggestion : Please make sure the service you are looking for is deployed or is in the correct namespace . 閳﹀绗?Related issues : 閳?< URL > 棣冩▼ If the above advice does not help , please let us know : 棣冩啝 < URL > . the error message is big and suggestion gets over-shaddowed . I suggest the error message say : service X was not found in default namespace , please try with ' minikube service X -n Y ' .	2	0
1 1 1.4 1.0 1.2 1.0 1.25 1.0 1.0 1.0 2.0 1 2.0 0 2 5 24	parallels : `minikube mount` : attempted to get host ip address for unsupported driver . Running : minikube mount . with the parallels driver gives this error message : : 棣冩寴 Error getting the host IP address to use from within the VM : Error , attempted to get host ip address for unsupported driver . According to < URL > , this is preferred over relying on driver-specific mounting . It's an issue for me right now because I can't figure out how to get share folders enabled through the Parallels UI to show up on the Minikube VM .	2	1
1 1 1.4 2.0 1.2 1.5 1.15 1.0 1.6666666666666667 2.0 2.0 2 2.0 1 2 4 20	feature : set ingress class for ingress plugin ( for traefik ) . If someone is using a different ingress class , like : traefik . , then the ingress configurations won't work . We should be able to set the ingress class for the ingress plugin to resolve this with an option like : -- ingress-class ={{ default ' nginx ' . IngressClass }} . as described here : < URL >	2	1
2 2 1.4 2.0 1.5 2.0 1.5 2.0 1.3333333333333333 2.0 0.875 24 1.0 2 3 6 39	update-context is confusing with profiles : ' IP was already configured ' . : $ . /out/minikube update-context -p minikube 棣冩 IP was already correctly configured for 192.168.39.232 $ kubectl config view | grep current current-context : v1134 $ . /out/minikube update-context -p v1134 棣冩 IP was already correctly configured for 192.168.39.241 $ kubectl config view | grep current current-context : v1134 . Notice how the context never changes , but minikube thinks it is already correctly configured ?	2	0
2 2 1.8 2.0 1.7 2.0 1.45 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 3 4 8	controlPlaneEndpoint = localhost breaks clusterapi-apiserver : webhook . go connection refused . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : x86_64 linux Minikube version ( use : minikube version . ): > = v 0.28.1 - OS ( e.g. from /etc/os-release ): ubuntu , varying versions - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): kvm2 - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): > = v 0.28.1 What happened : cluster-api errors with the following : ERROR : logging before flag . Parse : E0814 22:20:52 . 810904 1 webhook . go : 192 ] Failed to make webhook authorizer request : Post < URL > dial tcp 127.0.0.1 : 8443 : connect : connection refused ERROR : logging before flag . Parse : E0814 22:20:52 . 811051 1 errors . go : 90 ] Post < URL > dial tcp 127.0.0.1 : 8443 : connect : connection refused . What you expected to happen : With 0.28.0 it connected to the apiserver as expected . How to reproduce it ( as minimally and precisely as possible ): Start minikube and install the clusterapi controller : curl -L < URL > | kubectl apply -f - Output of : minikube logs . ( if applicable ) : Anything else do we need to know : This behavior changed with the merge of this line . Removing the line reverts the behavior and allows the application to function correctly . < URL >  	1	0
2 2 1.0 1.0 0.7 0.0 1.05 1.0 1.6666666666666667 2.0 0.0 0 0.0 2 2 5 30	Windows : addons disable failed : [ disabling addon xxx : Process exited with status 1 . The exact command to reproduce the issue : minikube addons disable heapster The full output of the command that failed : ! disable failed : [ disabling addon deploy/addons/heapster/influx-grafana-rc . yaml : Process exited with status 1 ] * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL > The output of the : minikube logs . command : The operating system version : Windows 10  	1	0
2 2 1.4 2.0 1.3 2.0 1.3 2.0 1.6666666666666667 2.0 1.0813953488372092 86 1.0 1 12 28 57	site : create addon gallery on the website . we could have a an addon gallery on the website , where people can see addons , and filter by Operating system or driver , and a link to Add your software to minikube addons ... here is how .  	2	2
0 0 1.2 2.0 1.5 2.0 1.4 2.0 0.6666666666666666 0.0 2.0 1 2.0 4 5 8 22	save cache files under ~ / . cache/minikube . minikube writes a lot to : ~ / . minikube/cache . , but other programs have agreed to use : ~ / . cache/ < program > . instead . Consider switching to : $XDG_CACHE_HOME . ( default : ~ / . cache . ) for files that I can freely delete if I want to trade free disk for some repeated downloads or computation . Perhaps the rest of : ~ / . minikube . should go under : $XDG_CONFIG_HOME . ( default : ~ / . config/minikube . ) to help curb the homedir dotfile mess , but that's not as important to me . Maybe some people are syncing : ~ / . config . between multiple hosts ?	2	1
0 0 0.8 0.0 0.9 0.5 1.3 2.0 0.6666666666666666 0.0 0.0 0 0.0 0 0 5 16	minikube can't re-configure ingress to use a different ssl certificate . Steps to reproduce the issue : Tutorial that was followed : < URL > start minikube locally : minikube start . enable ingress : minikube addons enable ingress . add the auto-generated certificate in . pem : : kubectl -n kube-system create secret tls mkcert -- key ~ / . minikube/key . pem -- cert ~ / . minikube/cert . pem . : minikube addons configure ingress . In the prompt to add the certificate : be stupid and have a typo : : bogus-namespace/mkcert . Re-enable the ingress : : minikube addons disable ingress . : minikube addons enable ingress . Notice the mistake Try to re-configure the ingress : : minikube addons configure ingress . instead of getting the prompt again , it auto-proceeds to : ingress was successfully configured . <--- this is the bug ! Why would I be stuck with this typo forever ? Run : minikube logs -- file = logs . txt . and drag and drop the log file into this issue Full output of failed command if not : minikube start . :	0	0
2 2 1.2 1.0 1.4 1.5 1.25 1.0 1.0 1.0 1.0 1 1.0 4 7 13 44	sometimes K8s still try to pull cached images . When I am verifying #10044 , I found sometimes k8s still try to pull cached images .  	1	0
0 0 1.0 1.0 1.0 1.0 0.95 1.0 1.0 1.0 1.2702702702702702 37 1.0 0 2 6 27	Make minikube work in Google Cloud Shell ( using kic ) . also related < URL > : medya @cloudshell : ~ ( k8s-minikube)$ . /minikube start -- vm-driver = docker * minikube v 1.7.0 -beta . 0 on Debian 9.11 * Selecting experimental ' docker ' driver from user configuration ( alternates : []) * Creating Kubernetes in docker container with ( CPUs = 2 ) , Memory = 2000MB ( 1995MB available ) ... * X Failed to enable container runtime : br_netfilter : command failed : [ docker exec -- privileged minikube sudo modprobe br_netfilter ]: exit status 1 * * minikube is exiting due to an error . If the above message is not useful , open an issue : - . currently it fails at this line : : func enableIPForwarding(cr CommandRunner ) error { c : = exec . Command('sudo ' , ' modprobe ' , ' br_netfilter ' ) if _ , err : = cr . RunCmd(c ); err ! = nil { return errors . Wrap(err , ' br_netfilter ' ) } } . running the command manually I get this output : $ docker exec -it minikube sudo modprobe br_netfilter modprobe : ERROR : .. /libkmod/libkmod . c : 586 kmod_search_moddep () could not open moddep file ' /lib/modules/ 4.19.79 +/modules . dep . bin ' modprobe : FATAL : Module br_netfilter not found in directory /lib/modules/ 4.19.79 + .	0	1
1 1 1.0 1.0 1.1 1.5 1.2 1.5 1.0 1.0 0.0 0 0.0 0 3 9 32	missing tag for nvidia-driver-installer addon . Steps to reproduce the issue : minikube start minikube addons enable nvidia-driver-installer kubectl get pod -A | grep nvidia-driver-installer : kube-system nvidia-driver-installer-v9fxt 0/1 Init : ImagePullBackOff . This is a regression after #9551  	1	0
1 1 0.6 0.0 1.0 1.0 1.2 1.5 0.3333333333333333 0.0 1.1 80 1.0 4 7 10 46	do not crash , when missplelled container runtime . : . /out/minikube start -- driver = docker -- container-runtime = conatinerd 棣冩 minikube v 1.9.2 on Darwin 10.13.6 閴?Using the docker driver based on user configuration 棣冩寴 Failed to generate config : unknown runtime type : ' conatinerd ' 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > . in this case i misspelled containerd and instead of crying and crashing and asking user to create an issue , we could say ' conatinerd ' is not a valid container-runtime , the valid options are ...  	1	1
2 2 1.4 2.0 1.5 2.0 1.3 2.0 1.3333333333333333 2.0 1.1428571428571428 119 1.0 0 3 9 35	The kicbase image downloads twice . There is a regression in minikube 1.20.0 , that the kicbase image will download twice : once to cache , once to daemon The workaround is to do minikube start -- download-only ( or maybe : docker pull . ) , or to just let it download The fix was in PR ~ ~ #11063 ~ ~ But it didn't make it to release	0	0
0 0 0.6 0.0 0.8 0.5 1.1 1.0 0.6666666666666666 0.0 1.024390243902439 41 1.0 4 5 6 30	Download our own kvm2 driver if installed version is missing or out of date . Related : #3975 #4387 Also , mentioned in our roadmap : < URL >  	1	1
1 1 1.2 1.0 1.1 1.0 0.9 1.0 1.3333333333333333 1.0 1.0238095238095237 42 1.0 5 6 7 31	Write a script to automatically propose PR's due to a new Kubernetes release . I would love a script that we can run from a cronjob or Jenkins that monitors < URL > for new releases . The script should propose a PR for any release newer than NewestKubernetesVersion : < URL > If the release is a non-beta release , it should also change DefaultKubernetesVersion in the same PR : < URL > This script should either build upon or replace < URL > - which makes the changes necessary in our code base , but is designed for interactive use and does not know how to look for new Kubernetes releases . We can provide a github token via an environment variable .	0	1
2 2 1.4 2.0 1.1 1.5 1.15 1.0 1.3333333333333333 2.0 0.9888888888888889 90 1.0 3 8 12 50	Add validation to : minikube config set memory . Figured this out while writing documentation : Don't allow the user to specify more memory than is available Don't allow the user to specify less memory than is possible If the user is using Docker , and their maximum exceeds Docker's limit , point them to the appropriate Docker documentation to increase it  	1	1
0 0 1.0 1.0 1.0 1.0 1.05 1.0 1.0 1.0 1.0 1 1.0 2 2 2 5	Make minikube offline-friendly by default ( needs testing ) . We get many bug reports where users have issues accessing the internet due to proxy issues . I'm trying to collect them here : < URL > This somewhat goes along with #1623 - but requires making sure we test that minikube works offline .	0	1
2 2 1.4 2.0 1.4 2.0 1.5 2.0 1.0 1.0 0.0 0 0.0 3 4 6 40	how can I configure kube-scheduler in minikube ? . I want configure a special kube-scheduler in minikube . I'm following this article in minikube ( < URL > In step2 modify-scheduler-configuration , I must specify a kube-scheduler with a yaml , how can I do this ? First , I try to execute the command : kubectl create -f kube-scheduler . yaml . , but the pod startup failure . Then , I read the following link < URL > and this link < URL > So I try to execute the command : minikube start -- extra-config = scheduler . config = kube-scheduler . yaml . , But the pod/kube-scheduler-minikube status is CrashLoopBackOff , I don't think I used the : -- extra-config . command properly , please give me some help . thanks very much  	2	2
2 2 1.0 1.0 1.2 1.5 1.35 1.5 0.6666666666666666 0.0 0.9541984732824428 131 1.0 0 4 10 27	Investigate configuring Kubernetes to pull from the host Docker daemon . This was an idea from Brian De Alwis over chat : Could we change minikube so that Kubernetes pulls from the host Docker daemon ? Similar to how Docker Desktop works today . This would allow image caches to persist across clusters , and remove the latency of pushing images from the host into the cluster .  	1	1
2 2 1.8 2.0 1.4 2.0 1.55 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 3 10	kube-dns still deployed with coredns . BUG REPORT : Minikube version ( use : minikube version . ): v 0.30.0 - OS ( e.g. from /etc/os-release ): Windows - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): Virtualbox - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.30.0 What happened : After default installation ( no specific arguments passed ) , minikube has both kube-dns and coredns deployments with one kube-dns service pointing on the 2 implementations What you expected to happen : After default installation , minikube addons list indicates coredns is enabled and kube-dns is disabled . I expect to have kube-dns service that targets only coredns pod How to reproduce it ( as minimally and precisely as possible ): : $ minikube start [ ... ] $ kubectl -n kube-system get deploy | grep dns coredns 1 1 1 1 8h kube-dns 1 1 1 1 8h . Anything else do we need to know : Everything seems to work fine with 2 implementations at the same time . If I delete kube-dns deployment , it seems to still work ( not yet extensively tested BTW )	0	0
2 2 1.0 1.0 1.2 1.5 1.0 1.0 1.3333333333333333 2.0 2.0 1 2.0 2 2 4 14	dashboard integration test leaks kubectl processes . There is a process leak related to the dashboard & minikube proxy on the mac mini integration test node . : $ ps ax | grep proxy 8063 ?? S 0:00 . 07 /usr/libexec/networkserviceproxy 8498 ?? S 0:05 . 18 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 9142 ?? S 0:05 . 22 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 9905 ?? S 0:05 . 12 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 10639 ?? S 0:05 . 07 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 11704 ?? S 0:04 . 54 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 12482 ?? S 0:04 . 45 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 13045 ?? S 0:04 . 40 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 13684 ?? S 0:04 . 41 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 14275 ?? S 0:04 . 36 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 14875 ?? S 0:04 . 26 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 15466 ?? S 0:03 . 96 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 16032 ?? S 0:03 . 88 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 16641 ?? S 0:03 . 46 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 17228 ?? S 0:03 . 43 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 .	2	0
0 0 0.4 0.0 0.7 0.5 1.0 1.0 0.3333333333333333 0.0 0.9512195121951219 164 1.0 3 7 8 36	site : Add more troubleshooting and debugging commands . when someone asks how to debug a problem in minikube , we should provide all the deubbing commands we do , our current troubleshooting page doesn't have all the commands an advanced user can do < URL > we should add all the post mortem commands we do in our integration tests to our troubleshoting page here < URL > < URL >  	2	2
2 2 1.2 2.0 1.0 1.0 1.15 1.5 1.3333333333333333 2.0 0.0 0 0.0 1 4 11 30	`minikube config set embed-certs true` does have no effect . Steps to reproduce the issue : Run : minikube config set embed-certs true . . Verify that it has been set : : minikube config get embed-certs . Restart minikube . View : $HOME/ . kube/config . . There is no field : user . client-certificate-data . in the : minikube . profile . Expected behavior The certificate should be embedded directly in the : minikube . profile inside : $HOME/ . kube/config . . However , starting the cluster using : minikube start -- embed-certs . will work .  	1	1
2 2 1.4 2.0 1.1 1.0 1.05 1.0 1.0 1.0 1.75 4 2.0 0 1 5 27	hyperv : failure : start : exit status 1 ( not running minikube as admin ? ) . The exact command to reproduce the issue : : minikube start -- vm-driver = hyperv . The full output of the command that failed : : $ minikube start -- vm-driver = hyperv * minikube v 1.4.0 on Microsoft Windows 10 Enterprise 10.0.17763 Build 17763 * Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . E1015 10:10:29 . 795414 2708 cache_images . go : 79 ] CacheImage kubernetesui/dashboard : v 2.0.0 -beta4 -> C : /Users/balintp/ . minikube/cache/images/kubernetesui/dashboard_v 2.0.0 -beta4 failed : fetching image : unrecognized HTTP status : 503 Service Unavailable * Starting existing hyperv VM for ' minikube ' ... * Retriable failure : start : exit status 1 * Deleting ' minikube ' in hyperv ... * Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . * Starting existing hyperv VM for ' minikube ' ... * Retriable failure : start : exit status 1 * Deleting ' minikube ' in hyperv ... * Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . * Starting existing hyperv VM for ' minikube ' ... * Retriable failure : start : exit status 1 * Deleting ' minikube ' in hyperv ... . The output of the : minikube logs . command : n/a The operating system version : Microsoft Windows 10 Enterprise 10.0.17763 Build 17763  	1	0
1 1 1.4 2.0 1.4 2.0 1.4 2.0 1.0 1.0 0.8994708994708994 189 1.0 0 3 11 51	improve UI advice when user needs to delete the cluster . here we confuse the user with two things 閴?You cannot change the memory size for an exiting minikube cluster . Please first delete the cluster . and 棣冩▼ Failed to start ssh bare metal machine . Running ' minikube delete -p foo ' may fix it : config : please provide an IP address we should had sticked with first advice : drewpca(pts/0 ): ~ % minikube start -p foo -- memory 6GB 棣冩 [ foo ] minikube v 1.17.1 on Debian rodete 閴?Using the ssh driver based on existing profile 閴?You cannot change the memory size for an exiting minikube cluster . Please first delete the cluster . 棣冩啢 Starting control plane node foo in cluster foo 棣冦亞 StartHost failed , but will try again : config : please provide an IP address 棣冩▼ Failed to start ssh bare metal machine . Running ' minikube delete -p foo ' may fix it : config : please provide an IP address 閴?Exiting due to GUEST_PROVISION : Failed to start host : config : please provide an IP address 棣冩▼ If the above advice does not help , please let us know : 棣冩啝 < URL > .   	1	0
1 1 1.6 2.0 1.3 1.5 1.3 1.5 1.6666666666666667 2.0 0.0 0 0.0 1 2 3 17	ingress-dns addon docs/setup . The documentation for ingress-dns is confusing . It looks like it is instructing users to use resolvconf to configure the new DNS server and then disable resolvconf . It would be nice to either ( or both ) explain what is being done for each of these steps and why , as well as have some systemd only instructions , like : : sudo systemd-resolve -- interface $BRIDGEINTERFACE -- set-dns $MINIKUBE_IP -- set-domain test . I also noticed that dnsmasq was running in the background , and that there are minikube configurations there . This obviously cannot run at the same time with systemd-resolved running . There's little docs on that setup or how to use it , or trade-offs between this solution and using dnsmasq . Steps to reproduce the issue : minikube addons enable ingress-dns Follow Docs : < URL >  	2	2
0 0 0.8 1.0 1.3 1.5 0.95 1.0 1.0 1.0 2.0 1 2.0 3 4 10 30	`minikube kubectl` -- arguments and stdin for `apply -f -` not possible . The exact command to reproduce the issue : : $ minikube kubectl version -- short . : $ echo ' foo:' | minikube kubectl apply -f - . : $ echo ' foo:' | minikube kubectl -- apply -f - . : $ echo ' foo:' | minikube kubectl apply -- -f - . : $ minikube -- kubectl version -- short . The full output of the command that failed : : Error : unknown flag : -- short . : Error : unknown shorthand flag : ' f ' in -f . : error : no objects passed to apply . : error : no objects passed to apply . : minikube version : v 1.1.0 . The expected output of the command that failed : Just like : minikube kubectl -- version -- short . : : Client Version : v 1.11.8 Server Version : v 1.11.8 . Just like : echo ' foo:' | kubectl apply -f - . : : error : error validating ' STDIN ' : error validating data : [ apiVersion not set , kind not set ]; if you choose to ignore these errors , turn validation off with -- validate = false . Just like : echo ' foo:' | kubectl apply -f - . : : error : error validating ' STDIN ' : error validating data : [ apiVersion not set , kind not set ]; if you choose to ignore these errors , turn validation off with -- validate = false . Just like : echo ' foo:' | kubectl apply -f - . : : error : error validating ' STDIN ' : error validating data : [ apiVersion not set , kind not set ]; if you choose to ignore these errors , turn validation off with -- validate = false . Just like : minikube kubectl -- version -- short . : : Client Version : v 1.11.8 Server Version : v 1.11.8 . The output of the : minikube logs . command : The operating system version : MacOS vmdriver = hyperkit kubectl version v 1.11.8 on v 1.11.8 k8s cluster , installed via : minikube kubectl .	2	0
2 2 0.4 0.0 0.7 0.0 0.95 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 4 20	Add embedding gcflags from environment variable in Makefile . While hacking around the codebase I noticed there's no way to embed gcflags into binary build command in Makefile : < URL > I wanted to debug the code via GoLand ( < URL > which requires the user to set certain gcflags and I had to hardcode the flags into Makefile and then remove them before submiting a PR . This is cumbersome . What I propose is to enable embedding gcflags in Makefile via environment variable , similarly to : -ldflags='$(MINIKUBE_LDFLAGS )' . Let me know what you think and I can take a stab at implementing this . Thanks !	2	1
2 2 1.4 2.0 1.5 2.0 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 2 4 6 12	Support multiple -- mount-string arguments . When run with the docker driver , minikube takes a : -- mount-string . argument . Provided the minikube container does not yet exist , it is created with the specified host folder or docker volume mounted at the specified location e.g. : minikube start -- mount -- mount-string ' host_volume : /mount_point ' . Mounts the : host_volume . docker volume at : /mount_point . in the minikube container . In fact the : -- mount-string . flag can be specified multiple times , but only the last specified is used . A very small change to : cmd/minikube/cmd/start_flags . go . would allow multiple : -- mount-string . arguments to be used .	2	1
2 2 1.6 2.0 1.5 2.0 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 3 7 13 52	Minikube fails on KVM2 on fresh minikube installation on Fedora 33 ( with solution ) . Steps to reproduce the issue : Follow instructions from < URL > Install minikube instructions from < URL > to install minikube Run minikube with : $ minikube config set vm-driver kvm2 $ minikube start -- memory 4096 Full output of failed command : Exiting due to PROVIDER_KVM2_ERROR : /usr/bin/virsh domcapabilities -- virttype kvm failed Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : The resolution for the problem was doing two things : 1 . Adding systemd . unified_cgroup_hierarchy = 0 in the kernel command line ( via grubby ) . 2 . Adding my users to appropriate libvirt and kvm groups sudo usermod -aG kvm $USER sudo usermod -aG libvirt $USER Perhaps it's worth adding somewhere to documentation , but i don't know where - there is no page with ' common installation problems ' .  	2	2
2 2 1.6 2.0 1.7 2.0 1.45 2.0 2.0 2.0 0.9741935483870968 155 1.0 1 1 7 23	Add support for windows server data center for HyperV driver .  	1	1
0 0 0.8 0.0 1.0 1.0 0.85 0.5 0.6666666666666666 0.0 1.11 100 1.0 1 9 21 74	document in the contributing section about cgo and libvirt . on a fresh ubuntu computer I had to install these to be able to make lint install cgo < URL > install libvirt sudo apt-get install -y libvirt-dev    	1	2
2 2 1.4 2.0 1.2 1.5 1.15 1.0 1.0 1.0 0.0 0 0.0 6 7 9 57	Docker : Failed to setup kubeconfig : inspect IP bridge network . srahmed @hp : ~ $ minikube start 棣冩 minikube v 1.10.1 on Ubuntu 20.04 閴?Automatically selected the docker driver 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 2200MB ) ... 棣冩儞 Preparing Kubernetes v 1.18.2 on Docker 19.03.2 ... E0513 19:32:35 . 665322 20138 start . go : 95 ] Unable to get host IP : inspect IP bridge network ' de7f841a3590/n13c6d9205f9b ' . : docker inspect -- format ' {{( index . IPAM . Config 0 ) . Gateway }}' de7f841a3590 13c6d9205f9b : exit status 1 stdout : stderr : Error : No such object : de7f841a3590 13c6d9205f9b 棣冩寴 failed to start node : startup failed : Failed to setup kubeconfig : inspect IP bridge network ' de7f841a3590/n13c6d9205f9b ' . : docker inspect -- format ' {{( index . IPAM . Config 0 ) . Gateway }}' de7f841a3590 13c6d9205f9b : exit status 1 stdout : stderr : Error : No such object : de7f841a3590 13c6d9205f9b 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL >	0	0
1 1 1.2 1.0 1.4 1.5 1.4 2.0 1.0 1.0 1.1875 48 1.0 0 1 7 63	Increase default disk image allocation ? . Something that we discussed about the Docker VM , which allocates 2 GB RAM + 1 GB swap : As discussed in < URL > Maybe it is time to increase our default memory allocation to 4 GB , at least where available ? Similarly , we could up the default disk image to 40 GB , since it will only occupy as much as used . The idea would be to have something like computer games , ' recommended ' vs ' minimum ' : < URL > Minimum : 2 CPUs or more 2GB of free memory 20GB of free disk space Recommended : 4 CPUs or more 4GB of free memory 40GB of free disk space Currently we follow the recommendations from : kubeadm . , but it seems to be a bit low ? < URL > Before you begin : 2 GB or more of RAM per machine ( any less will leave little room for your apps ) 2 CPUs or more . We already use more memory than this , so it is mostly about the documentation . : 棣冩暉 Creating virtualbox VM ( CPUs = 2 , Memory = 6000MB , Disk = 20000MB ) ... . : 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 8000MB ) ... . The hard requirement is currently 1000MB , but that requires swap to even ' function ' . Ideally this would be followed up by an analysis of where the memory is actually going ... Like monitor the memory usage over time , like we did previously with the cpu usage ?  	2	2
0 0 0.8 0.0 1.1 1.5 1.2 2.0 0.6666666666666666 0.0 0.9824561403508771 57 1.0 0 2 9 20	logs no longer include libmachine commands , making hyperv impossible to debug . We used to have logs for Hyper-V and Virtualbox machine drivers that showed each command run . Where'd they go ?	0	0
2 2 1.8 2.0 1.6 2.0 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 1 4 31	update-context not working when using KUBECONFIG . The exact command to reproduce the issue : 閴?sudo -i env CHANGE_MINIKUBE_NONE_USER = true MINIKUBE_HOME = /home/somewhere KUBECONFIG = /tmp/ . somewhereelse/config minikube start -- vm-driver = none ... 閴?KUBECONFIG = /tmp/ . somewhereelse/config minikube start ... 閴?KUBECONFIG = /tmp/ . somewhereelse/config minikube status host : Running kubelet : Running apiserver : Running kubeconfig : Configured 閴?KUBECONFIG = /tmp/ . somewhereelse/config minikube update-context The full output of the command that failed : 棣冩寴 update config : Kubeconfig does not have a record of the machine cluster 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The output of the : minikube logs . command : No new logs are logged when : update-context . is run . The operating system version : centos 7 If I run : KUBECONFIG = /tmp/ . somewhereelse/config kubectx . , : minikube . context is present though . The idea was to run : minikube update-context . ( without the KUBECONFIG env variable ) so that my default kubectx would know about : minikube . context too . But that fails too with the same output pasted above .	2	0
1 1 0.6 0.0 0.8 0.5 1.05 1.0 1.0 1.0 1.2727272727272727 11 2.0 0 0 1 6	Allow users to pass kubeadm flags to ignore SystemVerification . To help with future situations such as < URL >	2	1
2 2 1.6 2.0 1.4 2.0 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 3 9 17 41	Include JSON Output as a standard option . . the : -- output . flag does not seem to be offered with most of the commands . : start . has it , but : pause . , : unpause . , and : stop . do not . Is it possible to make this flag available to all commands as an inherited option ? or at least more commands ?	0	1
0 0 0.6 1.0 0.5 0.0 0.85 1.0 0.3333333333333333 0.0 0.0 0 0.0 3 6 10 23	Add ability to run amd64 binary on M1 . Steps to reproduce the issue : 1 . Install docker for m1 : < URL > . 2 . Install Minikube : curl -LO < URL > sudo install minikube-darwin-amd64 /usr/local/bin/minikube . run minikube start minikube start log < URL > minikube logs < URL >	0	0
1 1 1.0 1.0 0.9 1.0 1.05 1.0 1.3333333333333333 1.0 0.9912280701754386 114 1.0 0 2 6 28	-- delete-on-failure does not regenerate configuration . -- delete-on-failure is unable to recover from issues like #8325 because the configuration file is not regenerated .	0	0
0 0 1.0 1.0 1.5 2.0 1.15 1.5 1.0 1.0 0.8387096774193549 217 1.0 2 4 6 23	Periodically tell user about minikube features/tips and tricks . we should have a Random Tips that on each time they start minikube it will tell them about a feature of minikube ... ( the Tips of the day should be a way that u could disable it ) : $ minikube start 棣冩 minikube v 1.19.0 on Darwin 11.2.3 閴?Using the docker driver based on existing profile 棣冩啢 Starting control plane node minikube in cluster minikube 棣冨籍 Updating the running docker ' minikube ' container ... 棣冩儞 Preparing Kubernetes v 1.20.2 on Docker 20.10.5 ... 棣冩敺 Verifying Kubernetes components ... 閳?Using image gcr.io/k8s-minikube/storage-provisioner:v5 棣冨皞 Enabled addons : storage-provisioner , default-storageclass 棣冨及 Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default ----- Tip of the day --------------------------------------------------------------------------- | Some Fancy fox UI : | Did you know you can specify any kubernetes version you want ? for example minikube start -- kubernetes-version = v 1.19.0 | | to disable this Tips Run .... minikube config set BLAHBLAH ----------------------------------------------------------------------------------------------- . or : ----- Tip of the day --------------------------------------------------------------------------- Did you know you can enroll getting notifcation for beta releases by minkube config set .... ? | to disable tips of the day run .... minikube config set BLAHBLAH ----------------------------------------------------------------------------------------------- . this will require us to have a database that has list of tips and tricks , file that can be added to our website automatically using : make generate-doc .  	1	1
2 2 1.4 2.0 1.3 2.0 1.35 2.0 1.6666666666666667 2.0 0.0 2 0.0 8 8 14 37	Add last start ( verbose ) logs to minikube logs command .	0	1
1 1 0.8 1.0 0.8 1.0 0.95 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 3 4 9	Debian package should include the kvm driver ( docker-machine-driver-kvm2 ) . Is this a BUG REPORT or FEATURE REQUEST ? : FEATURE REQUEST The minikube-XXX- . deb package is intended to be used on debian systems . To simplify setup and usage of kvm2 driver , it could be include within the package ( maybe provided through private debian repository ) . Thus the installation and upgrade of minikube will be more convenient . Please provide the following details : Environment : Linux Minikube version : 0.27 and 0.28 - OS : Ubuntu 18.04 - VM Driver : KVM2 What happened : I need to install and maintain ( upgrade ) docker-machine-driver-kvm2 manually as sudoer . What you expected to happen : Installing/upgrading the minikube through debian package , the docker-machine-driver-kvm2 is installed/upgraded accordingly .	0	1
