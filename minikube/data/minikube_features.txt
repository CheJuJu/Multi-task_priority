2 2 1.4 2.0 1.0 1.0 1.25 1.0 1.0 1.0 0.9657534246575342 146 1.0 1 2 5 35	  figure out choco install proccess .   	1	2
0 0 0.8 1.0 1.4 2.0 1.1 1.0 0.6666666666666666 1.0 1.2549019607843137 51 1.0 1 1 5 36	Prevent minikube profile command to create profile . one un-intentional side effect of minikube profile command is creating a new profile . minikube profiles should only be created by minikube start . so if you by mistake do : minikube profile lis . ( instead of list ) it will create an abandoned profile called list . I would like to disable the creation of profile by minikube profile command . however , ' minikube profile pname ' should still change the profile to ' pname ' if it exists . but should not create a new one . and it shoud also send a nice message to the user if it doesnt exist . ' The profie lis doesnt exist , if you want to create a profile you can by this command minikube start -p lis '	0	1
1 1 1.2 1.0 1.2 1.0 1.4 2.0 1.3333333333333333 1.0 0.0 0 0.0 0 0 1 5	Allow setting `extra-config` in config file . minikube v 0.32.0 : extra-config . is a verbose command line flag . Unfortunately , it can't be persisted in the minikube configuration file . Perhaps this is deliberate , perhaps it's an omission or perhaps there's a technical reason why it can't be easily implemented .	2	1
1 1 1.0 1.0 1.2 1.0 1.0 1.0 1.3333333333333333 1.0 1.144927536231884 69 1.0 2 6 14 55	none driver : support non systemd ways to restart docker . currently if u run none driver on a system that does have docker but it is not systemd it wont work ! because we expect docker restart to use systemctl ( and init . d scripts wont work ) : // Restart restarts Docker on a host func ( r * Docker ) Restart () error { c : = exec . Command('sudo ' , ' systemctl ' , ' restart ' , ' docker ' ) if _ , err : = r . Runner . RunCmd(c ); err ! = nil { return errors . Wrap(err , ' restarting docker . ' ) } return nil } . < URL > we could try do restart docker service using ' service restart docker ' if there is no systemctl service file for docker .  	1	1
2 2 1.8 2.0 1.7 2.0 1.45 2.0 2.0 2.0 0.0 0 0.0 1 2 7 28	service hello-world : services ' hello-world ' not found . HI Guys We are learning kubernetes by running through the online tutorial < URL > We've been using the Windows 10 version . All works well except for the command kubectl service , the exact command being kubectl service hello-world . Whenever we run this command we get the following error Any help would be useful Thanks Sel   	1	0
0 0 0.6 0.0 0.7 0.5 0.9 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 0 1 8	Minikube not automatically added to PATH when using installer ( Win7 , PATH truncation ? ) . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Bug report Environment : Minikube version ( use : minikube version . ): 0.28.2 - OS ( e.g. from /etc/os-release ): Windows 7 - Install tools : Windows installer What happened : I had not previously installed minikube . I downloaded and ran the Windows installer ( minikube-installer . exe ) The installer ran , and various files were added to C : /Program Files ( x86)/Kubernetes/Minikube minikube . exe was not added to my PATH . What you expected to happen : I expected minikube to be automatically added to my PATH , as per the installtion guide : Windows Installer [ Experimental ] Download the minikube-installer . exe file , and execute the installer . This will automatically add minikube . exe to your path with an uninstaller available as well . How to reproduce it ( as minimally and precisely as possible ): Download and run the installer on Windows 7 , for a fresh install .	1	0
2 2 1.0 1.0 1.1 1.0 1.2 1.0 1.6666666666666667 2.0 0.5 6 0.0 3 7 15 53	potential nil pointer dereference error . in cmd . validateDriver () , there could be a nil pointer dereference error : < URL > /assign	0	0
2 2 1.4 2.0 1.1 1.5 1.35 2.0 1.6666666666666667 2.0 0.0 0 0.0 3 3 3 8	kubelet : stat /etc/kubernetes/bootstrap-kubelet . conf : no such file or directory ( harmless race ) . minikube v 0.31 logs Jan 19 13:36:23 minikube kubelet[3037 ]: F0119 13:36:23 . 441215 3037 server . go : 262 ] failed to run Kubelet : unable to load bootstrap kubeconfig : stat /etc/kubernetes/bootstrap-kubelet . conf : no such file or directory	2	0
0 0 1.0 1.0 1.2 1.5 1.35 2.0 1.0 1.0 0.0 0 0.0 0 1 3 21	' minikube stop options ' not work . The exact command to reproduce the issue : : minikube stop options . The full output of the command that failed : ` 閴?~ minikube stop options 閴?Stopping ' minikube ' in hyperkit ... 棣冩磧 ' minikube ' stopped . ` Expected : ` 閴?Desktop minikube stop -h Stops a local kubernetes cluster running in Virtualbox . This command stops the VM itself , leaving all files intact . The cluster can be started again with the ' start ' command . Usage : minikube stop [ flags ] [ options ] Use ' minikube stop options ' for a list of global command-line options ( applies to all commands ) . ` Version : ` 閴?Desktop minikube version minikube version : v 1.5.2 commit : 792dbf92a1de583fcee76f8791cff12e0c9440ad ` The operating system version : ` 閴?Desktop uname -rsv Darwin 19.2.0 Darwin Kernel Version 19.2.0 : Sat Nov 9 03:47:04 PST 2019 ; root : xnu- 6153.61.1 ~ 20/RELEASE_X86_64 ` By the way , is there any way to only start the ' Docker Host ' instead of ' Docker Host ' + ' Kubelet ' + ' apiserver ' ? Because it consume 50+% cpu while idle which cause a lot of heat on my notebook :)	2	0
1 1 0.6 1.0 1.0 1.0 1.1 1.0 0.6666666666666666 1.0 1.0508474576271187 118 1.0 0 7 16 51	docker-env stops working after IP change . this is on docker-driver but it could be true on podman and kvm as well . : med @xmac : ~ /workspace/minikube ( windows_gh_action)$ eval $(minikube docker-env ); docker images error during connect : Get < URL > x509 : certificate is valid for 172.17.0.3 , 127.0.0.1 , not 172.17.0.2 . replicate : # ! /bin/bash for i in { 1 .. 5 } do echo ' try $i ' minikube start -p minikube minikube start -p p1 minikube start -p p2 eval $(minikube docker-env -p minikube ) docker images eval $(minikube docker-env -p p1 ) docker images eval $(minikube docker-env -p p2 ) docker images minikube stop -p minikube & minikube stop -p p1 & minikube stop -p p2 systemctl restart docker done . solution restaring the docker inside minikube makes docker to pick the certs .	0	0
2 2 1.2 2.0 1.3 2.0 1.3 1.5 1.3333333333333333 2.0 1.5 2 1.5 0 6 12 54	docker : stop/delete hung at libmachine : Stopping ' [ my-profile ]' ... . minikube version 1.7.3 : minikube status -p [ my-profile ] -- alsologtostderr -v = 1 . : : root . go : 242 ] Error reading config file at /Users/[]/ . minikube/config/config . json : open /Users/[]/ . minikube/config/config . json : no such file or directory . : minikube stop -p [ my-profile ] . : W0309 21:31:08 . 535489 66523 root . go : 242 ] Error reading config file at /Users/[]/ . minikube/config/config . json : open /Users/[]/ . minikube/config/config . json : no such file or directory 閴?Stopping ' [ my-profile ]' in docker ... I0309 21:31:08 . 536613 66523 main . go : 110 ] libmachine : Stopping ' [ my-profile ]' ... . : delete -- all -- purge . also shows the same error , and hangs . It was working for several command before this but somehow got into this state .	2	0
1 1 0.6 1.0 0.8 1.0 0.95 1.0 0.6666666666666666 1.0 0.8205128205128205 234 1.0 3 7 7 24	add `CII best practices` badge to github readme . simmilar to kuberentes/kubernetes [ ]    	1	2
2 2 1.6 2.0 1.4 2.0 1.05 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 1 27	Prefix informational messages with the current timestamp . It is great if minikube puts a timestamp in its informational messages by default . The exact command to reproduce the issue : minikube start The full output of the command that failed : 棣冩 minikube v 1.1.0 on darwin ( amd64 ) 棣冩崚 Downloading Minikube ISO ... 131.28 MB / 131.28 MB [= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ==] 100.00% 0s 棣冩暉 Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... 棣冩儞 Configuring environment for Kubernetes v 1.14.2 on Docker 18.09.6 棣冩崙 Downloading kubeadm v 1.14.2 棣冩崙 Downloading kubelet v 1.14.2 棣冩 Pulling images ... 棣冩畬 Launching Kubernetes ... [ the last step has been hanging for a long time and I am not sure when exactly it started ] The output of the : minikube logs . command : N/A The operating system version : macOS 10.14.5	2	1
0 0 1.0 1.0 0.9 1.0 0.95 1.0 1.0 1.0 0.9681528662420382 157 1.0 3 7 9 27	fix Readme Build Badge . after removing Travis the new build icon is pointing to integration tests instead of build < URL > side effect of < URL >    	1	2
2 2 1.6 2.0 1.2 2.0 1.1 1.5 1.3333333333333333 2.0 0.7777777777777778 9 1.0 2 10 14 48	  Add walkthrough for using multinode to docs . Should just be a step by step walkthrough on using a multi-node cluster , similar to our < URL > .	0	2
1 1 1.0 1.0 0.9 1.0 0.9 1.0 1.0 1.0 2.0 1 2.0 1 2 6 27	minikube docker-env doesn't understand powershell core . Steps to reproduce the issue : Install PowerShell core ( < URL > Run powershell core ( : pwsh . exe . ) : minikube start ... . : minikube docker-env . Full output of failed command : : 閴?minikube docker-env You can further specify your shell with either ' cmd ' or ' powershell ' with the -- shell flag . SET DOCKER_TLS_VERIFY = 1 SET DOCKER_HOST = tcp :/ / 192.168.184.153 : 2376 SET DOCKER_CERT_PATH = C : /Users/Me/ . minikube/certs SET MINIKUBE_ACTIVE_DOCKERD = minikube REM To point your shell to minikube's docker-daemon , run : REM @FOR /f ' tokens=*' %i IN (' minikube -p minikube docker-env ' ) DO @%i . Expected output : 閴?minikube docker-env -- shell powershell $Env :D OCKER_TLS_VERIFY = ' 1 ' $Env :D OCKER_HOST = ' tcp :/ / 192.168.184.153 : 2376 ' $Env :D OCKER_CERT_PATH = ' C : /Users/Me/ . minikube/certs ' $Env : MINIKUBE_ACTIVE_DOCKERD = ' minikube ' # To point your shell to minikube's docker-daemon , run : # & minikube -p minikube docker-env | Invoke-Expression . Note how passing in : -- shell powershell . yielded the correct results , but using the default auto-detect did not detect powershell . Note : running on regular powershell works just fine . This issue only affects powershell core .	2	0
1 1 0.4 0.0 0.8 1.0 1.05 1.0 0.3333333333333333 0.0 0.0 0 0.0 1 1 1 16	Add WAIT cmd implementation . . Is this a FEATURE REQUEST I will be glad to implement : wait . command in minikube . Do we have any objections ? Or problems related to this topic ?	2	1
1 1 1.0 1.0 0.9 1.0 1.25 1.0 1.0 1.0 0.9523809523809523 21 1.0 2 4 16 34	Hitting Ctrl-C on `minikube mount` fails to unmount volume . : % /usr/local/bin/minikube mount /tmp/700 : /tmp/700 棣冩惂 Mounting /tmp/700 into /tmp/700 on the minikube VM 棣冩惗 This daemon process needs to stay alive for the mount to be accessible ... ufs starting . If you hit Ctrl-C now , the VM still thinks the volume is mapped : : $ mount | grep 700 | grep /tmp/700 192.168.39.1 on /tmp/700 type 9p ( rw , relatime , sync , dirsync , dfltuid = 1001 , dfltgid = 1001 , access = any , msize = 65536 , trans = tcp , noextend , port = 43403 ) $ stat /tmp/700 stat : cannot stat ' /tmp/700 ' : Input/output error . This means that subsequent changes to the mount point , such as uid/gid , might result in confusion within the host VM .  	1	0
2 2 1.2 2.0 1.4 2.0 1.3 2.0 1.3333333333333333 2.0 1.2142857142857142 42 1.0 1 7 9 33	kic : add podman driver for minikube . a couple month ago we did an early prototype with podman there was a few issues . @josedonizetti once docker driver is stable , we should add podman  	1	1
0 0 0.8 1.0 0.8 0.5 1.0 1.0 0.3333333333333333 0.0 0.0 0 0.0 2 5 7 21	Enable version switcher on docs site . Running : minikube start -- vm-driver = kvm2 . without : docker-machine-driver-kvm2 . fails , as expected , but the new documentation URL does not provide installation steps for the driver . Specifically , < URL > does not provide any information on how to install : docker-machine-driver-kvm2 . , nor a download link .  	2	2
0 0 0.8 1.0 0.9 1.0 0.75 1.0 1.0 1.0 0.8230088495575221 226 1.0 1 2 3 22	minikube should let user know if Global Config wont be persistant . we should have a list of configs that we can set globally and add it to the documentation and we should make the memory global config-able : 閴?minikube delete 棣冩暉 Deleting ' minikube ' in hyperkit ... 棣冩媰 Removed all traces of the ' minikube ' cluster . ~ /git/jenkins-gce-gitops main 椤?椤?03:06:13 PM 閴?minikube config set memory 12g 閴?These changes will take effect upon a minikube delete and then a minikube start ~ /git/jenkins-gce-gitops main 椤?椤?03:06:17 PM 閴?minikube start 棣冩 minikube v 1.18.1 on Darwin 11.4 閴?Using the hyperkit driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating hyperkit VM ( CPUs = 4 , Memory = 6000MB , Disk = 20000MB ) ... | Vic Iglesias , 14 min it sets the flag right though 閴?minikube start -- memory 12g 棣冩 minikube v 1.18.1 on Darwin 11.4 閴?Using the hyperkit driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating hyperkit VM ( CPUs = 4 , Memory = 12288MB , Disk = 20000MB ) ... | .	0	0
2 2 1.2 1.0 1.3 1.5 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 2 6 27	Document which versions of Kubernetes are supported by minikube . Because : minikube get-k8s-versions . was removed with 0.30.0 , issue #1201 should be reopened .  	2	2
1 1 1.0 1.0 1.1 1.5 1.3 1.5 1.0 1.0 0.0 0 0.0 5 9 23 77	Improve error handling on ARM architectures . Can anyone instruct me a little bit about what is going on ? I am working with : ARM64 CentOS 4.18.0 - 80.7.2 . el7 . aarch64 Steps to reproduce the issue : wget < URL > ln -s minikube-linux-arm64 minikube minikube start Full output of failed command : : [ user1 @arm64 -server ~ ]$ minikube start * minikube v 1.9.2 on Centos 7.7.1908 ( arm64 ) * Using the docker driver based on existing profile * Starting control plane node m01 in cluster minikube * Pulling base image ... * Restarting existing docker container for ' minikube ' ... ! StartHost failed , but will try again : provision : get ssh host-port : get host-bind port 22 for ' minikube ' , output Template parsing error : template : : 1:4 : executing ' at < index ( index . NetworkSettings . Ports ' 22/tcp ' ) 0 > : error calling index : index of untyped nil : exit status 1 * Restarting existing docker container for ' minikube ' ... * X Failed to start docker container . ' minikube start ' may fix it . : provision : get ssh host-port : get host-bind port 22 for ' minikube ' , output Template parsing error : template : : 1:4 : executing ' at < index ( index . NetworkSettings . Ports ' 22/tcp ' ) 0 > : error calling index : index of untyped nil : exit status 1 * * minikube is exiting due to an error . If the above message is not useful , open an issue : - < URL > . Full output of : minikube start . command used , if not already included : As above . Optional : Full output of : minikube logs . command : : [ user1 @kunpeng920 ~ ]$ minikube logs * The control plane node must be running for this command - To fix this , run : ' minikube start ' .  	1	0
0 0 0.8 0.0 1.2 1.5 1.35 2.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 1 1	Add support for linux s390x . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Please provide the following details : Environment : Linux s390x Minikube version ( use : minikube version . ): v 0.27.0 - OS ( e.g. from /etc/os-release ): NAME='Ubuntu ' VERSION='16 . 04.4 LTS ( Xenial Xerus )' ID = ubuntu ID_LIKE = debian PRETTY_NAME='Ubuntu 16.04.4 LTS ' VERSION_ID='16 . 04 ' HOME_URL='<URL>' SUPPORT_URL='<URL>' BUG_REPORT_URL='<URL>' VERSION_CODENAME = xenial UBUNTU_CODENAME = xenial VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): none ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): - Install tools : Others : The above can be generated in one go with the following commands ( can be copied and pasted directly into your terminal ): : minikube version echo ' ; echo ' OS:'; cat /etc/os-release echo ' ; echo ' VM driver ' : grep DriverName ~ / . minikube/machines/minikube/config . json echo ' ; echo ' ISO version ' ; grep -i ISO ~ / . minikube/machines/minikube/config . json . What happened : minikube-linux-s390x binary is not available . What you expected to happen : Add support to release minikube-linux-s390x binary for linux/s390x . kubernetes and kubeadm has support for linux/s390x . Willing to work and provide linux/s390x support for minikube . Checked all testcases (./ test.sh ) and integration-non-driver . All are successful . How to reproduce it ( as minimally and precisely as possible ): Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	1
1 1 1.4 1.0 1.2 1.0 0.95 1.0 1.6666666666666667 2.0 2.0 2 2.0 3 7 11 40	crio + docker + wsl2 : Module br_netfilter not found in directory /lib/modules/ 4.19.128 -microsoft-standard . Steps to reproduce the issue : Run : minikube start -- container-runtime = cri-o . Get : modprobe : FATAL : Module br_netfilter not found in directory /lib/modules/ 4.19.128 -microsoft-standard . Full output of failed command : : 棣冩 minikube v 1.15.1 on Microsoft Windows 10 Pro 10.0.19041 Build 19041 閴?Automatically selected the docker driver 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩 Pulling base image ... 棣冩崙 Downloading Kubernetes v 1.19.4 preload ... > preloaded-images-k8s-v6-v 1.19.4 -cri- o-o verlay-amd64 . tar . lz4 : 551.20 MiB / 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 3885MB ) ... 閴?Exiting due to RUNTIME_ENABLE : br_netfilter : sudo modprobe br_netfilter : Process exited with status 1 stdout : stderr : modprobe : FATAL : Module br_netfilter not found in directory /lib/modules/ 4.19.128 -microsoft-standard 棣冩▼ If the above advice does not help , please let us know : . Full output of : minikube start . command used : < URL > I was trying to run minikube with cri-o as container runtime and docker as driver as I want to prepare for Kubernetes v . 1.20 deprecating the docker container runtime .	2	0
0 0 0.4 0.0 0.5 0.0 0.65 0.0 0.0 0.0 1.1547619047619047 84 1.0 2 2 6 34	Add architecture to the installation instructions . Currently hardcoded : Binary download : curl -LO < URL > sudo install minikube-linux-amd64 /usr/local/bin/minikube . Debian package : curl -LO < URL > sudo dpkg -i minikube_latest_amd64 . deb . RPM package : curl -LO < URL > sudo rpm -ivh minikube-latest . x86_64 . rpm . This needs to also support other architectures , starting with arm64 . Binary download : curl -LO < URL > sudo install minikube-linux-arm64 /usr/local/bin/minikube . Debian package : curl -LO < URL > sudo dpkg -i minikube_latest_arm64 . deb . RPM package : curl -LO < URL > sudo rpm -ivh minikube-latest . aarch64 . rpm .  	0	2
0 0 1.0 1.0 1.1 1.0 1.05 1.0 1.0 1.0 0.0 0 0.0 0 0 0 2	hyperv : Default to disabling dynamic memory ( -- hyperv-disable-dynamic-memory to driver ) . Is this a BUG REPORT or FEATURE REQUEST ? Feature Request Minikube version v 0.20.0 Environment : - OS Windows 10 - VM Driver Hyper-V Couple of issues already point out to problem with Hyper-V and dynamic memory : < URL > & < URL > Would it not be better to start the Hyper-V machine with dynamic memory turned off ? Currently this switch in not given in to Powershell configuration , but it would be a simple thing to do : -DynamicMemoryEnabled $true --- check here : < URL > I would be happy to send a pull request for this .	0	1
1 1 1.4 2.0 1.2 1.5 1.15 1.0 1.6666666666666667 2.0 1.2592592592592593 27 1.0 1 2 4 22	Upgrade podman and cri-o . Currently we are running : Podman 1.4 ( : v 1.4.4 . ) CRI-O 1.15 ( : v 1.15.2 . ) Should try to upgrade to : Podman 1.6 ( : v 1.6.3 . ) - < URL > CRI-O 1.16 ( : v 1.16.0 . ) - < URL >  	1	1
2 2 1.4 2.0 1.2 1.5 1.15 1.0 1.6666666666666667 2.0 0.9782608695652174 138 1.0 7 8 11 25	add spinner to Preparing Kubernetes that takes 20 seconds ... . users would like to be remineded that minikube is waiting on kubeadm init for more than 20 seconds .	0	1
2 2 1.0 1.0 1.5 2.0 1.35 2.0 0.6666666666666666 0.0 0.0 0 0.0 1 4 11 51	kvm2 driver support simulate numa node . k8s support numa docs : < URL > minikube can simulate numa node kvm2 driver add support command this mean the node had two numa node . these node will evenly distributed cpu core & memory . : minikube start -- driver = kvm2 -- kvm-numa-count = 2 閴?minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_) | |/') _ _ | |_ __ /' _ ` _ `/| |/' _ `/| || , < ( ) ( )| ' _`/ /'__`/ | ( ) ( ) || || ( ) || || |/`/ | (_) || |_) )( ___/ (_) (_) (_) (_) (_) (_) (_) (_) (_) `/___/'(_ , __/'`/____) $ /bin/toolbox Trying to pull docker :/ /fedora : latest ... Getting image source signatures Copying blob f147208a1e03 done Copying config a78267678b done Writing manifest to image destination Storing signatures 8818f2a63b341c3bcfa41fc6879115e692482d0d65c450029cc9bdcba5879c2d 8818f2a63b341c3bcfa41fc6879115e692482d0d65c450029cc9bdcba5879c2d Untagged : docker.io/library/fedora:latest Deleted : a78267678b7e6e849c7e960b09227b737a38d5073a5071b041a16bd4b609ef92 Spawning container docker-fedora-latest on /var/lib/toolbox/docker-fedora-latest . Press ^] three times within 1s to kill container . [ root @minikube ~ ]# yum install numactl -y [ root @minikube ~ ]# numactl -H available : 2 nodes ( 0-1 ) node 0 cpus : 0 1 2 3 node 0 size : 16034 MB node 0 free : 15349 MB node 1 cpus : 4 5 6 7 node 1 size : 16126 MB node 1 free : 13300 MB node distances : node 0 1 0 : 10 20 1 : 20 10 .	0	1
0 0 1.2 2.0 1.0 1.0 1.05 1.0 0.6666666666666666 0.0 1.2222222222222223 18 1.0 0 2 9 27	Document that none driver doesn't work with snap docker . Ubuntu 18.04 : sudo snap install docker . < URL > We should document that the snap installation of docker does not work with minikube ... : $ sudo systemctl status docker Unit docker . service could not be found . . And recommend the apt installation of docker ( from Docker ) , when using the : none . driver . See #4952  	2	2
2 2 1.0 1.0 0.6 0.0 0.8 0.5 0.6666666666666666 0.0 0.0 0 0.0 5 15 21 52	clarify error message with driver none . Steps to reproduce the issue : minikube start -- driver = none minikube fails to start ( which is expected ) minikube suggest a wrong command to fix the issue : : 棣冩寴 The ' none ' driver requires root privileges . Please run minikube using ' sudo minikube -- driver = none ' . . The error message lacks the necessary parameter start after minikube , it should print like this : : 棣冩寴The ' none ' driver requires root privileges . Please run minikube using ' sudo minikube start -- driver = none ' . Full output of failed command : Full output of : minikube start . command used , if not already included : 閴?Using the none driver based on existing profile 閴?' none ' driver reported an issue : the ' none ' driver must be run as the root user 棣冩寱 Suggestion : For non-root usage , try the newer ' docker ' driver 棣冩寴 The ' none ' driver requires root privileges . Please run minikube using ' sudo minikube -- driver = none ' .  	2	2
0 0 0.8 1.0 1.1 1.0 0.9 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 9 13 33	hyperv restart : waiting for component = kube-apiserver : timed out waiting for the condition . I have installed minikube for windows and made necessary changes for Hyper V but still I get this error when I try to restart ( I did delete and try again as well but no luck ) : PS C : /WINDOWS/system32 > minikube start -- vm-driver = hyperv -- hyper v-v irtual-switch / ' Primary Virtual Switch ' -- kubernetes-version='v1 . 13.0 ' o minikube v 0.35.0 on windows ( amd64 ) x Kubernetes downgrade is not supported , will continue to use v 1.13.4 i Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . : Re-using the currently running hyperv VM for ' minikube ' ... : Waiting for SSH access ... - ' minikube ' IP address is 192.168.1.155 - Configuring Docker as the container runtime ... - Preparing Kubernetes environment ... - Pulling images required by Kubernetes v 1.13.4 ... : Relaunching Kubernetes v 1.13.4 using kubeadm ... : Waiting for pods : apiserver ! Error restarting cluster : wait : waiting for component = kube-apiserver : timed out waiting for the condition .	2	0
1 1 1.0 1.0 1.0 1.0 1.2 1.0 1.3333333333333333 1.0 0.6 5 0.0 0 0 2 10	' minikube stop ' does not work properly with -- vm-driver = none . As noted here at < URL > Right now it seems that minikube start is the only command aware of -- vm-driver = none . Running minikube stop keeps resulting in errors related to docker-machine , and as luck would have it also results in none of the Kubernetes containers terminating , nor the kubelet service stopping . Of course , if you wish to actually terminate minikube , you will need to execute service kubelet stop and then ensure the k8s containers are removed from the output in docker ps .	2	0
2 2 1.0 1.0 1.2 1.5 1.2 1.5 1.3333333333333333 2.0 1.2241379310344827 58 1.0 2 7 17 49	implement service command for docker on mac os . I think we could use the same SSH connection that was added by @josedonizetti for making tunnel work for service , with one difference that this could be multiple service at random ports and not needed to make the ports map to same port .	0	0
1 1 0.8 1.0 1.3 1.0 1.4 1.5 1.0 1.0 1.2727272727272727 22 1.0 1 6 6 23	Use the minikube IP for the hostname . Currently libmachine sets the IP of the current host to 127.0.1.1 , matching Debian : /etc/hostname : minikube . /etc/hosts : 127.0.0.1 localhost 127.0.1.1 minikube . < URL > < URL > The IP address 127.0.1.1 in the second line of this example may not be found on some other Unix-like systems . The Debian Installer creates this entry for a system without a permanent IP address as a workaround for some software ( e.g. , GNOME ) as documented in the bug #719621 . The matches the hostname defined in the ' /etc/hostname ' . For a system with a permanent IP address , that permanent IP address should be used here instead of 127.0.1.1 . So we should really be using the same IP address as : minikube ip . uses , for /etc/hosts .	2	1
2 2 2.0 2.0 1.4 2.0 1.5 2.0 2.0 2.0 0.0 0 0.0 1 1 3 6	Missing docker . service . d/10-machine . conf file in /etc/systemd/system/ in minikube ssh ( version v 0.28.2 ) . ** Is this a BUG REPORT In v . 0.28.0 version of minikube the global conf file in docker . service . d/10-machine . conf was missing which is present in minikube version v 0.20.0 . Just have clarify if the file was replaced with some other name or location . OR it was missing because of some conf issue Environment : - Windows 10 In Virtual Box Expected Presence of docker . service . d/10-machine . conf in /etc/systemd/system/ in minikube ssh	2	0
0 0 0.8 1.0 1.0 1.0 0.9 1.0 0.3333333333333333 0.0 1.2307692307692308 52 1.0 1 4 7 40	minikube service for docker .	0	1
0 0 0.8 1.0 1.3 1.5 1.15 1.0 0.3333333333333333 0.0 0.8333333333333334 6 1.0 1 3 6 26	Docker_Linux_containerd_arm64 integration tests broken . Integration tests for Docker_Linux_containerd_arm64 are completely broken . The Jenkins job fails to install gotestsum . It also seems to have a segfault on updating apt-get ( unsure if related ) .	2	0
0 0 0.8 1.0 1.3 1.5 1.35 1.5 0.3333333333333333 0.0 1.3 20 1.5 1 1 3 27	Upgrade Buildroot to 2019.02 LTS . Currently minikube is running Buildroot 2018.05 , featuring Linux 4.16 ( downgraded to 4.15 ) These are now out of support , since they were short-lived and not supported for the long term . : BUILDROOT_BRANCH ? = 2018.05.3 . ( the previous long-term versions would have been Buildroot 2018.02 and Linux 4.14 ) We should upgrade to Buildroot 2019.02 , featuring Linux 4.19 ( both of them long term support ) Currently the minor versions of each are 2019.02.6 and 4.19.76 , but this might change later . : BUILDROOT_BRANCH ? = 2019.02.6 . Doing so , we should look into the VirtualBox guest additions module that caused issues : ~ #2986 ~ : VBOX_GUEST_VERSION = 5.1.38 . VirtualBox 5.1 . x is no longer supported ! < URL > : BR2_LINUX_KERNEL_CUSTOM_VERSION = y BR2_LINUX_KERNEL_CUSTOM_VERSION_VALUE='4 . 15 ' . VirtualBox 5.2.32 ( released July 16 2019 ) This also addresses #5007 Might also consider #5105 ? UPDATED , for new Buildroot release : 2019.02.5 , Released September 2nd , 2019 2019.02.6 , Released October 4th , 2019  	1	1
2 2 1.8 2.0 1.8 2.0 1.25 1.5 2.0 2.0 1.0 3 1.0 4 5 10 40	  Create a webpage that provides descriptions for minikube error codes . Create a webpage that provides descriptions for minikube error codes ( as well as tips and advice for corrective actions ) that users can refer to .	0	2
1 1 1.2 1.0 1.3 1.0 1.3 1.5 1.0 1.0 1.106060606060606 66 1.0 1 5 8 31	Build KIC base image for ARM . Once we have cleaned up the kicbase image and started to use regular packages , we can build for other architectures too . So we can have both : amd64 . and : arm64 . variants . Currently it is quite the mess to do it , since the docker daemon doesn't support architectures ( only buildx and registry does ) But it is doable , Ubuntu 20.04 LTS supports both arch .  	1	1
1 1 1.4 2.0 1.3 1.5 1.3 1.0 1.0 1.0 2.0 1 2.0 0 1 1 10	Better DNS support with vm = none . How we could better support coredns name resolving ? For systemd we could use something like : < URL > For docker containers that consume Minikube services we could suggest to to use : -- dns = . option to point to coredns ip . /cc @balopat	2	1
0 0 0.8 1.0 0.5 0.0 0.8 1.0 1.0 1.0 0.0 0 0.0 3 7 8 28	 cache add ' with sha256 sum : ' Failed to cache ... repository can only contain the runes ' . Right now the minikube cache command allows to cache only images with tag , it will be great to support caching images with SHAs   	1	0
1 1 1.4 1.0 1.3 1.5 1.25 2.0 1.3333333333333333 1.0 1.1020408163265305 98 1.0 6 12 22 70	docker driver : static ip , make container IPs survive restart . . Feature request : assign static IP for minikube docker/podman containers currently if we stop minikube , we might actually , have a different IP that will cause a kubeadm Reset ( expensive ) : I0417 15:04:19 . 699937 20147 kubeadm . go : 434 ] needs reset : configs differ : -- stdout -- --- /var/tmp/minikube/kubeadm . yaml 202 0-0 4-17 22:01:08 . 390045502 +0000 +++ /var/tmp/minikube/kubeadm . yaml . new 0001-01-01 00:00:00 . 000000000 +0000 @@ -1 , 7 +1 , 7 @@ apiVersion : kubeadm.k8s.io/v1beta1 kind : InitConfiguration localAPIEndpoint : - advertiseAddress : 172.17.0.4 + advertiseAddress : 172.17.0.2 bindPort : 8443 bootstrapTokens : - groups : @@ -14 , 25 +14 , 25 @@ criSocket : /var/run/crio/crio . sock name : ' crio-20200417T145706-10869 ' kubeletExtraArgs : - node-ip : 172.17.0.4 + node-ip : 172.17.0.2 taints : [] --- apiVersion : kubeadm.k8s.io/v1beta1 kind : ClusterConfiguration apiServer : - certSANs : [' 127.0.0.1 ' , ' localhost ' , ' 172.17.0.4 ' ] + certSANs : [' 127.0.0.1 ' , ' localhost ' , ' 172.17.0.2 ' ] . as seen here : < URL >  	1	1
1 1 0.6 0.0 1.0 1.0 1.2 1.5 0.3333333333333333 0.0 0.9791666666666666 48 1.0 2 3 7 30	tunnel on macOS : application sending entire HTML page ( packet loss ? ) . @priyawadhwa had a demo at SF Kubernetes yesterday where the application sent traffic properly using port forwarding , but with : minikube tunnel . , only the page title loaded . It seemed like a packet loss issue . @priyawadhwa - do you mind sharing the command-lines we can use to replicate it ?	2	0
0 0 1.4 2.0 1.4 2.0 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 2 4 41	add extended KVM2 installation documentation for redhat . This documentation does not work for RHEL/CentOS 8 out of the box with fresh install including group ' Virtualization Host ' . It would be great if the following could be added ... * add a new user * add new user to libvirt group * edit /etc/libvirt/libvirtd . conf * set unix_sock_group = ' libvirt ' * set unix_sock_rw_perms = ' 0770 ' * restart libvirtd    	1	2
1 1 0.6 1.0 1.3 1.5 1.15 1.0 0.6666666666666666 1.0 1.1232876712328768 73 1.0 1 2 10 55	Unable to update hyperkit driver : Error downloading checksum file : bad response code : 404 . on the head on mac os : first purge : medmac@ ~ /workspace/minikube ( master ) $ . /out/minikube delete -- all -- purge 棣冩暉 Successfully deleted all profiles 棣冩媰 Successfully purged minikube directory located at - [/Users/medmac/ . minikube ] . then : medmac@ ~ /workspace/minikube ( master ) $ . /out/minikube start 棣冩 minikube v 1.9.0 -beta . 2 on Darwin 10.13.6 閴?Automatically selected the hyperkit driver . Other choices : docker , virtualbox , vmwarefusion 棣冩崙 Downloading driver docker-machine-driver-hyperkit : 閴?Unable to update hyperkit driver : download failed : < URL > invalid checksum : Error downloading checksum file : bad response code : 404 棣冩崚 Downloading VM boot image ... > minikube-v 1.9.0 -beta . 2 . iso . sha256 : 65 B / 65 B [ ------- ] 100.00% ? p/s 0s > minikube-v 1.9.0 -beta . 2 . iso : 173.76 MiB / 173.76 MiB 100.00% 22.08 MiB p/ 棣冩崙 Downloading preloaded images tarball for k8s v 1.18.0 -rc . 1 ... > preloaded-images-k8s-v2-v 1.18.0 -rc . 1-docker-overlay2-amd64 . tar . lz4 : 542.8 . error : : 閴?Unable to update hyperkit driver : download failed : < URL > invalid checksum : Error downloading checksum file : bad response code : 404 .	0	0
0 0 1.2 1.0 1.3 1.5 1.25 1.5 1.0 1.0 0.0 0 0.0 3 8 19 52	dangerous default behaviour ( mounting home directory inside minikube VM ) . Hello , When starting a minikube instance : minikube start . by default , the : -- mount-string = . is set to home directory : ~ . of the host machine , this argument is passed to the minikube mount command on start , the home directory is mounted under : /minikube-host . ( mode -493 =>) . executing : rm -rf / . inside the minikube VM ( or with ssh ) would have a catastrophic effect ( ~ directory of the host machine deleted ) . Since minikube is made for testing purposes , executing deletion command is highly probable . I think this default behavior is very dangerous and should be changed . Regards UPDATE : This behavior is related to VirtualBox's driver , not to minikube's mount , disabling the fs mounts provided by the hypervisors should fix this behavior ( by setting disable-driver-mounts to true ) thanks to @afbjorklund for his remarks	2	0
2 2 1.4 2.0 1.0 1.0 1.05 1.0 1.0 1.0 1.6666666666666667 3 2.0 1 3 7 24	minikube sometimes fails to enable the gcp-auth addon . < URL > for more details . current version of minikube managed by the Cloud Code plugin : : 1.21.0 .  	1	0
0 0 0.8 1.0 0.8 1.0 0.8 1.0 1.0 1.0 0.9222222222222223 180 1.0 1 2 7 30	ARM64 : GCP Auth addon . add support for gcp-auth addon arm64	0	1
2 2 1.4 1.0 1.3 1.5 1.1 1.0 1.3333333333333333 1.0 0.0 0 0.0 2 4 11 57	custom addon images get ignored on minikube restart . ** Note this environment is entirely air-gapped . minikube start -- driver = docker -- base-image=' : 5000/ gcr.io/k8s-minikube/kicbase:v0.0.18 ' -- insecure-registry=' : 5000 ' -- image-repository=' : 5000 ' -- alsologtostderr 2 > & 1 >> < URL > Note this capability added with < URL > Steps to reproduce the issue : Start minikube for offline environment ( above ) Enable ingress addones ( with -- image flag ) < URL > Stop minikube Start Minikube ( with or without flags , same result ) < URL > Ingress container fails to start ( wrong image ) < URL > Full output of failed command : Alsologtostderr is already included . Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : Workaround : ( disabling and re-adding ingress fixes it ) 1 . minikube addons disable ingress 2 . minikube addons enable ingress -- images =	0	0
2 2 2.0 2.0 1.5 2.0 1.1 1.0 2.0 2.0 0.0 0 0.0 0 4 9 27	  a network Diagram for the VirtualBox . Please add a network Diagram for the VirtualBox based minikube to the readme .	2	2
2 2 0.8 0.0 1.1 1.5 1.1 1.0 0.6666666666666666 0.0 0.7 10 0.5 3 11 15 48	Allow multi control plane clusters by knowing which node is the api server . Right now we assume if a node is a control plane it's automatically is the api server , and that's not necessarily the case so we need to make sure the api server node is explicitly specified .	2	1
2 2 1.0 1.0 1.1 1.5 0.85 1.0 1.6666666666666667 2.0 0.9642857142857143 140 1.0 1 5 7 29	' cache add ' command : improve error message for not existing image . . : PS C : /Users/jenkins > minikube cache add doesnt_exis t_t hing : haha E0910 18:30:13 . 540553 3980 cache . go : 63 ] save image to file ' doesnt_exis t_t hing : haha ' -> ' C :/ /Users//jenkins// . minikube//cache//images//doesnt_exis t_t hing_haha ' failed : nil image for doesnt_exis t_t hing : haha : GET < URL > UNAUTHORIZED : authentication required ; [ map[Action : pull Class : Name : library/doesnt_exis t_t hing Type : repository ]] X Exiting due to MK_CACHE_LOAD : save to dir : caching images : caching image ' C :/ /Users//jenkins// . minikube//cache//images//doesnt_exis t_t hing_haha ' : nil image for doesnt_exis t_t hing : haha : GET < URL > UNAUTHORIZED : authentication required ; [ map[Action : pull Class : Name : library/doesnt_exis t_t hing Type : repository ]] * * If the above advice does not help , please let us know : - < URL > .	0	1
1 1 1.6 2.0 1.0 1.0 1.1 1.0 1.3333333333333333 1.0 2.0 1 2.0 0 2 7 17	Setting nvidia-container-runtime when starting minikube . Feature Request If we want to use the GPU in minikube with : -- vm-driver = none . ( for networking reason we cannot use a VM ) , we currently need to default the nvidia container runtime on the host . We had some troubles when defaulting the runtime , especially when building the images . So I would like to set it only for minikube and not the whole docker daemon , just like we can when doing : docker run -- runtime = nvidia -- rm nvidia/cuda : 9.0 -base nvidia-smi . . I saw there is an option : -- container-runtime . but it does not support the nvidia runtime and I am not sure it is the good flag . We probably need a : -- docker-runtime . flag ? How complicated would it be to provide this feature ?	2	1
2 2 1.0 1.0 1.2 1.5 1.25 1.5 1.6666666666666667 2.0 1.125 8 1.5 1 2 2 10	Windows Install User Journey . Write user journey for Windows install  	2	2
2 2 1.2 1.0 1.4 1.5 1.4 2.0 1.6666666666666667 2.0 1.1355932203389831 59 1.0 0 6 10 37	Podman v2 breaks the podman driver . When using 2.0.0 or 2.0.1 , the ' podman ' driver doesn't work : : Error : must provide a non-empty container host port to publish . The same run command works OK with podman 1.8.2 , though . There might be more differences between the podman versions ...	0	0
2 2 1.2 1.0 1.3 1.5 1.25 1.0 1.6666666666666667 2.0 1.140625 64 1.0 1 6 12 59	create autmoated script to push preloaded tar ball . we should have a go script that does this : : for a list of kubernetes versions : if we dont have preloaded tar file for it , generate push to gcs bucket . . @priyawadhwa any guidance you could give for anyone who takes this task ? we could pontentially add this to our release script so we publish our preloaded tar files with each release in github release page .	0	1
1 1 1.0 1.0 1.2 1.0 1.1 1.0 1.0 1.0 1.3333333333333333 6 1.5 3 4 11 30	Support buildah . I added a previous ticket about : buildah . ( PR #3225 ) , but it timed out ... When we upgrade to Podman 1.2 , we should include support for buildah ( for building images ) . Probably by including : buildah . next to : podman . , especially if ' podman build ' doesn't work ... It will share the runc and the cni plugins with podman and with cri-o . No real plan yet on how to support building images from host , except for using : minikube ssh . . But it will do as a start . Could even run buildah in a container , but then it's hard to share images .   	1	1
0 0 1.0 1.0 0.8 1.0 1.1 1.0 1.0 1.0 0.45454545454545453 22 0.0 0 2 3 17	Implement a `minikube disk-usage` command so users can see how much space minikube is using . There have been some comments by users that they find minikube uses a lot of disk space and there's no easy way to see how much it's using . We should implement a command so users can easily view the disk space usage . This will include things like preloads and image caching and output the usage categorically .  	1	1
1 1 1.6 2.0 1.5 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 3 6 29	-- help shows incorrect command for ' a list of global command-line options ' . There is no invocation such as : minikube profile options . , but both : kubectl -- help . and : kubectl < command > -- help . , take : label . command for example , give corr	2	0
2 2 0.8 1.0 0.9 1.0 1.2 1.0 0.6666666666666666 0.0 1.0888888888888888 90 1.0 1 7 27 63	auto prune docker images on minikube start . to reduce disk pressure . ( this can be done in parallel one a start ) more info < URL > it would interesting to find out , would it be bad if we always prune images ? would there be cases that users be mad at us for deleteing abandoned images ?  	1	1
2 2 1.6 2.0 1.2 1.0 1.2 1.5 1.6666666666666667 2.0 0.0 0 0.0 0 2 3 18	timed out waiting to elevate kube-system RBAC privileges : creating clusterrolebinding - apiserver timeout . PS C : /MiniKube > . /minikube . exe start o minikube v 0.34.1 on windows ( amd64 ) Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... - ' minikube ' IP address is 192.168.99.101 - Configuring Docker as the container runtime ... - Preparing Kubernetes environment ... - Pulling images required by Kubernetes v 1.13.3 ... - Launching Kubernetes v 1.13.3 using kubeadm ... - Configuring cluster permissions ... ! Error starting cluster : timed out waiting to elevate kube-system RBAC privileges : creating clusterrolebinding : Post < URL > dial tcp 192.168.99 . 101:8443 : connectex : A connection attempt failed because the connected party did not properly respond after a period of time , or established connection failed because connected host has failed to respond .	2	0
1 1 1.2 1.0 1.2 1.5 1.2 1.5 1.0 1.0 1.236842105263158 38 1.0 1 3 7 28	output container run time in profile list . . /out/minikube profile list | ----------|----------- | ------------|----------- | --------------------|--------- | | Profile | VM Driver | NodeIP | Node Port | Kubernetes Version | Status | | ----------|----------- | ------------|----------- | --------------------|--------- | | minikube | docker | 172.17.0.2 | 8443 | v 1.17.2 | Running | | ----------|----------- | ------------|----------- | --------------------|--------- | need to add contaner run time column	0	1
1 1 1.6 2.0 1.3 1.0 1.35 2.0 1.3333333333333333 1.0 0.0 0 0.0 1 4 8 47	add a local multi-arch kic image Make target . Previous single arch kicbase make target is removed at this < URL > . It's is still very useful to have a local target , which is good for debugging . The initiative of this issue comes from this < URL > . The : -- output . flag of : docker buildx . provides multiple ways to export the build results . : local . : all result files to be exported to a pointed directory . ( I tried and it takes a long time to build the result ) : tar . : writes all result files as a single tarball . On multi-platform builds all results will be put in sub-directories by their platform . : oci . : writes the result manifest list as an OCI image layout tarball . ~ : docker . : currently , this is not supported for multi-arch ~ : image . : writes a manifest list ~ : registry . ~ In current implementation , we can pass the local build tag into : base-image . flag , and minikube will find the image < URL > . In proposed solution , minikube will load the tarball into the local daemon , or somehow provision the container from a minifest list .   	1	1
2 2 1.6 2.0 1.2 2.0 1.3 2.0 1.3333333333333333 2.0 0.0 0 0.0 3 5 6 22	hyperv : Unable to start VM : create : creating : exit status 1 . The exact command to reproduce the issue : PS C : /WINDOWS/system32 > minikube start The full output of the command that failed : * minikube v 1.4.0 on Microsoft Windows 10 Pro 10.0.18362 Build 18362 * Creating hyperv VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... * Retriable failure : create : creating : exit status 1 * Deleting ' minikube ' in hyperv ... * Creating hyperv VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... * Retriable failure : create : creating : exit status 1 * Deleting ' minikube ' in hyperv ... * Creating hyperv VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... * Retriable failure : create : creating : exit status 1 * Deleting ' minikube ' in hyperv ... * Creating hyperv VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... * Retriable failure : create : creating : exit status 1 * Deleting ' minikube ' in hyperv ... * X Unable to start VM : create : creating : exit status 1 * * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL > PS C : /WINDOWS/system32 > The output of the : minikube logs . command : PS C : /WINDOWS/system32 > minikube logs * X api load : filestore ' minikube ' : open C : /Users/clombardo . minikube/machines/minikube/config . json : The system cannot find the file specified . * * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL > PS C : /WINDOWS/system32 > The operating system version : Microsoft Windows 10 Pro Version 10.0.18362 Build 18362  	1	0
2 2 1.8 2.0 1.6 2.0 1.5 2.0 1.6666666666666667 2.0 0.0 0 0.0 2 2 4 7	efk : ' Kibana did not load properly ' because of missing SERVER_BASEPATH . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Environment : Linux 4.9.0 -4-amd64 #1 SMP Debian 4.9.65 -3+deb9u1 ( 2017-12-23 ) x86_64 GNU/Linux Minikube version ( use : minikube version . ): v . 0.28.0 - OS ( e.g. from /etc/os-release ): Debian GNU/Linux 9 ( stretch ) - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): none - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): What happened : When opening the kibana dashboard via proxy , it displayed ' Kibana did not load properly . Check the server output for more information . ' in a big red box What you expected to happen : The Kibana dashboard to load How to reproduce it ( as minimally and precisely as possible ): - enable the efk addon - kubectl proxy - Open < URL > in a browser Anything else do we need to know : The SERVER_BASEPATH environment variable was missing . I could fix the issue on my system by adapting the kibana-logging replication controller by adding this : SERVER_BASEPATH : /api/v1/namespaces/kube-system/services/kibana-logging/proxy However I believe , the plugin should work out of the box via proxy	2	0
2 2 1.8 2.0 1.3 2.0 1.15 1.0 2.0 2.0 0.0 0 0.0 0 4 4 7	Document the cni and cri requirements ( for ' none ' ) . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Summary Document/verify the new requirements of kubeadm , that was added in Kubernetes v 1.11.0 : kubernetes-cni (>= 0.6.0 ) cri-tools (>= 1.11.0 ) These tools have already been added to the Minikube ISO ( see 41fdd53e1add6dd0173223da838ed0bdcd799845 and fbd620a01032f747e05df1c63a88ab900e7b7596 ) Related kubernetes/release #573 rpm/deb kubernetes/minikube #3132 k8s v 1.11.0  	2	2
0 0 1.0 1.0 0.8 1.0 1.05 1.0 1.0 1.0 0.0 0 0.0 2 7 12 48	Addon registry-aliases alpine image missing bash command ( ash works ) . Steps to reproduce the issue : minikube start minikube addons enable registry minikube addons enable registry-aliases kubelet says : : Error : failed to start container ' update ' : Error response from daemon : OCI runtime create failed : container_linux . go : 349 : starting container process caused ' exec : /'bash/': executable file not found in $PATH ' : unknown . Easily fixed by changing the daemonset entrypoint ( command[0 ]) from : bash . to : ash .	2	0
0 0 1.0 1.0 0.9 0.5 1.0 1.0 0.3333333333333333 0.0 1.0 131 1.0 1 3 3 29	enable metric addon when someone enables dashboard .  	1	1
2 2 1.6 2.0 1.1 1.0 0.75 0.5 1.3333333333333333 1.0 0.0 0 0.0 1 1 6 29	virtualbox : kube-apiserver : timed out waiting for the condition . I attempted to start minikube on my Mac ( 10.11.6 ) , I got errors as below $ minikube start 棣冩 minikube v 1.0.1 on darwin ( amd64 ) 棣冩崚 Downloading Minikube ISO ... 142.88 MB / 142.88 MB [= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ==] 100.00% 0s 棣冦仚 Downloading Kubernetes v 1.14.1 images in the background ... 棣冩寱 Tip : Use ' minikube start -p ' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Restarting existing virtualbox VM for ' minikube ' ... 閳?Waiting for SSH access ... 棣冩懕 ' minikube ' IP address is 192.168.99.100 棣冩儞 Configuring Docker as the container runtime ... 棣冩儞 Version of container runtime is 1.11.1 閳?Waiting for image downloads to complete ... 閴?Preparing Kubernetes environment ... 閴?Unable to load cached images : loading cached images : loading image /Users/myid/ . minikube/cache/images/ k8s.gcr.io/kube-apiserver_v1.14.1 : transferring cached image : pre-copy : NewSession : ssh : rejected : administratively prohibited ( open failed ) 棣冩崙 Downloading kubeadm v 1.14.1 棣冩崙 Downloading kubelet v 1.14.1 棣冩 Pulling images required by Kubernetes v 1.14.1 ... 棣冩敡 Relaunching Kubernetes v 1.14.1 using kubeadm ... 閳?Waiting for pods : apiserver 棣冩寴 Error restarting cluster : wait : waiting for component = kube-apiserver : timed out waiting for the condition 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > 閴?Problems detected in ' kube-addon-manager ' : error : error when applying patch : error : error when applying patch : error : error when applying patch : I appreciate it if I can get help .	2	0
2 2 1.8 2.0 1.5 2.0 1.35 1.5 2.0 2.0 0.0 0 0.0 1 1 12 35	none : Minikube - v 1.5.0 cant start in ubuntu- 18.04.3 LTS . sudo minikube start -- vm-driver = none : The full output of the command that failed : minikube v 1.5.0 on Ubuntu 18.04 棣冩寱 Tip : Use ' minikube start -p ' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Starting existing none VM for ' minikube ' ... 閳?Waiting for the host to be provisioned ... 棣冩寴 minikube is unable to connect to the VM : dial tcp 192.168.0.4 : 22 : connect : connection refused This is likely due to one of two reasons : VPN or firewall interference none network configuration issue Suggested workarounds : Disable your local VPN or firewall software Configure your local VPN or firewall to allow access to 192.168.0.4 Restart or reinstall none Use an alternative -- vm-driver ubuntu- 18.04.3 LTS :	0	0
1 1 1.6 2.0 1.5 2.0 1.25 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 3 7 41	Allow configuring REGISTRY_STORAGE_DELETE_ENABLED for the registry addon . When enabling the Registry Addon , a docker registry is created in the cluster with defaults . The defaults allow a user to push images to the private registry , not removing them . The registry container needs to be spun up with the following environment variable : : REGISTRY_STORAGE_DELETE_ENABLED = true . Proposal Allow configuring the option similarly to how the registry-creds can be configured .	2	1
0 0 0.8 0.0 1.2 2.0 1.0 1.0 0.6666666666666666 0.0 1.0 3 1.0 1 3 5 25	Autocompletion support for minikube on Windows/PowerShell . PowerShell has auto-completion features for the native cmdlets . Lets see if we can have some support for auto-completion on Windows/PowerShell for minikube .	2	1
1 1 1.0 1.0 1.3 1.5 1.25 1.0 0.3333333333333333 0.0 1.0 7 1.0 2 7 11 28	Add info about maintainers to addons page of website .  	2	2
2 2 1.2 1.0 1.5 2.0 1.3 1.5 1.0 1.0 0.0 1 0.0 7 7 13 36	Add audit . json logs to minikube logs command . audit . json logs should be output in their own section . The logs should be formatted to be human readable , their native json format is hard to read .	0	1
2 2 1.4 2.0 1.0 1.0 1.15 1.0 1.6666666666666667 2.0 1.5333333333333334 15 2.0 1 4 11 35	Show download progress for the Preload Tarball . As you can see in the above screenshot , we don't show the progress of the download . It is missing the bandwidth , size of remaining content , ETA etc . like we show for the ISO . Since this preload tarball is big , the above information is essential .	2	1
1 1 1.2 1.0 1.2 1.0 0.95 1.0 0.6666666666666666 1.0 1.0963855421686748 83 1.0 4 13 24 56	  docs : need to addons on addons . our handbook and website doesnt not have much info about all the addons we have we need a section to list all the addons ( use the @priyawadhwa auto generate doc ) to list the addons and maybe a small descrption or example usage of each addon in their page . cc : are you interested ? @prasadkatti some of our addons alrready have a Readme we can use as a basic start < URL > but I believe they are not represtened in the site .   	1	2
1 1 1.8 2.0 1.6 2.0 1.55 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 0 2	Host DNS search domain is not propagated to minikube DNS . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Minikube version ( use : minikube version . ): v 0.20.0 Environment : - OS ( e.g. from /etc/os-release ): Windows 7 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): virtualbox - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): file :/ /C : /cygwin64/home/b26978/ . minikube/cache/iso/minikube-v 0.20.0 . iso - Install tools : - Others : What happened : When I create a new minikube using : minikube start . the DNS configuration inside the minikube VM doesn't use the same DNS search domain as the host uses . I'm behind a corporate firewall and the proxy hostname isn't resolvable from inside the VM , thus the http_proxy and https_proxy settings are useless . Apparently the DNS system is managed by systemd-resolved , and the configuration file at used is located at : /etc/systemd/resolved . conf . The Domains settings inside is commented in my configuration . I managed to get the DNS ( and proxy ) working by setting the Domains value in the /etc/systemd/resolved . conf and restarting the service : : systemctl daemon-reload systemctl restart systemd-resolved . Sadly , I'm not fluent in Go and I'm not able to submit a patch to fix the issue . What you expected to happen : The configuration file ( /etc/systemd/resolved . conf ) contains the Domains settings and it's set to the correct value . How to reproduce it ( as minimally and precisely as possible ): Set the DNS domain search to something ( non empty ) under Windows 7 . Create a new minikube vm using : minikube start . . Anything else do we need to know :	2	1
2 2 1.4 2.0 1.0 1.0 1.1 1.0 1.0 1.0 0.0 0 0.0 1 3 8 31	  Overview : links are broken on section ' Where should I go next ? ' . The links for Getting Started and Examples in the section < URL > at the bottom of the page are broken ( 404 status code ) : Where should I go next ? Getting Started : Get started with minikube Examples : Check out some minikube examples ! . They should link to the following links respectively : < URL > < URL >	0	2
2 2 1.2 1.0 0.9 1.0 1.2 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 5 6 23	containerd ignores -- insecure-registry . Steps to reproduce the issue : minikube start -- insecure-registry 192.168.1.31 : 5000 -- container-runtime containerd minikube ssh critctl pull 192.168.1.31 : 5000/ Full output of failed command : Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : : minikube ssh $ sudo -s $ crictl pull 192.168.1.31 : 5000/cn2/busybox FATA[0000 ] pulling image : rpc error : code = Unknown desc = failed to pull and unpack image ' 192.168.1.31 : 5000/cn2/busybox : latest ' : failed to resolve reference ' 192.168.1.31 : 5000/cn2/busybox : latest ' : failed to do request : Head < URL > http : server gave HTTP response to HTTPS client . Problem can be solved by adding : imports = ['/etc/containerd/config . minikube . toml ' ] . to /etc/containerd/config . toml and create /etc/containerd/config . minikube . toml with the following content : [ plugins ] [ plugins . cri ] [ plugins . cri . registry ] [ plugins . cri . registry . mirrors ] [ plugins . cri . registry . mirrors . ' 192.168.1.31 : 5000 ' ] endpoint = ['<URL>'] .	0	0
0 0 0.6 1.0 0.5 0.0 0.95 1.0 0.6666666666666666 1.0 1.2857142857142858 35 1.0 10 11 15 29	validate profile name ( shouldn't start with numbers ) . while trying to name my profile 2 by mistake , I noticed kic cant create a docker container starting with a numeric name . possilbe solutions 1- append minikube name to all VM and Containers we create or alternatievely validate profile name not to start with numbers .  	1	1
2 2 1.2 2.0 1.4 2.0 1.25 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 2 4 32	Unable to stop VM : open config . json : The system cannot find the file specified . . minikube stop X Unable to stop VM : open C : /Users/Uday . minikube/machines/minikube/config . json : The system cannot find the file specified . * * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL > Windows 10	0	0
1 1 1.6 2.0 1.1 1.0 1.2 1.0 1.3333333333333333 1.0 1.5 2 1.5 1 4 5 36	service list : add FQDN to service name . It would be great if the minikube service cmd could do a reverse dns lookup . Currently we have ... ```$ minikube service -n cdi cdi-uploadproxy-exposed -- url -- https < URL > : Then I have to do some bash magic to get a fqdn and perform other tasks using a fqdn so that I don't have to add aargs like ` -- insecure` / `-k` . It would be great if we had something like ... ```$ minikube service -n cdi cdi-uploadproxy-exposed -- url -- https -- fqdn < URL > . in which minikube does something like ... : dig -x `minikube ip` +short k8s.example.com .	2	1
1 1 0.8 1.0 0.8 1.0 0.75 1.0 1.0 1.0 0.0 0 0.0 1 2 9 57	run `minikube start` with -- image-mirror-country = cn failed . When i use the minikube 1.8.2 to start , it failed , But the 1.8.1 can work . The exact command to reproduce the issue : : minikube start -- cpus = 4 -- memory = 4g -- download-only = true -- driver = kvm2 -- image-mirror-country = cn -- alsologtostderr -v = 7 . The full output of the command that failed : : panic : semver : Parse(v 1.17.3 ): Invalid character(s ) found in major number ' v1 ' goroutine 1 [ running ]: github.com/blang/semver.MustParse(0xc0007ee036 , 0x7 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , ... ) /go/pkg/mod/ github.com/blang/semver@v3.5.0+incompatible/semver.go:319 +0x1c1 k8s.io/minikube/cmd/minikube/cmd.generateCfgFromFlags(0x2a45e60 , 0xc0007ee036 , 0x7 , 0x1a6b53a , 0x4 , 0x0 , 0x0 , 0x0 , 0x0 , 0x0 , ... ) /app/cmd/minikube/cmd/start . go : 825 +0x335 k8s.io/minikube/cmd/minikube/cmd.runStart(0x2a45e60 , 0xc000148a80 , 0x0 , 0x8 ) /app/cmd/minikube/cmd/start . go : 315 +0x3be github.com/spf13/cobra.(* Command ) . execute(0x2a45e60 , 0xc000148900 , 0x8 , 0x8 , 0x2a45e60 , 0xc000148900 ) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:830 +0x2aa github.com/spf13/cobra.(* Command ) . ExecuteC(0x2a44f60 , 0x0 , 0x1 , 0xc000637bc0 ) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:914 +0x2fb github.com/spf13/cobra.(* Command ) . Execute (...) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:864 k8s.io/minikube/cmd/minikube/cmd.Execute () /app/cmd/minikube/cmd/root . go : 108 +0x6a4 main . main () /app/cmd/minikube/main . go : 66 +0xea . The output of the : minikube logs . command : The operating system version : Ubuntu 18.04.4 LTS	0	0
0 0 1.4 2.0 1.4 2.0 1.4 2.0 1.3333333333333333 2.0 0.8148148148148148 243 1.0 0 0 1 14	log the virtualbox version in our minikube start logs . this info can come handy and we can also warn ppl with very old vbox to update them	0	1
0 0 1.4 2.0 1.1 1.5 1.05 1.0 1.3333333333333333 2.0 0.9504950495049505 101 1.0 2 9 24 78	v 1.10.0 -beta . 0 release broken : references nonexistent ISO . The latest beta does not work because it points to an unreleased ISO image . This seems like something we should have caught in the integration tests for the : Makefile . update : : E0421 10:14:44 . 486960 75329 iso . go : 87 ] Unable to download < URL > < URL > invalid checksum : Error downloading checksum file : bad response code : 404 . The release process need to be updated to ensure that this does not happen again .	0	0
1 1 1.0 1.0 1.3 2.0 1.3 2.0 1.6666666666666667 2.0 0.98 50 1.0 1 1 5 31	site issues on mobile : screenshot aspect ratio & font sizes . Mentioned by @medyagh    	1	2
2 2 1.2 2.0 1.3 1.5 1.45 2.0 1.3333333333333333 2.0 1.5925925925925926 27 2.0 1 2 3 18	istio operator addon is enabled by default ! . I noticed since we merged the istio addon , users get an istio operator by default : NAMESPACE NAME READY STATUS RESTARTS AGE isti o-o perator isti o-o perator-5488b4489d-4hf82 0/1 ContainerCreating 0 19s kube-system coredns-6955765f44-fdvqn 0/1 ContainerCreating 0 21s kube-system coredns-6955765f44-tvbtq 0/1 ContainerCreating 0 21s kube-system etcd-minikube 1/1 Running 0 6s kube-system kube-addon-manager-minikube 1/1 Running 0 6s kube-system kube-apiserver-minikube 1/1 Running 0 6s kube-system kube-controller-manager-minikube 1/1 Running 0 6s kube-system kube-proxy-w45wj 1/1 Running 0 21s kube-system kube-scheduler-minikube 1/1 Running 0 6s kube-system storage-provisioner 0/1 ContainerCreating 0 19s . : $ . /out/minikube addons list | grep enable - addon-manager : enabled - dashboard : enabled - default-storageclass : enabled - istio-provisioner : enabled - storage-provisioner : enabled . CC : @fenglixa could this behavior be avoided ?	0	0
2 2 1.2 2.0 1.1 1.0 1.2 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 1 1 8	Unstable DNS with None driver : Can't find mysql-server . default . svc . cluster . local : No answer . BUG REPORT Environment : Ubuntu 16.04 in virtual box Minikube version ( use : minikube version . ): v 0.28.2 - OS ( e.g. from /etc/os-release ): Ubuntu 16.04.1 LTS - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): none - Install tools : manual install using wget according to the welcome page What happened : The lookup works only rarely and if it does it's slow What you expected to happen : I expect the lookup is always working and fast ( its just on the same machine ) How to reproduce it ( as minimally and precisely as possible ): I install a fresh minikube . Install helm in minikube . Minikube is start with -- vm-driver = none on ubuntu 16.04 . I install a mysql server using : helm install -- name mysql-server stable/mysql . then I make a test : : kubectl run -ti -i fff2 -- image = busybox . : $nslookup mysql-server . default . svc . cluster . local . The nslookup I make several times , most of the times I get : nslookup mysql-server . default . svc . cluster . local Server : 10.96.0.10 Address : 10.96.0.10 : 53 *** Can't find mysql-server . default . svc . cluster . local : No answer Sometimes i actually get an answer but it's only rarely . minikube started with : sudo minikube start -- vm-driver = none .	2	0
2 2 1.4 1.0 1.4 1.5 1.3 1.0 1.3333333333333333 1.0 1.6 5 2.0 1 1 3 25	Running systemctl over the new ssh driver fails . Systemd returns an error code (!) for all kinds of status-like commands , and the new SSH driver doesn't like when the command it executes fails . This means that we cannot run things like ' is-active ' anymore : : $ minikube docker-env 棣冩寴 Error getting service status : ssh command error : command : sudo systemctl is-active docker err : Process exited with status 3 output : inactive 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > . Or at least not when it returns : inactive . , still OK with : active . . : is-active PATTERN ... Check whether any of the specified units are active ( i.e. running ) . Returns an exit code 0 if at least one is active , or non-zero otherwise . Unless -- quiet is specified , this will also print the current unit state to standard output . .	0	0
0 0 1.4 2.0 1.0 1.0 0.85 1.0 1.3333333333333333 2.0 0.0 0 0.0 2 2 7 42	v 1.13 w/ CRIO on GitHub actions : failed to initialize top level QOS containers : root container [ kubepods ] doesn't exist . We're having odd issues at our CI at < URL > with minikube 1.13.1 and CRI-O . After we switched from : minikube . : 1.11.0 . to : 1.13.1 . we had failures at CRI-O setup at all K8s versions we test against . But only with CRI-O , the rest work . Locally everything works as expected . I'd be happy to test run new version at our CI if needed . Steps to reproduce the issue : Run this script at Github Actions : < URL > I can't repro it locally . More info is available at our CI at vector : < URL > Full output of failed command : Sorry too much data . Please check our CI for logs ( multiple runs ): - < URL > - < URL >	2	0
0 0 0.6 0.0 0.7 0.5 0.8 1.0 0.3333333333333333 0.0 1.0 132 1.0 0 2 5 21	don't allow -- base-image flag to be set for VM drivers . currently if I specify -- base-image for an existing hyperkit VM , minikube doesnt even notice it . we should exit quick , and tell users , did you mean to use docker or podman driver insead ? and explain them base image is for docker/podman driver and for VM it is called ISO_URL : medya@ ~ /workspace/minikube ( crio_17_3 ) $ . /out/minikube start -- base-image = local/kicbase : v 0.0.10 -snapshot 棣冩 minikube v 1.12.1 on Darwin 10.15.6 閴?Using the hyperkit driver based on existing profile 棣冩啢 Starting control plane node minikube in cluster minikube 棣冨籍 Updating the running hyperkit ' minikube ' VM ... 棣冩儞 Preparing Kubernetes v 1.18.3 on Docker 19.03.12 ... 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : default-storageclass , storage-provisioner .  	1	0
2 2 1.6 2.0 1.5 2.0 1.45 2.0 2.0 2.0 0.0 1 0.0 1 4 7 22	minikube_ 1.17.0 -0_amd64 . deb : package architecture ( arm64 ) does not match system ( amd64 ) . Steps to reproduce the issue : This is #9995 again , because no fix was merged for it 10004 was closed without any changes being merged it looks like , and #9998 is approved but not merged . The broken binary from #9995 got addressed by @tstromberg doing < URL > , seems 1.17.0 needs this again , unless #9998 can be merged and a 1.17.1 release sent out to replace it ?	0	0
0 0 0.6 0.0 0.5 0.0 0.55 0.0 0.0 0.0 0.9915966386554622 119 1.0 1 2 4 21	status with cluster layout : kubeconfig StatusName is empty . : % minikube status -p v119 -o json -- layout = cluster | jq { ' Name ' : ' v119 ' , ' StatusCode ' : 418 , ' StatusName ' : ' Paused ' , ' Step ' : ' Done ' , ' StepDetail ' : ' 閳搭垽绗?Paused 14 containers in : kube-system , kubernetes-dashboard , storage-gluster , isti o-o perator ' , ' BinaryVersion ' : ' v 1.12.2 ' , ' Components ' : { ' kubeconfig ' : { ' Name ' : ' kubeconfig ' , ' StatusCode ' : 200 , ' StatusName ' : ' } } , ' Nodes ' : [ { ' Name ' : ' v119 ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' , ' Components ' : { ' apiserver ' : { ' Name ' : ' apiserver ' , ' StatusCode ' : 418 , ' StatusName ' : ' Paused ' } , ' kubelet ' : { ' Name ' : ' kubelet ' , ' StatusCode ' : 405 , ' StatusName ' : ' Stopped ' } } } ] } . It looks like we are not filling in the StatusName field for cluster components .  	1	0
0 0 1.0 1.0 1.2 1.0 1.25 1.0 1.0 1.0 1.6 5 2.0 0 2 10 33	kic : stop fails after running ' eval $(minikube docker-env )' . This is not high priority - but probably with the docker driver we should hold on to the original docker daemon settings instead of taking them from the env vars on the terminal . The exact command to reproduce the issue : : minikube start -- vm-driver = docker -- container-runtime = docker eval $(minikube docker-env ) minikube stop -v 10 -- alsologtostderr W0201 21:42:05 . 950478 6913 root . go : 244 ] Error reading config file at /Users/balintp/ . minikube/config/config . json : open /Users/balintp/ . minikube/config/config . json : no such file or directory 閴?Stopping ' minikube ' in docker ... I0201 21:42:05 . 951438 6913 main . go : 110 ] libmachine : Stopping ' minikube ' ... I0201 21:42:06 . 019634 6913 main . go : 110 ] libmachine : Error getting machine state : error stop node minikube : exit status 1 閴?Stopping ' minikube ' in docker ... I0201 21:42:11 . 610498 6913 main . go : 110 ] libmachine : Stopping ' minikube ' ... I0201 21:42:11 . 672461 6913 main . go : 110 ] libmachine : Error getting machine state : error stop node minikube : exit status 1 閴?Stopping ' minikube ' in docker ... I0201 21:42:22 . 545429 6913 main . go : 110 ] libmachine : Stopping ' minikube ' ... I0201 21:42:22 . 619237 6913 main . go : 110 ] libmachine : Error getting machine state : error stop node minikube : exit status 1 閴?Stopping ' minikube ' in docker ... ... 棣冩寴 Unable to stop VM : Temporary Error : Stop : minikube : stopping minikube : exit status 1 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > .	0	0
0 0 1.4 2.0 1.5 2.0 1.3 2.0 1.0 1.0 0.0 0 0.0 2 3 4 18	minikube service` not working with Docker driver on Windows 10 Pro 10.0.19041 -Build 19041 . I started minikube service on windows 10 pro build 19041 . Please find the attached screenshots The resource are not accessible at 172.17.0.3 : 30001 . i ; e NodeAddress : Nodeport mentioned in service . yaml file Neither the resource is available at < URL > which I have found after running . /minikube-windows-amd64 . exe service selenium-srv * Starting tunnel for service selenium-srv . | -----------|-------------- | -------------|------------------------ | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|-------------- | -------------|------------------------ | | default | selenium-srv | | < URL > | | -----------|-------------- | -------------|------------------------ |	2	0
1 1 1.0 1.0 1.3 1.5 1.4 2.0 1.0 1.0 0.0 0 0.0 3 4 12 56	Error getting primary cp : could not find master node . I am on this Mac ... : $ sw_vers ProductName : Mac OS X ProductVersion : 10.15.2 BuildVersion : 19C57 . I have installed minikube thus ... : $ brew install minikube . Here's the version of minikube installed ... : $ minikube version minikube version : v 1.8.1 commit : cbda04cf6bbe65e987ae52bb393c10099ab62014 . I try to start minikube and face issue ... : $ minikube start 棣冩 minikube v 1.8.1 on Darwin 10.15.2 閴?Automatically selected the hyperkit driver . Other choices : virtualbox , docker 棣冩寴 Error getting primary cp : could not find master node 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > .   	1	0
0 0 1.2 2.0 1.3 1.5 1.05 1.0 1.3333333333333333 2.0 0.832579185520362 221 1.0 0 0 6 32	Investigate CNCF processes to collect minikube usage metrics . To evaluate what features of minikube to invest we would like to collect anonymous usage metrics to find out which Driver/Platform or Container Runtime or Addon gets used most . we would need to find the processes in CNCF for this case .	0	1
1 1 1.2 1.0 1.4 2.0 1.5 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 4 21	Parallels driver is getting called as a built-in library instead of RPC Server . The issue is well-described in this thread ( starting from the linked comment ): < URL > We realized that the output of parallels driver does not depend on the actual binary of : docker-machine-driver-parallels . available in PATH . It appeared that : minikube . was calling driver's functions as a usual library , without launching the binary as the RPC Server . I found the root cause of this and will submit a fixing PR very soon	2	0
2 2 1.4 2.0 1.5 2.0 1.1 1.0 1.0 1.0 0.75 16 0.5 1 6 12 25	docker-env returns linux export commands if ssh'd from linux to windows . directly on powershell , docker-env works as expected : : PS C : /jenkins/ > . /minikube-windows-amd64 . exe docker-env $Env :D OCKER_TLS_VERIFY = ' 1 ' $Env :D OCKER_HOST = ' tcp :/ / 127.0.0.1 : 55002 ' $Env :D OCKER_CERT_PATH = ' C : /Users/jenkins/ . minikube/certs ' $Env : MINIKUBE_ACTIVE_DOCKERD = ' minikube ' # To point your shell to minikube's docker-daemon , run : # & minikube -p minikube docker-env | Invoke-Expression . but if you ssh into a windows machine from a linux machine , minikube will return linux export commands instead of the appropriate powershell commands : : jenkins @WINDOWS -SERVER- > powershell . exe -NoProfile -NonInteractive . /minikube- windows-amd64 . exe docker-env export DOCKER_TLS_VERIFY='1 ' export DOCKER_HOST='tcp://127 . 0.0.1 : 55002 ' export DOCKER_CERT_PATH='C:/Users/jenkins/ . minikube/certs ' export MINIKUBE_ACTIVE_DOCKERD='minikube ' # To point your shell to minikube's docker-daemon , run : # eval $(minikube -p minikube docker-env ) .   	1	0
2 2 1.2 2.0 0.8 0.5 1.1 1.0 2.0 2.0 0.9130434782608695 23 1.0 0 1 4 37	Download our own hyperkit driver if the installed one is missing or incompatible . There are 3 implementations of docker-machine-driver-hyperkit , that all install to the same path : The version that ships with Docker for Desktop The machine-drivers fork ( last updated in March 2018 , which brew installs ) The minikube fork ( last updated 19 days ago ) Any 3 of these versions may be an ancient and/or incompatible release with breaking bugs . Most hyperkit bugs are due to users not using the minikube fork . My rough idea is : Upstream our hyperkit driver Add versioning to the driver ( binary -- version ) If version is compatible , use it ! If the version is incompatible , fetch a new version into ~ / . minikube and ask for sudo access to chmod it , and use it instead .	0	1
1 1 1.4 1.0 1.4 1.5 1.25 1.5 1.3333333333333333 1.0 0.5555555555555556 9 0.0 1 12 13 48	  Enhancements to automated commands doc creation . Ref #7385 @tstromberg ' s review : : Looking specifically at the `addons` command , we're going to need some help making this more perfect in subsequent PR's: * Options inherited shouldn't show up per sub-command * Options shouldn't be shown if the only one is ` -- help` * Synopsis shouldn't be shown if it identical to the command description ( duplicate ) .   	1	2
1 1 0.8 1.0 1.1 1.0 0.85 1.0 1.0 1.0 0.6190476190476191 21 0.0 2 5 7 23	Ensure GCP Auth addon can use creds from GCE metadata server if present . This seems to be broken currently in Cloud Shell . We need to test it out , and see what's happening , and fix it once we understand the issue .	0	0
1 1 1.4 2.0 1.3 1.5 1.3 1.5 1.0 1.0 1.6666666666666667 3 2.0 1 4 8 57	-- host-only-cidr : invalid CIDR address : ' 192.168.99.1 /24 ' . With my ancient minikube version ( 1.4.0 ) , I had been using the option : -- host-only-cidr='192 . 168.99.1 /24 ' . which now gives me : X Unable to start VM . Please investigate and run '	2	0
0 0 0.8 1.0 0.9 1.0 0.95 1.0 1.0 1.0 1.0277777777777777 36 1.0 2 4 11 30	Replace third_party/go9p with a more robust userspace fs implementation . We currently maintain our own 9p fork in : third_party/go9p . , based on deprecated code . There are many more to consider switching to : < URL > - though we don't have to specifically consider 9p , or Go .  	1	1
1 1 1.2 1.0 1.2 1.5 1.2 1.5 1.0 1.0 0.9629629629629629 162 1.0 2 2 8 33	profile list for multi node shows error wrongfully . as noted by @sharifelgamal the missing part of this PR is left < URL > : | ----------|----------- | ---------|-------------- | ------|--------- | ---------|------- | | Profile | VM Driver | Runtime | IP | Port | Version | Status | Nodes | | ----------|----------- | ---------|-------------- | ------|--------- | ---------|------- | | minikube | docker | docker | 192.168.49.2 | 8443 | v 1.19.4 | Running | 1 | | p2 | docker | docker | 192.168.59.2 | 8443 | v 1.19.4 | Running | 2 | | ----------|----------- | ---------|-------------- | ------|--------- | ---------|------- | 閴?Found 1 invalid profile(s ) ! p2-m02 棣冩寱 You can delete them using the following command(s ): $ minikube delete -p p2-m02 .	0	0
2 2 1.4 2.0 1.2 1.0 1.35 1.5 1.6666666666666667 2.0 0.0 0 0.0 0 2 8 41	ISO : Enable CONFIG_XFS_QUOTA . Please enable CONFIG_XFS_QUOTA in linux kernel options for the VirtualBox ISO image . This leeds to problems with the kadalu storage provider . See : < URL > Steps to reproduce the issue : Boot minikube try to mount a xfs filesystem with quota support . Full output of failed command : mount : /bricks/storage-pool-1/data : wrong fs type , bad option , bad superblock on /dev/loop0 , missing codepage or helper program , or other error . Full output of : minikube start . command used , if not already included : 棣冩 minikube v 1.15.1 auf Darwin 11.0.1 閴?Using the virtualbox driver based on existing profile 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩敡 Restarting existing virtualbox VM for ' minikube ' ... 棣冩儞 Vorbereiten von Kubernetes v 1.19.4 auf Docker 19.03.13 ... 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : storage-provisioner , default-storageclass 棣冨及 Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default	2	1
1 1 0.8 1.0 0.7 1.0 1.1 1.0 1.0 1.0 0.0 0 0.0 4 5 7 55	Add a command to list supported drivers . Need a way to call minikube to get a list of supported drivers on deployed system . This information comes back from help but is extremely difficult to reliably parse via regex and looks to contain temporal content ( like experimental ): : minikube start -- help | grep -e ' -- driver='':' . : -- driver='': Driver is one of : virtualbox , parallels , vmwarefusion , hyperkit , vmware , docker , podman ( experimental ) ( defaults to auto-detect ) . This will aid us in fixing issue < URL >	0	1
2 2 0.8 0.0 0.8 0.0 1.1 1.5 0.6666666666666666 0.0 1.1153846153846154 104 1.0 2 9 15 53	Add some more ' fine print ' to the installation requirements . Somebody recently tried to install minikube on x86 Alpine Linux : ~ ~ #10805 ~ ~ This failed , for two different reasons : the regular minikube binary requires GNU C Library we don't have any 32-bit binaries for x86 available So we probably need to make it clear that those are required . I don't think we want to do static builds , and k8s doesn't do i686  	2	2
2 2 2.0 2.0 1.3 2.0 1.35 2.0 2.0 2.0 1.16 100 1.0 1 3 9 49	Upgrade CRI-O to 1.20.0 . Upgrade cri-o package , from 1.19.0 to 1.20.0 Tracker for #10476 ( ISO ) and #10477 ( KIC )	0	1
1 1 0.8 1.0 1.4 2.0 1.35 2.0 1.0 1.0 0.0 0 0.0 1 2 10 28	Metallb addon is not setting the IP range . Hi , I'm trying to use the metallb addon and I'm following < URL > about how to install it and configure it . After I add the first and end IPs ( 192.168.49.10 , 192.168.49.30 ) and I query the configmap , the addresses list appear empty . And I have to edit the configmap and add them manually , can you please help me so I can set the values from the : minikube addons configure metallb . command ? I actually have a script that bootstraps minikube with all of its addons and configs  	1	0
2 2 1.0 1.0 0.7 0.0 0.8 1.0 1.6666666666666667 2.0 1.123076923076923 65 1.0 1 7 13 60	handle podman-env in PointToHostDockerDaemon . : if err : = oci . PointToHostDockerDaemon (); err ! = nil { return state . Error , errors . Wrap(err , ' point host docker daemon ' ) } .  	1	1
2 2 1.2 1.0 1.1 1.0 1.2 1.0 1.0 1.0 0.8413461538461539 208 1.0 2 5 9 48	Multiarch support for registry addon . if yes add integration test for multi arch	0	1
1 1 1.4 2.0 1.5 2.0 1.4 1.5 1.6666666666666667 2.0 0.989010989010989 91 1.0 0 1 3 40	Solution message for `Failed to start the virtual machine ' minikube ' because one of the Hyper-V components is not running . ` . For GCE users , we should tell them nested Virtualization it isn't possible . For other users , perhaps point them to : < URL > NOTE : This may have some related issues to dupe against .    	1	2
2 2 1.4 2.0 1.3 1.0 1.3 1.5 1.6666666666666667 2.0 0.0 0 0.0 2 7 8 25	none without Docker installed : sudo systemctl start docker : exit status 5 . Unable to start minikube in RHEL 8 based , due to the missing container runtime as a systemd service . **[ l @minikube ~ ]$ sudo minikube start -- vm-driver = none 棣冩 minikube v 1.4.0 on Centos 8.0.1905 棣冩寱 Tip : Use ' minikube start -p ' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Starting existing none VM for ' minikube ' ... 閳?Waiting for the host to be provisioned ... 棣冩寴 Failed to enable container runtime : running command : sudo systemctl start docker : exit status 5 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > [ l @minikube ~ ]$ sudo minikube start -- vm-driver = none -- container-runtime = cri-o 棣冩 minikube v 1.4.0 on Centos 8.0.1905 棣冩寱 Tip : Use ' minikube start -p ' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Starting existing none VM for ' minikube ' ... 閳?Waiting for the host to be provisioned ... 棣冩寴 Failed to enable container runtime : running command : sudo systemctl restart crio : exit status 5 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > CentOS Linux release 8.0.1905 ( Core ) :	2	0
1 1 0.8 1.0 1.1 1.0 1.1 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 1 5 26	How do you enable SSL Passthrough in the ingress deployment . Hi , I am trying to enable ingress on minikube and then allow -- enable-ssl-passthrough I have tried editing the deployment with kubectl I have tied patching the deployment but everything I try results in no changes to the underlying resource . My question is , is ssl passthrough even possible on the version of ingress that comes as an addon to minikube and if so how do you do it . I would really appreciate the help . Thanks , Paul  	2	2
2 2 1.2 2.0 1.1 1.0 1.05 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 5 5 28	[ NON_C_DRIVE ] when run from C :/ but user config is on different drive . Unable to get Minikube start even when I do it from C : Drive I run the below command and receive the below error C : /Kubernetes > minikube start * minikube v 1.2.0 on windows ( amd64 ) * Downloading Minikube ISO ... 129.33 MB / 129.33 MB [= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ==] 100.00% 0s * Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... E0718 09:14:58 . 371558 12332 start . go : 559 ] StartHost : create : creating : open / . minikube/cache/iso/minikube-v 1.2.0 . iso : The system cannot find the path specified . X Unable to start VM * Error : [ NON_C_DRIVE ] create : creating : open / . minikube/cache/iso/minikube-v 1.2.0 . iso : The system cannot find the path specified . * Advice : Run minikube from the C : drive . * Related issues : - < URL > If the above advice does not help , please let us know : < URL > After some digging in I found the . minikube folder is getting created on the : H network drive , this is my company provided PC , i'm not sure if its has anything to do with permission . when ever I open the windows CMD its start with H : and I navigate to C : and run the command , yet for some reason the . minikube gets created in : H . I'm new to this , just started to learn K8s , any help on this is deeply appreciated . Version : - Windows 10 Regards , Shreyas	0	0
0 0 0.8 1.0 1.0 1.0 1.05 1.0 0.3333333333333333 0.0 0.9770114942528736 87 1.0 0 5 9 47	-- wait = true exits prematurely , breaks conformance test script . NOTE : This is not a discussion about defaults , this is about the behavior when explicitly setting : -- wait = true . . The : . /hack/conformance_tests . sh . script is broken because Ku	1	0
2 2 1.6 2.0 1.4 2.0 1.35 2.0 1.3333333333333333 1.0 0.0 0 0.0 1 5 13 55	hyperv : The memory value assigned (' 4019 ' MB ) is not properly aligned . I am new to k8s and trying to start minikube but kept seeing the error below . The exact command to reproduce the issue : minikube start -- driver = hyperv The full output of the command that failed : Hyper-V/New-VM : Failed to modify device ' Memory ' . Invalid memory value assigned for ' minikube ' . Memory values must be properly aligned . ' minikube ' failed to modify device ' Memory ' . ( Virtual machine ID 2BFC2B6D-BB10-4492-9C37-E36F134A22F6 ) Invalid memory value assigned for ' minikube ' . The memory value assigned (' 4019 ' MB ) is not properly aligned . Try again with a memory value that is properly aligned . ( Virtual machine ID 2BFC2B6D-BB10-4492-9C37-E36F134A22F6 ) At line : 1 char : 1 The output of the : minikube logs . command : E0310 10:20:15 . 208507 6448 main . go : 106 ] libmachine : [ stderr =====> ] : Hyper-V/Get-VM : Hyper-V was unable to find a virtual machine with name ' minikube ' . At line : 1 char :3 + ( Hyper-V/Get-VM minikube ) . state + ~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidArgument : ( minikube : String ) [ Get-VM ] , VirtualizationException + FullyQualifiedErrorId : InvalidParameter , Microsoft . HyperV . PowerShell . Commands . GetVM * X command runner : getting ssh client for bootstrapper : Error creating new ssh host from driver : Error getting ssh host name for driver : Host is not running * The operating system version : Windows 10 pro	0	0
0 0 0.2 0.0 0.4 0.0 0.85 0.5 0.3333333333333333 0.0 0.0 0 0.0 2 5 5 37	hyperv : Unable to get host IP due to case mismatch in virtual switch . minikube start -- vm-driver = hyperv -- hyper v-v irtual-switch = Minikube Running the above command fails with ' Unable to get host IP : ip for interface ( Minikube ): Could not find interface vEthernet ( Minikube ) ' Investigating my virtual network adapter using the following PowerShell command shows that I had a case mismatch ( large K in Kube ) compared to the output message : Get-NetAdapter | select Name Name ---- vEthernet ( MiniKube ) . Renaming the adapter using the command below to have a small K resolved my issue and allowed Minikube to successfully start : Rename-NetAdapter -Name ' vEthernet ( MiniKube )' -NewName ' vEthernet ( Minikube )' Get-NetAdapter | select Name Name ---- vEthernet ( Minikube ) . During testing I also identified that regardless of what you supply to the hyper v-v irtual-switch parameter it still looks for vEthernet ( Minikube ) e.g. minikube start -- vm-driver = hyperv -- hyper v-v irtual-switch = AnyOldThing Unable to get host IP : ip for interface ( Minikube ): Could not find interface vEthernet ( Minikube )	2	0
2 2 1.6 2.0 1.1 1.0 0.85 1.0 1.6666666666666667 2.0 0.9642857142857143 56 1.0 1 4 8 19	ln : failed to create symbolic link ' /var/lib/minikube/etcd/minikube ' : File exists . Subsequent error on minikube restart : : minikube v 1.4.0 on Darwin 10.14.3 棣冩寱 Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . 棣冨籍 Using the running virtualbox ' minikube ' VM ... 閳?Waiting for the host to be provisioned ... 棣冩儞 Preparing Kubernetes v 1.16.0 on Docker 18.09.8 ... 棣冩敡 Relaunching Kubernetes using kubeadm ... E0920 14:15:18 . 905571 77264 kubeadm . go : 415 ] failed to create compat symlinks : cmd failed : sudo ln -s /data/minikube /var/lib/minikube/etcd ln : failed to create symbolic link ' /var/lib/minikube/etcd/minikube ' : File exists : Process exited with status 1 . Originally posted by @vsethi in < URL >	2	0
0 0 1.0 1.0 1.0 1.0 1.1 1.0 0.3333333333333333 0.0 1.162962962962963 135 1.0 0 1 2 17	Running minikube under multipass . Our friends at Canonical have made a ' workflow ' for running minikube with < URL > ... It will create an Ubuntu VM for you , install Docker Engine and then run : minikube . on it : : $ multipass launch minikube Launched : minikube $ multipass exec minikube -- minikube status minikube type : Control Plane host : Running kubelet : Running apiserver : Running kubeconfig : Configured . See < URL > ( it is using the deb installation from < URL >  	2	2
0 0 1.2 1.0 1.1 1.0 1.1 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 0 1 5	The exit code for a minikube ssh command is always 0 . The minikube ssh command always returns with an exit code 0 , even if it fails . : minikube ssh ' nonexistingcommand ' && minikube ssh ' date ' . The second command should not be executed , because the first one failed .	0	0
2 2 1.4 2.0 1.2 1.5 1.35 2.0 1.3333333333333333 2.0 0.9606299212598425 127 1.0 0 0 5 36	Do not auto-select Hyper-V driver if session has no privilege . We need to copy the logic from this check into the hyper-v driver health check , so that Hyper-V isn't accidentally selected when the user does not actually have the privilege to use it : : X Exiting due to PR_HYPERV_AS_ADMIN : Failed to start host : creating host : create : precreate : Hyper-v commands have to be run as an Administrator * Suggestion : Right-click the PowerShell icon and select Run as Administrator to open PowerShell in elevated mode . . I suggest an error message like : ' Hyper-V requires elevated privileges : Right-click the PowerShell icon and select Run as Administrator to use minikube with the Hyper-V driver`	0	0
0 0 1.4 2.0 0.7 0.0 0.95 1.0 1.0 1.0 1.0 2 1.0 0 0 2 21	Document building Docker images without local docker . This documentation should include : * how to use minikube's docker daemon to build images However , it looks like we already explain this < URL > . cc @tstromberg does this require any additional information ?  	2	2
1 1 1.0 1.0 1.3 1.5 1.2 1.0 1.0 1.0 0.0 1 0.0 1 10 21 55	fix tunnel cleanup to remove load balancer service external-ip . If a tunnel process get killed , the services still have the external-ip address set . : kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S ) AGE hello-minikube1 LoadBalancer 10.104.16.191 10.104.16.191 8080:32394 /TCP 10m kubernetes ClusterIP 10.96.0.1 < none > 443/TCP 17m . minikube tunnel supports a cleanup flag : minikube tunnel -c . , that cleanup routing table , etc , but not the external ip on the service , we should change it to also cleanup the external-ip .  	1	1
0 0 1.6 2.0 1.2 1.5 0.95 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 1 8 36	update kube-webhook-certgen image 1.2.0 -> 1.2.2 in deploy . yaml . Need to update kube-webhook-certgen image 1.2.0 . -> 1.2.2 in < URL > < URL > kube-webhook-certgen has been updated recently to 1.2.2 for adding s390x support . < URL > < URL > as per steps given here : < URL >	0	1
2 2 1.6 2.0 1.3 1.0 1.25 1.0 1.6666666666666667 2.0 1.025 40 1.0 3 4 5 29	Add support for VM-free deployments using Podman . See #4389 Podman is not yet as popular as Docker , but it's important to keep our options open and our code base abstractions clean . This may turn out to be a popular option for CentOS/RedHat VM's .  	1	1
2 2 1.0 1.0 1.0 1.0 0.85 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 1 5 18	VirtualBox on VMware : Creating VM , max retries exceeded ( connect : connection refused ) . So , I am trying to install minikube on my laptop . I have a windows 7 machine on which I have installed VMware work station and created a centos 7 machine . On that centos machine I want to install minikube . I have followed few steps and installed all the dependencies but finally # minikube start is giving error [ root @localhost ~ ]# minikube start -- vm-driver = virtualbox SERVICE_CLUSTER_IP_RANGE='X . X.X . X/24 ' -- container-runtime = docker -- extra-config kubelet . EnableCustomMetrics = true There is a newer version of minikube available ( v 0.33.1 ) . Download it here : < URL > To disable this notification , run the following : minikube config set WantUpdateNotification false Starting local Kubernetes v 1.8.0 cluster ... Starting VM ... Downloading Minikube ISO 140.01 MB / 140.01 MB [= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ==] 100.00% 0s E0214 04:21:19 . 799121 16837 start . go : 150 ] Error starting host : Error creating host : Error executing step : Creating VM . : Maximum number of retries ( 5 ) exceeded . Retrying . E0214 04:21:19 . 813063 16837 start . go : 156 ] Error starting host : Error creating host : Error executing step : Creating VM . : Maximum number of retries ( 5 ) exceeded	2	0
0 0 0.4 0.0 0.7 0.5 0.8 1.0 0.6666666666666666 1.0 2.0 1 2.0 1 1 4 27	Minikube start issues - Podman v 2.0.2 - Fedora 32 . Steps to reproduce the issue : : minikube start -- driver = podman -- container-runtime = cri-o . Full output of failed command : < URL > Full output of : minikube start . command used , if not already included : Logs provided above . Optional : Full output of : minikube logs . command : : ~ 閴?minikube logs 閳?棣冩寴 Unable to get machine status : state : unknown state ' minikube ' : sudo -n podman container inspect minikube -- format ={{ . State . Status }}: exit status 125 stdout : stderr : Error : error inspecting object : no such container minikube . Podman Version : : Version : 2.0.2 API Version : 1 Go Version : go 1.14.3 Built : Thu Jan 1 01:00:00 1970 OS/Arch : linux/amd64 . Minikube version : : minikube version : v 1.12.0 commit : c83e6c47124b71190e138dbc687d2556d31488d6 . Most likely related to issues #8587 and #8674	2	0
2 2 1.2 2.0 1.2 1.5 1.35 2.0 2.0 2.0 1.1702127659574468 47 1.0 0 2 6 78	Podman Remote driver on Mac or Win . We should test ( and document ) running the : podman . driver remotely , on non-Linux platforms . Basically these platforms install : podman-remote . as : podman . , and run commands over varlink . There is no documentation on how to set up the remote VM for Mac or Win , unfortunately ... The upstream install docs all assume that such a Linux host is already available over SSH : < URL > MacOS Podman is a tool for running Linux containers . You can do this from a MacOS desktop as long as you have access to a linux box either running inside of a VM on the host , or available via the network . You need to install the remote client and then setup ssh connection information in the podman-remote . conf file . Windows Podman is a tool for running Linux containers . You can do this from a Windows desktop as long as you have access to a linux box either running inside of a VM on the host , or available via the network . You need to install the remote client and then setup ssh connection information in the podman-remote . conf file . < URL > Theoretically using : podman-machine . should work , as long as you use the systemd one : < URL > It is also possible to do it yourself , using the hypervisor of your choice ... ( and a Fedora ISO ) There is a walkthrough here : < URL > We need to document some minikube-specific configuration , like switching over to : cgroupfs .  	2	2
2 2 1.0 1.0 1.0 1.0 1.15 1.0 1.0 1.0 0.0 0 0.0 0 1 2 6	Add HyperKit instructions to accessing_etcd . md file . . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Neither ? OK 閳?so if you look at the history of this file : < URL > @franzbeltran seems to have initially said that you should access minikube's host ( the actual computer ) with the ip address : 192.168.99.1 . ( apparently < URL > ) . @franzbeltran then updated the documentation , removing that IP address , and adding in ( instead ) : 10.0.2.2 . which is indeed correct for a < URL > . I've switched from : virtualbox . -> : hyperkit . and was lucky enough to find the first IP address when combing through the issues , so why was it removed ?  	2	2
1 1 1.4 2.0 1.3 1.5 1.05 1.0 1.6666666666666667 2.0 1.3333333333333333 6 1.5 0 0 4 35	add APIServer endpoint to `minikube profile list` . For Skaffold ( < URL > we know the current context . That context has an API server endpoint : : - cluster : certificate-authority : /Users/ ... / . minikube/ca . crt server : < URL > name : docker3 . I want to be able to join my API Server endpoint to a minikube profile . Currently this is not possible for the Docker driver , as the Node spec contains the node IP not the API Server endpoint : : .... ' Nodes ' : [ { ' Name ' : ' docker3 ' , ' IP ' : ' 172.17.0.4 ' , ' Port ' : 8443 , ' KubernetesVersion ' : ' v 1.17.2 ' , ' ControlPlane ' : true , ' Worker ' : true } ... . Can we add a cluster-wide property , : APIServer . to the json listing ?	2	1
2 2 1.2 2.0 1.0 1.0 1.05 1.0 1.3333333333333333 2.0 0.9662162162162162 148 1.0 0 3 4 37	do not pollute the json output for status with with Error . while a minikube is ' Starting ' the json output has some errors that shows the container is Not UP yet ... this error line should be converted to warnning or info when it is trying to parse ' : medya @cloudshell : ~ /campwiz2$ minikube status -o = json -l = cluster | jq . StatusName E1001 22:38:25 . 301197 184671 status . go : 533 ] unable to convert exit code to int : strconv . Atoi : parsing ' : invalid syntax E1001 22:38:25 . 302112 184671 status . go : 533 ] unable to convert exit code to int : strconv . Atoi : parsing ' : invalid syntax E1001 22:38:25 . 302145 184671 status . go : 533 ] unable to convert exit code to int : strconv . Atoi : parsing ' : invalid syntax .	0	0
0 0 0.6 0.0 1.1 1.5 0.95 1.0 1.0 1.0 1.5 2 1.5 0 1 1 14	Improve `minikube cp` command(include directory copy ) . : minikube cp . command only support regular file through : scp . . Maybe it will be more useful if it can copy directory too or handle symlink . Docker(Moby ) project well structured test cases for copy files from/to container . It will be good reference to handling directory and symbolic link . < URL > < URL > < URL > < URL >  	1	1
2 2 1.4 2.0 1.4 2.0 1.25 1.5 1.6666666666666667 2.0 0.0 4 0.0 0 2 11 49	Make minikube logs command output audit and last start logs when minikube not running . Running : minikube logs . when minkube is not running Current : : % minikube logs 棣冦仐 Profile ' minikube ' not found . Run ' minikube profile list ' to view all profiles . 棣冩啝 To start a cluster , run : ' minikube start ' . Expected : : ==> Audit <== ... ==> Last Start <== ... .	0	1
2 2 1.6 2.0 1.5 2.0 1.1 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 1 3 14	Parsing of Kubernetes version strings is brittle and doesn't display useful error messages . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Bug report Please provide the following details : Environment : Minikube version : v 0.30.0 OS : ( e.g. from /etc/os-release ): macOS Mojave 10.14 VM Driver : vmwarefusion ( VMware Fusion Pro 10.1.3 ) ISO version : minikube-v 0.30.0 . iso What happened : Tried to create a Kubernetes 1.12.1 cluster with minikube like so : : $ minikube start -- kubernetes-version 1.12.1 . Got the following error : : Starting local Kubernetes 1.12.1 cluster ... Starting VM ... Getting VM IP address ... Moving files into cluster ... E1015 10:23:32 . 389960 16692 start . go : 254 ] Error updating cluster : generating kubeadm cfg : parsing kubernetes version : parsing kubernetes version : strconv . ParseUint : parsing ' : invalid syntax = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = An error has occurred . Would you like to opt in to sending anonymized crash information to minikube to help prevent future errors ? To opt out of these messages , run the command : minikube config set WantReportErrorPrompt false = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = Please enter your response [ Y/n ]: . What you expected to happen : Either have a 1.12.1 cluster created or an error message telling me that versions need to be in a : vX . Y . Z . format ( the v is actually important and nothing works without it ) . How to reproduce it ( as minimally and precisely as possible ): : $ minikube start -- kubernetes-version 1.12.1 . The actual VM driver is irrelevant .  	1	0
0 0 0.8 1.0 0.5 0.0 0.75 1.0 1.0 1.0 0.5909090909090909 22 0.0 0 0 1 16	do not let users run ` -- no-kubernetes` on the none driver . The none driver is just running kubeadm directly on the user's machine , so no kubernetes makes no sense . We should just tell the user and error out .	0	0
0 0 0.2 0.0 0.9 0.5 1.0 1.0 0.3333333333333333 0.0 0.6666666666666666 3 0.0 3 3 8 39	-- delete-on-failure does not delete on GUEST_DRIVER_MISMATCH . Steps to reproduce the issue : : minikube start -- driver = docker . : minikube start -- driver = hyperkit -- delete-on-failure . See message on GUEST_DRIVER_MISMATCH : minikube profile list 	0	0
0 0 1.2 1.0 1.0 1.0 1.0 1.0 0.6666666666666666 1.0 1.0 3 1.0 0 0 0 1	Add a helper that registers flags both to the command-line and our viper config . We should add a helper that registers our flags both to the command and our viper config . This way , we can ensure that all flags get added as configurable values in the minikube config .	2	1
0 0 0.4 0.0 0.6 0.0 1.05 1.0 0.0 0.0 0.0 0 0.0 6 11 14 49	pulling kic base image does not respect -- image-repository or -- image-mirror-country flags for CN registry mirrors . 闁插秶骞囬梻顕€顣介幍鈧棁鈧惃鍕嚒娴?閿?婢惰精瑙﹂惃鍕嚒娴犮倗娈戠€瑰本鏆ｆ潏鎾冲毉 閿?minikube start -- registry-mirror = < URL > -- image-repository = registry.cn-hangzhou.aliyuncs.com/google_containers -- vm-driver = docker -- alsologtostderr -v = 8 : minikube logs . 閸涙垝鎶ら惃鍕翻閸?閿?W0406 10:08:19 . 436876 13086 exit . go : 101 ] Failed to start docker container . ' minikube start ' may fix it . : recreate : creating host : create : creating : create kic node : create container : failed args : [ run -d -t -- privileged -- security-opt seccomp = unconfined -- tmpfs /tmp -- tmpfs /run -v /lib/modules : /lib/modules : ro -- hostname minikube -- name minikube -- label created_by . minikube.sigs.k8s.io=true -- label name.minikube.sigs.k8s.io=minikube -- label role.minikube.sigs.k8s.io= -- label mode.minikube.sigs.k8s.io=minikube -- volume minikube : /var -- cpus = 2 -- memory = 2200mb -- expose 8443 -- publish = 127.0.0.1 :: 8443 -- publish = 127.0.0.1 :: 22 -- publish = 127.0.0.1 :: 2376 gcr.io/k8s-minikube/kicbase:v0.0.8@sha256:2f3380ebf1bb0c75b0b47160fd4e61b7b8fef0f1f32f9def108d3eada50a7a81 ] output : Unable to find image ' gcr.io/k8s-minikube/kicbase:v0.0.8@sha256:2f3380ebf1bb0c75b0b47160fd4e61b7b8fef0f1f32f9def108d3eada50a7a81 ' locally docker : Error response from daemon : Get < URL > net/http : request canceled while waiting for connection ( Client . Timeout exceeded while awaiting headers ) . See ' docker run -- help ' . : exit status 125 娴ｈ法鏁ら惃鍕惙娴ｆ粎閮寸紒鐔哄閺?閿?ubuntu 18.04  	1	1
0 0 1.2 2.0 1.0 1.0 1.25 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 0 0 7	Make minikube ( and kvm2 ) installable with ' apt ' . I couldn't find any instructions on how to install minikube with apt-get , is a package available ? There is one for : kubectl .	2	1
2 2 2.0 2.0 1.6 2.0 1.65 2.0 2.0 2.0 0.0 0 0.0 3 4 9 45	Add integration test for minikube ssh : stdin input . Steps to reproduce the issue : Download Minikube v 1.11.0 as : minikube_v 1.11.0 . ( and v 1.10.1 as : minikube_v 1.10.1 . for reference ) . Do : . /minikube_v 1.11.0 start . ( VirtualBox installed ) . Then do : : prompt$ echo ' some test data ' | . /minikube_v 1.11.0 ssh -- cat - some test data some test data . The data is handled ( twice ? ) , but the process does not terminate . For reference , the same command using : minikube_v 1.10.1 . is working as expected : : prompt$ echo ' some test data ' | . /minikube_v 1.10.1 ssh -- cat - some test data prompt$ . I assume this is a regression ? Please ask if you need any more information .	2	0
2 2 1.4 1.0 1.4 1.5 1.3 1.5 1.3333333333333333 1.0 0.0 0 0.0 0 3 8 28	minikube crash after restart os . after restart os and open terminal : : The docker host is currently not running . os info : 閴?~ lsb_release -a No LSB modules are available . Distributor ID : Ubuntu Description : Ubuntu 18.04.2 LTS Release : 18.04 Codename : bionic . minikube version : 閴?~ minikube version minikube version : v 1.2.0 . kubectl version : 閴?~ kubectl version Client Version : version . Info{Major:' 1 ' , Minor:' 15 ' , GitVersion:' v 1.15.1 ' , GitCommit:' 4485c6f18cee9a5d3c3b4e523bd27972b1b53892 ' , GitTreeState:' clean ' , BuildDate:' 2019-07-18T 09:18:22 Z ' , GoVersion:' go 1.12.5 ' , Compiler:' gc ' , Platform:' linux/amd64 ' } . kubectl config : 閴?~ kubectl config view apiVersion : v1 clusters : cluster : certificate-authority : /home/ronyang/ . minikube/ca . crt server : < URL > name : minikube contexts : context : cluster : minikube user : minikube name : minikube current-context : ' kind : Config preferences : {} users : name : minikube user : client-certificate : /home/ronyang/ . minikube/client . crt client-key : /home/ronyang/ . minikube/client . key . after delete minikube : ~ minikube delete 棣冩暉 Deleting ' minikube ' from virtualbox ... 棣冩寖 The ' minikube ' cluster has been deleted . . and open new terminal ` Error getting host : Machine does not exist for api . Exists(minikube ) Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > `	2	0
1 1 1.4 2.0 1.4 2.0 1.15 1.0 1.0 1.0 0.0 0 0.0 2 4 5 11	minikube mount fails on MacOS Big Sur . Steps to reproduce the issue : Install minikube with homebrew : brew install hyperkit && brew install minikube . Start minikube : minikube start . Try to mount a folder in minikube : minikube mount /folder : /folder . Run : minikube logs -- file = logs . txt . and drag and drop the log file into this issue Full output of failed command if not : minikube start . : < URL > < URL > This appears to be the same issue in #12537 and #11475 . I'm not sure why the reporters closed those tickets when they found a workaround because this appears to still be an issue in the code . A workaround should not be required  	1	0
2 2 1.6 2.0 1.4 2.0 1.35 2.0 1.6666666666666667 2.0 1.2608695652173914 46 1.0 4 8 10 42	annonate machine name with home name . while I was trying to add minikube integration tests for skaffold ( < URL > I gave a MINIKUBE_HOME env on my machine , then I realized if the profile name is ' minikube ' there would be another ' minikube ' vm or container with same name . it would be nice when we create VMs or containers , we add a prefix of MINIKUBE_HOME variables so they dont collide and using minikube as a testing tool don't get in the way ! in docker-driver we annoate the containers with a label .  	1	1
0 0 0.6 1.0 0.7 1.0 0.75 1.0 0.3333333333333333 0.0 0.967741935483871 124 1.0 2 4 6 28	Update default version to Kubernetes v 1.19.1 . Release notes : < URL >	0	1
0 0 1.0 1.0 1.3 2.0 1.05 1.0 1.0 1.0 0.0 0 0.0 0 2 9 40	Hung downloading ISO ( network contention with image pull ) . My minikube hangs when downloading the ISO using Minikube 1.0.0 : : curl -Lo minikube < URL > chmod +x minikube sudo mv minikube /usr/local/bin minikube start 棣冩 minikube v 1.0.0 on linux ( amd64 ) 棣冦仚 Downloading Kubernetes v 1.14.0 images in the background ... 棣冩暉 Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... 2019/04/02 11:03:47 No matching credentials were found , falling back on anonymous 棣冩崚 Downloading Minikube ISO ... 2019/04/02 11:03:47 No matching credentials were found , falling back on anonymous 2019/04/02 11:03:47 No matching credentials were found , falling back on anonymous 2019/04/02 11:03:47 No matching credentials were found , falling back on anonymous 2019/04/02 11:03:47 No matching credentials were found , falling back on anonymous 2019/04/02 11:03:47 No matching credentials were found , falling back on anonymous 1.11 MB / 142.88 MB [ > ------------------------------------------ ] 0.78% 2m33s . OS : Ubuntu 18.04 Minikube : 1.0.0 Tried with v 0.35.0 and I do not have this issue : : sudo cp ~ /Downloads/minikube-linux-amd64- 0.35.0 /usr/local/bin/minikube minikube start 棣冩 minikube v 0.35.0 on linux ( amd64 ) 棣冩寽 Kubernetes downgrade is not supported , will continue to use v 1.14.0 棣冩暉 Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... 棣冩崚 Downloading Minikube ISO ... 26.91 MB / 184.42 MB [= =====>----------------------------------- ] 14.59% 2m21s .	0	0
2 2 1.6 2.0 1.2 1.0 1.15 1.0 1.6666666666666667 2.0 0.8461538461538461 13 1.0 2 5 12 47	multi-node : unable to choose CNI ( always applies kindnet ) . Right now we enforce our own CNI for multinode clusters , ignoring any input from the user . We should respect the : -- enable-default-cni . parameter and not apply CNI when that's false .	0	0
1 1 1.2 1.0 1.1 1.0 1.05 1.0 0.6666666666666666 1.0 1.1730769230769231 52 1.0 1 5 13 46	Automatic restart is hiding the real error . We get a lot of reports , that include a partial log from the restart - but nothing from the inital start . It would be nice to improve this , to either fail on the first error or show how to report all logs ? Typical report : : sparkles : Using the docker driver based on existing profile : shrug : docker ' minikube ' container is missing , will recreate . Using the virtualbox driver based on existing profile Restarting existing virtualbox VM for ' minikube ' ... The details from the first start , that shows why it fails , is not included in the bug report or logs .  	1	1
1 1 1.4 2.0 1.3 1.5 1.3 1.5 1.0 1.0 1.3 10 2.0 1 1 7 13	Warn if incompatible kubectl version is in use . Currently , it's possible to use minikube on a host running v 1.6.1 against a master running v 1.12.1 . This isn't a supported configuration , as per < URL > We should have a flight check that validates that ancient version of kubectl aren't in use . This would also serve as an early warning to users about issues such as #3203  	1	1
0 0 0.8 1.0 0.9 1.0 1.1 1.0 0.6666666666666666 0.0 1.1538461538461537 78 1.0 1 3 3 21	Upgrade to Podman 2.1 . We are currently running Podman 1.9 Once 2.1 is stable , we could upgrade  	1	1
1 1 1.4 1.0 1.1 1.0 1.25 1.5 1.0 1.0 0.0 0 0.0 0 3 5 35	Kernel with CONFIG_IKHEADERS for BPF tools on Kubernetes . I would like the Minikube kernel to be compiled with < URL > . This is missing from < URL > . < URL > need to have access to kernel headers . It can be done either by installing : linux-headers . packages or by having a kernel compiled with : CONFIG_IKHEADERS . so that enough information can be retrieved via : /sys/kernel/kheaders . tar . xz . . BCC tools are used in < URL > , a collection of tools for developers of Kubernetes applications . I would like Inspektor Gadget to support Minikube . /cc @mauriciovasquezbernal	2	1
1 1 1.8 2.0 1.4 2.0 1.35 2.0 1.6666666666666667 2.0 0.7857142857142857 14 0.5 1 6 10 47	GCP Auth addon . Figure out how to automatically plumb GCP credentials into minikube pods For ease of use for anyone using GCP services in the k8s app in minikube .	0	1
0 0 1.2 1.0 0.9 1.0 0.95 1.0 1.3333333333333333 2.0 0.975609756097561 123 1.0 1 3 5 27	Add intial arm64 integration test . To help cover both Linux & future macOS/arm64 users .	0	1
2 2 1.4 2.0 1.6 2.0 1.4 2.0 1.0 1.0 0.0 0 0.0 0 1 3 17	  docs/handbook/pushing : reorder the content . At < URL > the top lists methods to upload images : docker-env command podman-env command ctr/buildctl command Afterwards the methods are described in more detail in the order : 1 . Pushing directly to the in-cluster Docker daemon ( docker-env ) 2 . Push images using 閳?cache 閳?command . 3 . Pushing directly to in-cluster CRI-O . ( podman-env ) The order of the methods at the top , shall be the same as the order , in which the methods are described in more detail .	0	2
0 0 0.8 1.0 0.7 1.0 1.1 1.0 0.3333333333333333 0.0 0.0 1 0.0 4 15 20 51	Handbook : ' Configuration ' page should link to ' config ' command doc . In ' Listing config properties ' getting rid of the properties/ config items list seems a bit of a regression to me . I understand that that will change with each new ( possibly minor ) version of minikube , but it would be great to not just have a list , but also have a short explanation of what each property does , as there's no additional information of what the possible options are . even just linking to the files where the current options are defined ( as the kubernetes doc does in some places ) would be handy . This would also allow people to understand if any properties have been disabled in newer minikube versions  	2	2
1 1 1.0 1.0 1.3 2.0 1.3 2.0 1.0 1.0 1.0 1 1.0 0 6 9 56	Do not allow running darwin/amd64 minikube on darwin/arm64 macs . M1 macs allow to run amd64 in rosetta mode . We need to prohibit this , as it leads to downloading amd64 preloads , running amd64 kubeadm and SEGFAULT in the end . runtime . GOARCH is ' amd64 ' in this case , so we need an extra step to detect what we're running im emulation mode , like using : gopsutil . library , or running an external binary like : arch . or : uname .	0	0
2 2 1.6 2.0 1.1 1.5 1.05 1.0 2.0 2.0 0.0 0 0.0 0 4 8 48	update the Outdated catalog in OLM addon . I was wondering why my minikube instance is installing old operator versions . Then I found out that the : CatalogSource . image in the OLM addon points to the SHA of the image that is several months old . < URL > It might make sense for this one case to be an exception from < URL > and to use : latest . tag . Otherwise , every user will have to update the image refernce themself to pick up the latest changes in Operator Catalog .	1	1
2 2 1.2 2.0 1.4 2.0 1.25 2.0 1.3333333333333333 2.0 1.1166666666666667 60 1.0 0 2 10 37	Podman v2 doesn't work with podman-env . If you want to build images using : podman . , you will have to use release 1.9.3 The earlier releases ( 1.8.2 ) hangs , and the later releases ( 2.0.0 ) gives an error . It should also be OK to use the 1.6 . x releases , available in many distributions . The ' varlink ' socket is not supported anymore , so need to change minikube for it . < URL > ... Podman API v 1.0 for Podman is considered to be deprecated . If there are issues with the Podman API v 1.0 in versions of Podman prior to v 2.0 and those versions are still under support on Red Hat Enterprise Linux ( RHEL ) , the Podman team will make a best effort to address those issues . However , no new feature requests for the API v 1.0 will be considered and any problems found with the API v 1.0 in Podman v 2.0 will not be addressed . This also serves as a notification that the Podman v 1.0 ( varlink ) API will be removed from the main GitHub branch of Podman in the near future . With the release of Podman v 2.0 the Podman developers deprecated the Podman API v 1.0 in favor of the new Podman v 2.0 RESTful API . The plan is to remove varlink completely from the Podman v 3.0 development branch which will be created some time after September 2020 . Original post : < URL >  	1	0
0 0 1.4 2.0 1.3 1.5 1.2 1.0 1.3333333333333333 2.0 1.0833333333333333 12 1.0 0 4 7 21	Latest release kvm2 driver still linking to newer libvirt . Seems that the driver was not built using the Docker image : : $ wget -q < URL > $ ldd docker-machine-driver-kvm2 . /docker-machine-driver-kvm2 : /usr/lib/x86_64-linux-gnu/libvirt-lxc . so . 0 : version `LIBVIRT_LXC_ 2.0.0 ' not found ( required by . /docker-machine-driver-kvm2 ) . /docker-machine-driver-kvm2 : /usr/lib/x86_64-linux-gnu/libvirt . so . 0 : version `LIBVIRT_ 2.2.0 ' not found ( required by . /docker-machine-driver-kvm2 ) . /docker-machine-driver-kvm2 : /usr/lib/x86_64-linux-gnu/libvirt . so . 0 : version `LIBVIRT_ 3.0.0 ' not found ( required by . /docker-machine-driver-kvm2 ) . /docker-machine-driver-kvm2 : /usr/lib/x86_64-linux-gnu/libvirt . so . 0 : version `LIBVIRT_ 1.3.3 ' not found ( required by . /docker-machine-driver-kvm2 ) . /docker-machine-driver-kvm2 : /usr/lib/x86_64-linux-gnu/libvirt . so . 0 : version `LIBVIRT_ 2.0.0 ' not found ( required by . /docker-machine-driver-kvm2 ) . It is supposed to still be using libvirt 1.3.1 , for compatibility . : FROM gcr.io/gcp-runtimes/ubuntu_16_0_4 .	0	0
2 2 1.0 1.0 1.1 1.0 1.4 2.0 1.3333333333333333 2.0 1.6111111111111112 18 2.0 0 2 5 27	detect connectivity problem to image repository before start . rather than spinning up minikube and wait and retry and then crashing it would be nicer , if minikube tries to ping or check the network connectivity to gcr ( or whatever image repository specified ) and if it there is no network connection minikube should nicely not continue and recommend the user to start with correct image-repos options related : < URL >  	1	1
0 0 0.8 0.0 0.8 0.0 0.95 1.0 1.3333333333333333 2.0 0.9629629629629629 27 1.0 1 4 8 29	Add minikube IP to NO_PROXY if HTTPS_PROXY is set . This avoids the issue of folks not being able to contact the apiserver .	0	1
0 0 0.4 0.0 0.5 0.0 0.85 1.0 0.6666666666666666 1.0 0.3 10 0.0 1 2 2 23	`minikube version -- components` docker error . : % minikube version -- components ... docker : error ... . MacOS : 11.5.1 Docker : 20.10.7  	1	0
1 1 0.2 0.0 0.5 0.0 0.85 0.5 0.3333333333333333 0.0 1.1754385964912282 114 1.0 1 3 6 24	Random failures from DeleteAllProfiles unit test . Due to it making direct calls to : docker . , and finding profiles outside the test context . : --- FAIL : TestDeleteAllProfiles ( 0.20 s ) delete_test . go : 203 : ListProfiles length = 9 , expected 8 valid : [] invalid : [ 0xc000d73320 0xc000d73350 0xc000d73380 0xc000d733b0 0xc000d733e0 0xc000d73410 0xc000d73440 0xc000d73470 0xc000d734d0 ] . There should be a mocked docker ( : oci . Docker . ) , similar to the mock : miniHome . . : pDirs , err : = profileDirs(miniHome ... ) cs , err : = oci . ListOwnedContainers(oci . Docker ) . : err = os . Setenv(localpath . MinikubeHome , td ) . The invalid profiles are : [ p1 p2_empty_profile_config p3_invalid_profile_config p4_partial_profile_config p5_missing_machine_config p6_empty_machine_config p7_invalid_machine_config p8_partial_machine_config minikube ] It also shouldn't just make random calls to Docker like that , but that's another story . ( #8577 )	0	0
1 1 0.2 0.0 0.7 0.5 0.95 1.0 0.3333333333333333 0.0 1.1428571428571428 126 1.0 0 3 5 21	Start cri-docker service automatically , if using docker and kubernetes version > = 1.24 . As mentioned in ~ #9868 ~ , using docker requires CRI starting from kubernetes 1.24 up : < URL > It is being casually updated by Mirantis , for use in their commercial products : < URL > < URL > We should make sure to include : cri-dockerd . , and switch to : /var/run/cri-docker . sock . Most things should be prepared in the cruntime already . ~ ~ Unfortunately it seems upstream doesn't provide binaries ? ~ ~ ~~> There is a Makefile with a few targets . : make deb . or : make rpm . will probably have you covered , and you can install the packages as normal . If you閳ユ獧e using a different distribution , : make static . will give you raw binaries you can invoke . ~ ~ ~~< URL >~~  	1	1
0 0 1.0 1.0 0.9 1.0 0.95 1.0 1.0 1.0 1.625 16 2.0 3 4 8 33	docs : remove or replace /concepts boilerplate . seems to be a boilerplate leftover < URL >    	1	2
2 2 1.2 1.0 1.3 1.5 1.3 1.5 1.6666666666666667 2.0 1.25 8 1.0 2 3 7 30	Upgrade Podman to 1.2 ( and cri-o ) . We can upgrade Podman , from 1.0.0 to 1.2.0 . < URL >   	1	1
1 1 1.4 1.0 1.2 1.0 1.05 1.0 1.6666666666666667 2.0 1.263157894736842 19 1.0 0 4 5 29	Add branding to distribution . Currently we ship the distribution with an unmodified : /etc/os-release . like so : : ( / echo ' NAME = Buildroot ' ; / echo ' VERSION =$( BR2_VERSION_FULL )'; / echo ' ID = buildroot ' ; / echo ' VERSION_ID =$( BR2_VERSION )'; / echo ' PRETTY_NAME=/'Buildroot $(BR2_VERSION)/' / ) > $(TARGET_DIR)/usr/lib/os-release . But we do our own configuration and modification , to the basic Buildroot system . The libmachine upstream boot2docker . iso does this , and looks something like : : docker @default : ~ $ cat /etc/os-release NAME = Boot2Docker VERSION = 19.03.1 ID = boot2docker ID_LIKE = tcl VERSION_ID = 19.03.1 PRETTY_NAME='Boot2Docker 19.03.1 ( TCL 10.1 )' ANSI_COLOR='1;34 ' HOME_URL='<URL>' SUPPORT_URL='<URL>' BUG_REPORT_URL='<URL>' . The original distribution ( in our case ' buildroot ' ) ends up under ' ID_LIKE ' . Also the Linux version gets a suffix , to indicate custom kernel configuration . : docker @default : ~ $ uname -r 4.14.134 -boot2docker . We could do our own branding here , and advertise minikube and ISO version ? : NAME = minikube VERSION = 1.3.0 ID = minikube ID_LIKE = buildroot VERSION_ID = 1.3.0 PRETTY_NAME='minikube 1.3.0 ( Buildroot 2018.05.3 )' HOME_URL='<URL>' . < URL >	2	1
2 2 1.6 2.0 1.1 1.5 1.0 1.0 2.0 2.0 0.0 0 0.0 0 1 4 25	Add automation to generate help documents for CLI commands . Discussed over here - < URL > Add automation to generate the documents of all the CLI commands available including the flags .  	1	1
2 2 1.2 2.0 1.2 1.5 1.25 1.5 1.3333333333333333 2.0 1.0147058823529411 68 1.0 0 0 2 30	Lock files should be per-user ! . I was running into this issue while trying to let users create their own test clusters at a shared box . Is there a similar issue for the following files , which need to be separated as well ? : /tmp/juju-kubeconfigUpdate /tmp/juju-kube-config . In addition , is it safe using a shared user for now ( e.g. via : sudo -u < user > minikube ... . ) to manage different ' profiles ' ? As far as I understood from the code ( from : juju/mutex_flock . ) , these files only get used for locking temporarily , correct ? May I also ask why these temporary files never actually seem to get deleted ? Originally posted by @enote -kane in < URL >	2	0
1 1 1.0 1.0 1.1 1.0 1.0 1.0 1.3333333333333333 1.0 1.140625 128 1.0 4 6 9 25	Document differences between kind and kic . We should have a page in the documentation , documenting the difference between kind and minikube's kic . Some of the differences are historic , like docker vs . podman . And some are choices , like containerd vs . crio . But it can be confusing for the user , when there are two components with similar name and similar code . Just like with : kubeadm . , there is supposed to be a collaboration between the Kubernetes SIGs involved ... Major similarities : Uses system containers . Uses Ubuntu OS ( systemd ) Uses kubeadm installer . Basic differences : KIC (' Kubernetes in Container ' ) minikube focus on helping application developers and new Kubernetes users . SIG Cluster Lifecycle Defaults to latest stable . Ubuntu 20.04 LTS Dependencies from system packages . ( some of them from the < URL > project ) Installs kubernetes from binaries . Preload tarballs building on ' base ' . Default runtime : docker Installs SSH daemon KIND (' Kubernetes in Docker ' ) kind was primarily designed for testing Kubernetes itself , but may be used for local development or CI . SIG Testing Defaults to latest/greatest . Ubuntu 21.10 Dependencies from upstream tarballs . ( some of them built locally , like < URL > ) Builds kubernetes from source . Node image building on ' base ' . Default runtime : containerd Uses ' docker exec ' To Be Continued ...  	2	2
2 2 1.4 2.0 1.5 2.0 1.3 1.5 2.0 2.0 0.9821428571428571 112 1.0 0 1 4 41	service on docker with Ctrl-C : `error stopping tunnel : stopping ssh tunnel : os : process already finished` . This is a useless and necessarily scary error message from : minikube service . when Ctrl-C is hit on macOS and the Docker driver : : 棣冨竴 Opening service triage-party/triage-party in default browser ... 閴?Because you are using a Docker driver on darwin , the terminal needs to be open to run it . ^C閴?Stopping tunnel for service triage-party . 棣冩寴 error stopping tunnel : stopping ssh tunnel : os : process already finished 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > .  	1	0
1 1 1.4 2.0 1.2 1.5 1.45 2.0 1.0 1.0 0.0 0 0.0 1 1 1 13	DNS resolution not working with vm-driver none . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.30.0 - OS ( e.g. from /etc/os-release ): Ubuntu 16.04.4 LTS ( Xenial Xerus ) - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): none - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): - Install tools : - Others : The above can be generated in one go with the following commands ( can be copied and pasted directly into your terminal ): : minikube version echo ' ; echo ' OS:'; cat /etc/os-release echo ' ; echo ' VM driver:'; grep DriverName ~ / . minikube/machines/minikube/config . json echo ' ; echo ' ISO version ' ; grep -i ISO ~ / . minikube/machines/minikube/config . json . What happened : service name resolution is un-stable , once it is resolved , after a while it won't What you expected to happen : service name should be resolved . How to reproduce it ( as minimally and precisely as possible ): Create a pod and expose a service . Poll to the above service from another pod . Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	0
0 0 1.0 1.0 1.0 1.0 1.25 1.5 1.0 1.0 0.0 0 0.0 1 4 7 43	cloudshell addon for in-cluster web terminal . Would having a web-based terminal running in-cluster be a welcome addon to minikube ? < URL > this in < URL > ( not with minikube , yet ; just the basis ) . The workflow I imagine is : minikube addons enable cloudshell ; minikube addons open cloudshell . and your browser would take you to a shell to runs ' in-cluster ' from where you could use : kubectl . ( which would already be included in that shell ) and what not . It would also be neat if , after basics are there , one could develop code inside that shell , and build containers ( ideally without < URL > , but with building root less inside the cloudshell ) , push < URL > , and run . @tstromberg	2	1
1 1 0.4 0.0 1.0 1.0 1.05 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 0 4 25	minikube driver check should not call sudo , if sudo fails . Steps to reproduce the issue : minikube start ( without : -- driver . ) Full output of : minikube logs . command : : 棣冩 minikube v 1.22.0 on Debian 11.0 閴?Automatically selected the docker driver . Other choices : kvm2 , ssh 棣冩啢 Starting control plane node minikube in cluster minikube [ ... ] . It seems that minikube stills calls podman executable ( and sudo ) even if it tells that it uses docker . I see logs/mails coming from sudo telling me that minikube tried to run podman executable without providing a password Messages like : edoras.bigon.be : Jul 14 09:36:54 : bigon : a password is required ; TTY = pts/2 ; PWD = /home/bigon/bin ; USER = root ; COMMAND = /usr/bin/podman version -- format {{ . Version }} edoras.bigon.be : Jul 14 10:05:28 : bigon : a password is required ; TTY = pts/2 ; PWD = /home/bigon ; USER = root ; COMMAND = /usr/bin/podman ps -a -- filter label = name.minikube.sigs.k8s.io=minikube -- format {{ . Names }} edoras.bigon.be : Jul 14 10:05:28 : bigon : a password is required ; TTY = pts/2 ; PWD = /home/bigon ; USER = root ; COMMAND = /usr/bin/podman volume ls -- filter label = name.minikube.sigs.k8s.io=minikube -- format {{ . Name }} edoras.bigon.be : Jul 14 10:05:28 : bigon : a password is required ; TTY = pts/2 ; PWD = /home/bigon ; USER = root ; COMMAND = /usr/bin/podman network ls -- filter = label = created_by . minikube.sigs.k8s.io -- format {{ . Name }} [ ... ]	0	0
0 0 1.4 2.0 1.6 2.0 1.45 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 0 0 3	Omission of -- kubernetes-version upgrades cluster to the default version . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : Mac Sierra ( 10.12.6 ) Minikube version ( use : minikube version . ): - OS ( e.g. from /etc/os-release ): Buildroot 2017.02 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): virtualbox - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): minikube-v 0.23.6 . iso What happened : minikube upgraded itself from 1.8 to 1.9 when it was stopped and started What you expected to happen : don't upgrade How to reproduce it ( as minimally and precisely as possible ): : minikube start -- kubernetes-version v 1.8.0 kubectl version minikube stop minikube start kubectl version .    	1	2
2 2 1.4 2.0 1.5 2.0 1.45 2.0 2.0 2.0 0.0 0 0.0 1 1 3 11	VM has 50% resting CPU usage when idle . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : macOS 10.13.6 Minikube version ( use : minikube version . ): v 0.29.0 - OS ( e.g. from /etc/os-release ): macOS 10.13.6 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): hyperkit - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.29.0 - Install tools : n/a - Others : n/a What happened : I just installed and set up a fresh minikube cluster . CPU usage is pegged at ~ 50% even though no pods have been launched and nothing is happening on the cluster . I've observed the same behavior across both hyperkit and VirtualBox . I ran : minikube addons enable heapster . to get some insight into where all the CPU is going . It looks like kube-apiserver-minikube and kube-controller-manager-minikube are the primary offenders . What you expected to happen : I expected the CPU usage to fall to basically zero when at rest . I understand that this may just be the baseline CPU usage for some of these services ( liveness checks , etc ) . But when running in minikube mode , it'd really be nice to slow down the CPU consumption so that we don't kill all of our laptop batteries . How to reproduce it ( as minimally and precisely as possible ): Create a minikube cluster on macOS with the appropriate versions . Output of : minikube logs . ( if applicable ) : n/a Anything else do we need to know : n/a  	1	0
2 2 1.0 1.0 1.0 1.0 1.2 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 4 5 13	Set default location for PV mounts . Is this a BUG REPORT or FEATURE REQUEST ? Feature request/query Minikube version ( use : minikube version . ): v 0.30.0 - OS : Arch Linux - VM Driver : None The < URL > list a number of locations that Minikube can provision persistent volumes , as well as sample config for a PV , but there doesn't seem to be any way to configure the storage-provisioner to provision volumes anywhere but /tmp/hostpath-provisioner . Unless I'm misunderstanding < URL > , it looks like the path in : tmp . is fixed and unparameterised . Is there some way of providing a patched storage-provisioner to minikube with a path of the user's choice ? Our use case involves persisting a lot of large data files , for which a tmpfs-based PV quickly becomes impractical .	2	1
2 2 1.0 1.0 0.9 0.5 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 3 5 7 33	棣冩▼ Failed to start ssh bare metal machine . Running ' minikube delete ' may fix it : provision : fast detect : OS type not recognized 閴?Exiting due to GUEST_PROVISION : Failed to start host : provision : fast detect : OS type not recognized . Steps to reproduce the issue : 1 . 2 . 3 . Full output of : minikube logs . command : Full output of failed command :	2	0
1 1 1.2 1.0 1.4 1.5 1.3 1.5 1.0 1.0 1.4 5 2.0 1 3 8 28	Changing VM hardware ( memory , cores ) has no effect after provisioning . Pretty sure this would be the case with other drivers as well . The exact command to reproduce the issue : : . /minikube-windows-amd64 . exe start -- vm-driver = hyperv -- hyperv-virtual-switch='Default Switch ' -- memory = 1700 . /minikube-windows-amd64 . exe start -- vm-driver = hyperv -- hyperv-virtual-switch='Default Switch ' -- memory = 1500 -- cpus = 1 . The cluster initially gets provisioned with the memory and CPU but if I start it again with new parameters , the VM is just started with the original values as you can see from the below screenshot - I think this should be configurable . The only thing which should not be configurable in size is the hard disk size . The operating system version : Windows 10 Enterprise . Pretty sure this would be applicable across all drivers and all the operating systems .   	1	1
2 2 1.6 2.0 1.1 1.5 0.95 1.0 1.3333333333333333 2.0 0.0 0 0.0 3 6 8 34	  < URL > linux x86_64 rpm download link fails . Download link for ' Linux x86-64 rpm in < URL > is given as : curl -LO < URL > But this doesn't work . Real link appears to be at curl -LO < URL > ( underscore instead of hyphen )	0	2
1 1 1.2 1.0 0.8 0.5 1.1 1.0 1.3333333333333333 1.0 1.0 1 1.0 2 3 16 56	' Error from Server ' on Minikube pause / unpause with Skaffold . Minikube 1.7.3 , started with : -- vm-driver docker . To reproduce : - Pause minikube : : minikube pause -p [ my-profile ] . - Unpause - Run : skaffold dev . with : -- minikube-profile [ my-profile ] . Error printed in the Skaffold output : : [ java-hello-world-6c6ffc74bf-cpv5v server ] Error from server : Get < URL > dial tcp 192.168.9.2 : 10250 : connect : connection refused . This does not happen when starting the Skaffold session after Minikube is in a : running . state . I can only repro this when running Skaffold immediately after unpausing Minikube .	2	0
0 0 0.6 1.0 0.5 0.5 0.85 1.0 0.3333333333333333 0.0 1.175 40 1.0 4 6 11 32	docs : add tutorial for RBAC . as seen in this issue , using RBAC could be confusing the working solution is provided here < URL > we need to document a good example of using rbac in minikube and add it to the tutorials in the website  	2	2
0 0 1.0 1.0 1.4 2.0 1.2 1.5 0.6666666666666666 0.0 0.9891304347826086 92 1.0 2 3 5 42	  docs don't mention how to enable feature gates . < URL > is missing mention of feature gates ! example : : minikube start -- feature-gates = EphemeralContainers = true .   	1	2
1 1 0.6 0.0 0.8 0.5 0.9 1.0 1.0 1.0 0.8 5 1.0 1 3 3 26	Create flake chart for rolling average of 7 days . . Currently our flake charts show a daily average , it would also be helpful to see a chart averaged by the past week  	1	1
0 0 1.4 2.0 1.0 1.0 1.15 1.0 1.3333333333333333 2.0 0.9425287356321839 174 1.0 1 2 3 23	add arm64 binaries to gcloud . gcloud on arm64 does not have minikube as a installable component , we need to share the arm64 binaries with gcloud	0	0
2 2 0.6 0.0 0.7 0.0 0.9 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 1 6 59	CoreDNS fails on minions on multi-node clusters . Can't resolve external DNS from non-master pods . . So , I already fixed this and lost some of the logs . But it's pretty straight-forward . 1 . Make a cluster : minikube start -- vm-driver = kvm2 -- cpus = 2 -- nodes 3 -- network-plugin = cni / -- addons registry -- enable-default-cni = false / -- insecure-registry ' 10.0.0.0 /24 ' -- insecure-registry ' 192.168.39.0 /24 ' / -- extra-config = kubeadm . pod-network-cidr = 10.244.0.0 /16 / -- extra-config = kubelet . network-plugin = cni kubectl apply -f < URL > . n.b. I built from head a couple days ago : minikube version : v 1.10.0 -beta . 2 commit : 80c3324b6f526911d46033721df844174fe7f597 . make a pod on master and a pod on a node from node pod : : curl google.com . from master pod : : curl google.com . CoreDNS was crashing per < URL > Fixed with : kubectl patch deployment coredns -n kube-system -- patch ' {' spec ' :{ ' template ' :{ ' spec ' :{ ' volumes ' :[{ ' name ' : ' emptydir-tmp ' , ' emptyDir ' :{ }}] , ' containers ' :[{ ' name ' : ' coredns ' , ' volumeMounts ' :[{ ' name ' : ' emptydir-tmp ' , ' mountPath ' : ' /tmp ' }]}]}}}}' . Edit : had wrong flannel yaml listed .	0	0
0 0 1.0 1.0 1.3 2.0 0.9 1.0 1.0 1.0 1.1134020618556701 97 1.0 5 11 22 71	alias on the website docs doesnt seem to work . Example : this link < URL > doesnt work the correct link is < URL > in that page I see there is an alias for it : --- title : ' Proxies & VPN's ' weight : 6 description : > How to use minikube with a VPN or HTTP/HTTPS Proxy aliases : - docs/reference/networking/vpn - docs/reference/networking/proxy --- . but the alias doesnt seem to be working	0	0
1 1 1.2 1.0 1.2 1.0 1.05 1.0 1.6666666666666667 2.0 0.6666666666666666 12 0.0 0 5 10 47	Save kic base image to cache on -- download-only if docker isn't running . Right now , if we run : minikube start -- download-only -- driver docker . it fails if docker is not running . We should be able to download all required artifacts even if docker is not yet up . If we detect that docker isn't running , we should save the kic base image as a tarball in the minikube cache and load it if it's there on : minikube start . This way we still download all required artifacts when : -- download-only . is passed in .	0	1
2 2 1.4 2.0 1.2 1.5 1.2 1.5 2.0 2.0 1.125 56 1.0 4 8 14 50	Upgrade CRI-O to 1.18.3 . For Kubernetes 1.19 etc , move to crio 1.18.1 and podman 1.9.3 EDIT : Most likely , this also needs a newer version of conmon	0	1
0 0 1.2 2.0 1.0 1.0 1.3 2.0 1.3333333333333333 2.0 0.9473684210526315 171 1.0 3 6 11 42	minikube . exe in WSL2 is using windows home . as seen in internal issue cloud code was failing because it could not run : minikube docker-env . command but it turns out running minikube . exe docker-env doesnt work but without . exe works because the . exe looks for a different minikubehome : quoct @quoct0 -w : ~ $ minikube docker-env -- shell none -p cloud-run-dev-internal DOCKER_TLS_VERIFY = 1 DOCKER_HOST = tcp :/ / 127.0.0.1 : 32794 DOCKER_CERT_PATH = /home/quoct/ . minikube/certs MINIKUBE_ACTIVE_DOCKERD = cloud-run-dev-internal quoct @quoct0 -w : ~ $ /mnt/c/Users/quoct/AppData/Local/Google/Cloud/ SDK/google-cloud-sdk/bin/minikube . exe profile lis^C quoct @quoct0 -w : ~ $ /mnt/c/Users/quoct/AppData/Local/Google/Cloud/ SDK/google-cloud-sdk/bin/minikube . exe docker-env -- shell none -p cloud-run-dev-internal * There is no local cluster named ' cloud-run-dev-internal ' - To fix this , run : ' minikube start -p cloud-run-dev-internal ' . this causes confusiion for users . we need to fix this . we shoudl either make the . windows binary . exe to work in WSL2 or we should warn the user about the confusion  	1	0
0 0 0.6 0.0 0.6 0.0 0.75 0.0 0.3333333333333333 0.0 1.2307692307692308 13 2.0 0 0 4 8	Update podman to v 1.0.0 . It's out : < URL > It will require updating podman.mk, podman . hash , and going through < URL >  	1	1
2 2 1.6 2.0 1.5 2.0 1.45 2.0 1.6666666666666667 2.0 0.8564356435643564 202 1.0 9 11 17 54	crictl installed in kic-base does not have a version . : why crictl version shows this ? are we installing an ancient version of Crictl ? ( we have been facing issue of pulling images ) that we are suspecting cricttl is the source docker @minikube : ~ $ sudo crictl version Version : 0.1.0 RuntimeName : docker RuntimeVersion : 20.10.3 RuntimeApiVersion : 1.41.0 docker @minikube : ~ $ sudo crictl -- version crictl version unknown . but when I installed crictl using wget I get correct verison : docker @minikube : ~ $ wget < URL > docker @minikube : ~ $ sudo tar zxvf crictl-$VERSION-linux-amd64 . tar . gz -C /usr/local/bin crictl docker @minikube : ~ $ crictl -- version crictl version unknown docker @minikube : ~ $ /usr/local/bin/crictl -- version crictl version v 1.20.0 .  	1	0
2 2 1.2 1.0 1.0 1.0 1.1 1.0 1.3333333333333333 1.0 0.47619047619047616 21 0.0 0 1 3 17	Output suggested memory flag instead of existing memory flag . Example : : $ minikube start -- memory = 16385 棣冩 minikube v 1.24.0 on Darwin 12.0.1 閴?Automatically selected the docker driver . Other choices : hyperkit , ssh 閴?Exiting due to RSRC_OVER_ALLOC_MEM : Requested memory allocation 16385MB is more than your system limit 16384MB . 棣冩寱 Suggestion : Start minikube with less memory allocated : ' minikube start -- memory = 16385mb ' . The suggested command of : minikube start -- memory = 16385mb . is obviously going to fail , we should output the suggested limit . Note : If you pass a flag that includes units ( : 16385mb . vs : 16385 . ) , the message will output the suggested limit as expected : $ minikube start -- memory = 16385mb 棣冩 minikube v 1.24.0 on Darwin 12.0.1 閴?Automatically selected the docker driver . Other choices : hyperkit , ssh 閴?Exiting due to RSRC_OVER_ALLOC_MEM : Requested memory allocation 16385MB is more than your system limit 16384MB . 棣冩寱 Suggestion : Start minikube with less memory allocated : ' minikube start -- memory = 4000mb ' . This is due to it not returning an int value here : < URL >	0	0
0 0 1.4 2.0 1.3 2.0 1.0 1.0 1.3333333333333333 2.0 0.826271186440678 236 1.0 0 1 5 29	docker . socket vs docker . service in our code base . for non-system OS  	1	1
0 0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9777777777777777 45 1.0 1 6 6 29	Detect -- extra-config issues immediately rather than waiting 10 minutes . If a user feeds bad data into apiserver , the apiserver crashes immediately with messages that can be found using : minikube logs -- problems . . : ~ minikube logs -- problems Thu 18 Jul 2019 09:29:35 AM PDT 閴?Problems detected in ' kube-apiserver ' : error : invalid audit log mode 777 , allowed modes are ' batch , blocking , blocking-strict ' 閴?Problems detected in ' kube-addon-manager ' : error : unable to recognize ' STDIN ' : Get < URL > dial tcp 127.0.0.1 : 8443 : connect : connection refused error : unable to recognize ' STDIN ' : Get < URL > dial tcp 127.0.0.1 : 8443 : connect : connection refused error : unable to recognize ' STDIN ' : Get < URL > dial tcp 127.0.0.1 : 8443 : connect : connection refused . This makes experimentation hard . Instead of telling users about it immediately , we currently make them wait ~ 8 minutes . We can do better :)   	1	1
1 1 1.2 1.0 1.3 1.5 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 3 7 56	Support backward compatibility for old profiles . I have installed the latest minikube version v 1.8.1 ( I can't remember the previous version ) , but then all my clusters wouldn't work because their profiles became invalid . I checked the code that reads the configuration from the profile and obviously the structure of the profile has changed . I suggest to keep the profiles backward compatible as far back as possible , and display warnings about attributes that will be changed in the profile rather than prompting the users to delete the profile . If you feel that's the right way I can start with putting back some of the renamed attributes ? The exact command to reproduce the issue : : minikube profile list . The full output of the command that failed : 閳跨媴绗?Found 4 invalid profile(s ) ! keda-external-scaler minikube twitch-keda twitter-scaler 棣冩寱 You can delete them using the following command(s ): $ minikube delete -p keda-external-scaler $ minikube delete -p minikube $ minikube delete -p twitch-keda $ minikube delete -p twitter-scaler The operating system version : MacOS Catalina  	1	0
0 0 1.2 2.0 1.1 1.0 1.0 1.0 1.3333333333333333 2.0 0.96 125 1.0 1 1 7 41	root-owned config exits with ExHostConfig istead of ExHostPermissions . To reproduce : : minikube stop . : sudo chown -R root $HOME/ . minikube . : minikube start . : . /out/minikube start -- driver = docker ; echo $status E0921 18:18:31 . 603532 43694 cloud_events . go : 57 ] unable to write to /Users/tstromberg/ . minikube/profiles/minikube/events . json : open /Users/tstromberg/ . minikube/profiles/minikube/events . json : permission denied 棣冩 minikube v 1.13.1 on Darwin 10.15.6 閴?Exiting due to HOST_CONFIG_LOAD : Unable to load config : read : open /Users/tstromberg/ . minikube/profiles/minikube/config . json : permission denied .  	1	0
2 2 1.4 2.0 1.0 1.0 0.75 0.0 1.3333333333333333 2.0 1.0560747663551402 107 1.0 2 5 17 78	TestPause/serial/SecondStartNoReset : api server error . as seen here : < URL > TestPause/serial/SecondStartNoReset reveals a problem in minikube that right after a full healthy start ( -- wait = all ) the apiserver will go to an state of error ( one suspection could be other tests doing too many kubedm init ( which is expensive CPU and memory ) as seen in this test : : pause_test . go : 78 : expected the second start log outputs to include ' The running cluster does not need a reset ' but got : . and the exact relevant error is : : I0424 15:10:25 . 506755 13119 kubeadm . go : 452 ] needs reset : apiserver in state Error .	0	0
2 2 1.0 1.0 1.2 1.0 1.1 1.0 1.3333333333333333 1.0 0.0 0 0.0 2 3 15 35	status : ' Error getting ssh host name for driver : IP not found ' . : minikube . command-line used : minikube status - The full output of the command that failed Error getting bootstrapper : getting kubeadm bootstrapper : command runner : getting ssh client for bootstrapper : Error creating new ssh host from driver : Error getting ssh host name for driver : IP not found Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - The output of the ' minikube logs ' command - Which operating system version was used Windows 10	2	0
2 2 1.0 1.0 1.2 1.5 1.1 1.0 1.3333333333333333 2.0 0.907103825136612 183 1.0 4 5 10 33	allow Minikube users to config receive notification for BETA and RC .	0	1
0 0 0.2 0.0 0.6 0.0 0.65 0.0 0.0 0.0 0.0 0 0.0 1 5 5 22	site : Add command-line download instructions for windows binary . fixes #11861 < URL >    	1	2
2 2 1.6 2.0 1.5 1.5 1.15 1.0 2.0 2.0 0.0 0 0.0 2 5 7 39	disabling heapster after enabling crashes minikube . Versioning information below . I was just going through the Hello Minikube tutorial so I hadn't done much with it at all . As per the tutorial , I enabled the heapster addon which worked successfully . Then I tried to disable it as per the tutorial and got the following message : : C : /Users/Mark > minikube addons disable heapster ! disable failed : [ disabling addon deploy/addons/heapster/influx-grafana-rc . yaml : Process exited with status 1 ] . The other thing that might be notable is that : minikube addons enable heapster . didn't actually create any heapster pods or services for me ( the tutorial suggests that those pods would be created , but they weren't available in the output of : kubectl get pod , svc -n kube-system . ) I'm using Minikube 0.35.0 on Windows 10 with VirtualBox . Here are my kubectl and Kubernetes versions : : Client Version : version . Info{Major:' 1 ' , Minor:' 14 ' , GitVersion:' v 1.14.0 ' , GitCommit:' 641856db18352033a0d96dbc99153fa3b27298e5 ' , GitTreeState:' clean ' , BuildDate:' 2019-03-25T 15:53:57 Z ' , GoVersion:' go 1.12.1 ' , Compiler:' gc ' , Platform:' windows/amd64 ' } Server Version : version . Info{Major:' 1 ' , Minor:' 13 ' , GitVersion:' v 1.13.4 ' , GitCommit:' c27b913fddd1a6c480c229191a087698aa92f0b1 ' , GitTreeState:' clean ' , BuildDate:' 2019-02-28T 13:30:26 Z ' , GoVersion:' go 1.11.5 ' , Compiler:' gc ' , Platform:' linux/amd64 ' } .	2	0
1 1 1.2 1.0 1.3 1.0 1.1 1.0 1.0 1.0 0.9534883720930233 129 1.0 2 2 7 38	Fatal errors should include the path to the post-mortem INFO log . Currently , we provide this information , which no one sees : < URL > I was thinking that when we have a fatal exit , we should be able to say something like : : Please open a GitHub issue . If you are able to drag and drop the following log-file into the issue , we'll be able to make faster progress : /path/to/minikube . INFO . log . Related : #9323	0	1
2 2 1.4 1.0 1.3 1.0 1.4 1.5 1.6666666666666667 2.0 0.0 0 0.0 1 5 9 21	apiserver pod is not able to resolve internal DNS : Name or service not known . Run minikube , then create a pod ( using apiserver's image ) , then run : : kubectl exec -it test-pod -- sh -c ' ping kubernetes ' . The output correctly says : : PING kubernetes . default . svc . cluster . local ( 10.96.0.1 ) 56(84 ) bytes of data . . When trying to do the same thing on the api server : : kubectl exec -it kube-apiserver-minikube -- namespace kube-system -- /bin/sh -c ' ping kubernetes ' . We get : : ping : kubernetes : Name or service not known . This happens because the : resolv . conf . in the apiserver pod is not pointing to the DNS service . Why is this the case ? Admission controllers ( webhook servers ) don't work if the apiserver can't resolve services .	2	0
1 1 0.8 1.0 0.7 0.5 1.1 1.0 0.6666666666666666 1.0 0.9834710743801653 121 1.0 0 3 12 25	-- extra-config controller-manager options are ignored ( masked by leader-elect ) . We have a regression bug in our kubeadm YAML generation , seemingly caused by #8431 where we set leader-elect = false . We end up writing two : controllerManager . section	0	0
1 1 1.4 2.0 1.1 1.0 1.3 1.0 1.0 1.0 0.0 0 0.0 2 4 15 46	Support DOCKER_HOST user override from within minikube pod container . Overview : My team and I are currently looking for a way to launch minikube instances on Kubernetes pods using the DinD as a sidecar method for CI related purposes . We're having issues getting minikube running in one pod container to communicate with a Docker daemon running in another pod container ( and listening at 0.0.0.0 : 2375 ) . Seems as though the : DOCKER_HOST . envvar is not being respected by minikube nor is the -- : docker-opt [ = -H tcp :/ /localhost : 2375 ] . cmd-flag . System Information : : # Linux , Fedora32 , Centos7 $ minikube version minikube version : v 1.9.2 commit : 93af9c1e43cab9618e301bc9fa720c63d5efa393 . Related Issue(s ): < URL > re : : unix :// /var/run/docker . sock . error despite supposed override - < URL > Steps to reproduce the issue : minikube start -- driver = docker && kubectl config use-context minikube helm install example buildkite/agent -- set agen t_t oken -- set dind . enabled = true -- set extraEnv='[{'name ' : ' DOCKER_HOST ' , ' value ' : ' localhost : 2375 ' }]' kubectl exec -it example-buildkite-agent- ... -- namespace example -- container agent -- { command-to-install-minikube } kubectl exec -it example-buildkite-agent- ... -- namespace example -- container agent -- minikube start -- driver = docker Full output of failed command : : Cannot connect to the Docker daemon at unix :// /var/run/docker . sock . Is the docker daemon running ? .  	1	1
2 2 0.8 1.0 1.2 1.5 1.2 1.5 1.0 1.0 1.1587301587301588 63 1.0 1 2 16 55	service list : info target port not being populated . : | -----------|---------------- | -------------|-------------------------- | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|---------------- | -------------|-------------------------- | | default | hello-minikube | | < URL > | | -----------|---------------- | -------------|-------------------------- | . in this case the target port is 8080 from < URL >	0	0
2 2 1.2 1.0 1.2 1.0 1.0 1.0 1.6666666666666667 2.0 0.8446601941747572 206 1.0 5 6 19 59	  site : Add tutorial for using -- user . we have a flag called -- user but most ppl dont know what it is meant for and how to use it we could add a tutorial how to use this flag < URL >	0	2
2 2 1.4 2.0 1.4 2.0 1.35 1.5 1.6666666666666667 2.0 0.0 0 0.0 2 4 6 24	hyperv : Unable to start VM : start : exit status 1 . I have created a new virtual switch named as:' Amit minikube ' now , when I'm trying to start that using the below command i get : PS C : /WINDOWS/system32 > minikube start -- vm-driver = hyperv -- hyper v-v irtual-switch ' Amit minikube ' o minikube v 0.34.1 on windows ( amd64 ) i Tip : Use ' minikube start -p ' to create a new cluster , or ' minikube delete ' to delete this one . : Restarting existing hyperv VM for ' minikube ' ... ! Unable to start VM : start : exit status 1 Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL > Any comments from anyone details required from my end ?	2	0
1 1 1.4 1.0 1.3 1.5 1.2 1.5 1.3333333333333333 1.0 0.0 0 0.0 1 7 9 38	Command start should delete node with empty ip or retrieve one . Steps to reproduce the issue : Open config . json , run : minikube node add minikube-m02 . Interrupt the add command after the node info presented in config . json but before : ip . is set Run : minikube stop && minikube start . , the node couldn't start	1	0
0 0 0.2 0.0 0.8 0.5 0.95 1.0 0.3333333333333333 0.0 1.2916666666666667 24 1.0 1 1 10 26	none driver can't be used with non-Docker runtimes ( looks for ' docker ' executable ) . There is something wrong in the runtime detection of the precreate : : $ sudo minikube start -- vm-driver = none -- container-runtime = cri-o 棣冩 minikube v 1.4.0 on Ubuntu 16.04 棣冦仚 Running on localhost ( CPUs = 4 , Memory = 7800MB , Disk = 138379MB ) ... 棣冩敡 Retriable failure : create : precreate : exec : ' docker ' : executable file not found in $PATH 棣冦仚 Running on localhost ( CPUs = 4 , Memory = 7800MB , Disk = 138379MB ) ... 棣冩敡 Retriable failure : create : precreate : exec : ' docker ' : executable file not found in $PATH 棣冦仚 Running on localhost ( CPUs = 4 , Memory = 7800MB , Disk = 138379MB ) ... 棣冩敡 Retriable failure : create : precreate : exec : ' docker ' : executable file not found in $PATH ^C . For some reason it is calling the wrong : Available . function ? : func ( r * Docker ) Available () error { _ , err : = exec . LookPath('docker ' ) return err } . : func ( r * CRIO ) Available () error { return r . Runner . Run('command -v crio ' ) } . And the docker runtime seems to be checking locally ? ( I guess us using docker machine makes it always there ) It also forgot to look for : crictl . , but that is another story . ( and interesting how this is regarded as a ' retriable failure ' )	2	0
2 2 1.2 2.0 1.2 2.0 1.15 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 0 1 5	Make the VM IP settable . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Perhaps Both Please provide the following details : Environment : Windows 10 or Mac High Sierra Minikube version ( use : minikube version . ): ALL - Install tools : choco or homebrew What happened : Minikube ip changes after stopping and starting it . This is inconsistent as sometimes it doesn't change What you expected to happen : Minikube ip to be configurable How to reproduce it ( as minimally and precisely as possible ): minikube start minikube ip minikube delete minikube start minikube ip As a feature request , I would appreciate it if there was a flag that could specify that IP when you run minikube start .	2	1
0 0 1.0 1.0 1.4 2.0 1.15 1.0 1.3333333333333333 2.0 1.096774193548387 62 1.0 1 1 5 18	The release build is reported as modified (' dirty ' ) . There was some kind of regression to the release process in 1.12.1 : : minikube version : v 1.12.0 commit : c83e6c47124b71190e138dbc687d2556d31488d6 minikube version : v 1.12.1 commit : 5664228288552de9f3a446ea4f51c6f29bbdd0e0-dirty minikube version : v 1.12.2 commit : be7c19d391302656d27f1f213657d925c4e1cfc2-dirty minikube version : v 1.12.3 commit : 2243b4b97c131e3244c5f014faedca0d846599f5-dirty . The only visible change in ' hack ' is the go version upgrade in 39a4268317e17dd3ed07a4484be7c652fa2dea2c : @@ -31 , 7 +31 , 7 @@ export KUBECONFIG='${TEST_HOME}/kubeconfig ' export PATH=$PATH:'/usr/local/bin/:/usr/local/go/bin/:$GOPATH/bin ' # installing golang so we could do go get for gopogh -sudo . /installers/check_install_golang . sh ' 1.13.9 ' ' /usr/local ' || true +sudo . /installers/check_install_golang . sh ' 1.14.4 ' ' /usr/local ' || true docker rm -f -v $(docker ps -aq ) > /dev/null 2 > & 1 || true docker volume prune -f || true . So something else is changing the source code during building , changing status : : Makefile : COMMIT ? = $(if $(shell git status -- porcelain -- untracked-files = no ) , ' ${COMMIT_NO}-dirty ' , ' ${COMMIT_NO }') .	0	0
2 2 0.8 0.0 1.0 1.0 0.9 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 3 5 20	Windows installer : /SD ( completely silent mode ) doesn't work . Download < URL > Run with silent parameters < URL > Expected : No message box . Actual : Message box is shown UPDATE : : /S . mode seems to work , but not : /SD . . So seems to be simple fix .	2	0
0 0 0.4 0.0 0.4 0.0 0.65 0.0 0.6666666666666666 0.0 1.2033898305084745 59 1.0 3 7 18 51	kic : minikube tunnel on mac should ask to enter ' yes ' . the tunnel on mac works except when you run you have enter yes for ssh verification : $ . /out/minikube tunnel -- alsologtostderr I0224 13:34:12 . 822427 4894 tunnel . go : 63 ] Creating docker machine client ... I0224 13:34:12 . 822455 4894 tunnel . go : 68 ] Creating k8s client ... starting tunnel for hello-minikube1- 10.111.6.26 -8080 I0224 13:34:56 . 248116 4894 loadbalancer_patcher . go : 118 ] Patched hello-minikube1 with IP 127.0.0.1 The authenticity of host ' [ 127.0.0.1 ]: 32782 ([ 127.0.0.1 ]: 32782 )' can't be established . ECDSA key fingerprint is SHA256 : /tCaIwGlH33qIVkBp+2TRCdiHDu/isjLGZ8q9kWI08c . Are you sure you want to continue connecting ( yes/no ) ? yes . cc : @josedonizetti	0	0
2 2 1.2 1.0 0.9 1.0 0.8 0.5 1.6666666666666667 2.0 1.125 16 1.0 1 1 3 24	pkg/minikube : race detected during execution of test . See < URL > : --- FAIL : TestCreateSSHShell ( 0.29 s ) testing . go : 809 : race detected during execution of test . : --- FAIL : TestGetServiceURLsForService ( 0.00 s ) --- FAIL : TestGetServiceURLsForService/correctly_return_serviceURLs ( 0.00 s ) testing . go : 809 : race detected during execution of test --- FAIL : TestGetServiceURLsForService/no_host ( 0.00 s ) testing . go : 809 : race detected during execution of test --- FAIL : TestGetServiceURLsForService/correctly_return_empty_serviceURLs ( 0.00 s ) testing . go : 809 : race detected during execution of test .	2	0
1 1 1.8 2.0 1.4 1.5 1.35 1.5 1.6666666666666667 2.0 1.4285714285714286 7 2.0 0 0 4 20	delete should be able to delete the ` . minikube` folder ( w/ a flag ) . From < URL > : minikube delete . should have a switch/flag to delete the : . minikube . folder as well . This should ask for confirmation and display a warning before proceeding ahead . /assign	2	1
2 2 1.6 2.0 1.1 1.0 1.2 1.0 1.6666666666666667 2.0 0.9672131147540983 61 1.0 4 7 9 28	Reduce VM CPU overhead by 20% . Related to #3207 and #5398 3207 can probably be closed - I don't believe we are at 50% usage any longer .  	1	1
2 2 1.0 1.0 1.1 1.0 1.2 1.0 1.3333333333333333 1.0 0.9772727272727273 44 1.0 3 3 5 26	Allow ISO's to be downloaded from GitHub . The ISO should live alongside our binaries on the release page .   	1	1
2 2 1.2 1.0 0.8 0.5 1.15 1.0 1.6666666666666667 2.0 0.9841269841269841 63 1.0 6 9 11 30	Graphical User Interface - Menubar UI Prototype . The exact command to reproduce the issue : The full output of the command that failed : The output of the : minikube logs . command : The operating system version :  	1	1
0 0 1.6 2.0 1.6 2.0 1.45 2.0 1.3333333333333333 2.0 0.0 0 0.0 1 4 7 40	Setting node name via kubeadm extra config leads to a crash . I tried setting the k8s node name with : minikube start -- extra-config = kubeadm . node-name = foo . , but minikube crashed with a timeout . The kubelet logs contain : node ' minikube ' not found . , which is probably due to this line in : pkg/minikube/bootstrapper/kubeadm/versions . go . : : NewUnversionedOption(Kubelet , ' hostname-override ' , constants . DefaultNodeName ) , . I was able to make it work with : : minikube start -- extra-config = kubeadm . node-name = foo -- extra-config = kubelet . hostname-override = foo . This took me by surprise , since I was expecting the : kubeadm . setting to affect the : kubelet . one in absence of any explicit settings for the latter . Maybe this just needs some extra docs , since the above workaround is available . Or perhaps a top-level : -- node-name . option for : minikube start . that ensures a consistent handling of the flag in the various components .	2	0
2 2 1.0 1.0 1.3 2.0 1.4 2.0 1.3333333333333333 2.0 0.8450704225352113 213 1.0 5 5 9 56	implement minikube image rm `IMG_NAME` . we need to add a feature for users to easily delete an image from minikube maybe : minikube image rm IMG_NAME .	0	1
2 2 1.0 1.0 0.9 0.5 1.15 1.5 1.0 1.0 0.0 0 0.0 0 0 1 20	Driver auto-detection can hang minikube . Minikube versions in 1.5 . x all work properly and proceed properly after running : minikube start -- vm-driver = virtualbox . but for each version of 1.6 . x that I've tried , everything hangs . The logs seem to indicate that a : config . json . is missing , but from advice I've seen , this should be created upon : minikube start . and should not be manually added . I've tried switching back and forth between versions and each time the 1.5.2 works properly , but 1.6.0 / 1.6.2 always hangs ( I've left it for 20+ minutes ) . I am running with a ryzen CPU , but not sure if this is related to #6168 as the examples mentioned there seem to get further in the process after : minikube start . than mine does . The exact command to reproduce the issue : : minikube start -- vm-driver = virtualbox . The full output of the command that failed : : $ minikube start -- vm-driver = virtualbox 棣冨竴 minikube 1.6.2 is available ! Download it : < URL > 棣冩寱 To disable this notice , run : ' minikube config set WantUpdateNotification false ' 棣冩 minikube v 1.6.0 on Ubuntu 19.10 . The output of the : minikube logs . command : : $ minikube logs 棣冩寴 Error getting config : stat /home/myuser/ . minikube/profiles/minikube/config . json : no such file or directory 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > . The operating system version : Ubuntu 19.10	0	0
0 0 0.4 0.0 0.8 0.0 0.75 0.0 0.6666666666666666 0.0 0.9421965317919075 173 1.0 0 1 1 22	new line for error after spinner . i intentionally renamed the kube-context to experiment an error but the error is missing new line : $ kc config use-context minikube-changed Switched to context ' minikube-changed ' . $ kc get pods No resources found in default namespace . $ minikube start 棣冩 minikube v 1.16.0 on Darwin 10.15.7 閴?Using the docker driver based on existing profile 棣冩啢 Starting control plane node minikube in cluster minikube 棣冨籍 Updating the running docker ' minikube ' container ... 棣冩儞 Preparing Kubernetes v 1.20.0 on Docker 20.10.0 ... / 棣冦亞 Unable to restart cluster , will reset it : getting k8s client : client config : client config : context ' minikube ' does not exist 閳?Generating certificates and keys ... 閳?Booting up control plane ... 閳?Configuring RBAC rules ... 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : storage-provisioner , default-storageclass 棣冨及 Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default medya@ ~ /workspace/minikube ( baremtal_ghaction ) $ kc get pods No resources found in default namespace . medya@ ~ /workspace/minikube ( baremtal_ghaction ) $ kc config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-desktop docker-desktop docker-desktop * minikube minikube minikube default minikube-changed minikube minikube default .  	1	0
2 2 1.4 2.0 1.5 2.0 1.35 2.0 1.3333333333333333 2.0 0.0 1 0.0 0 2 3 19	Pausing apiserver and then running minikube status reports stopped apiserver . Steps to reproduce the issue : minikube start minikube pause minikube status MInikube then hangs .  	1	0
2 2 1.4 2.0 1.2 1.5 1.1 1.0 1.6666666666666667 2.0 0.9420289855072463 138 1.0 3 3 7 55	-- download-only should store/load kicbase on local filesystem . To ensure minikube can startup in an air-gapped environment . Related issue : #10149	0	1
1 1 1.2 1.0 1.1 1.0 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 1 20	Enable autostart when starting . Consider adding a feature to enable autostart for minikube instances For example : : minikube start -p hellokube -- autostart = true . This will autostart the instance when the host machine boots up / logs in . This will give a closer experience to how docker for desktop works . If additional configuration needs to be done fore each hypervisor then that is done as a one time setup the first time the configuration is used if it is not already done . For example , if this is on virtualbox then the feature will also auto configure virtualbox plist configuration .	2	1
0 0 0.6 0.0 0.7 0.0 0.75 0.5 0.3333333333333333 0.0 1.0357142857142858 112 1.0 1 3 7 79	 -- base-image ' should respect ' -- image-repostory ' .  	1	0
1 1 1.4 2.0 1.3 2.0 1.15 1.5 1.6666666666666667 2.0 2.0 1 2.0 1 1 2 21	Auto install helm . Consider adding a feature to auto install helm . For example : : minikube start -p hellokube -- with-helm . Helm is becoming a tool that is more and more used . Having to remember install it after each time a new minikube instance is launched is pretty annoying . Its like having to install composer after installing php or installing npm after installing node . While I can agree that not everyone uses helm it is used often enough to justify an option to auto install it .	2	1
2 2 1.4 2.0 1.1 1.0 0.85 1.0 1.3333333333333333 2.0 2.0 1 2.0 2 8 14 47	act_gact kernel module missing for network packet control . The : act_gact . module is missing . Aside from general networking , : act_gact . is used by Chaos Engineering tools such as < URL > to simulate network packet loss .. This is a similar use case to #6033 and #6271 . The full output of the command that failed : : $ modprobe -n act_gact modprobe : FATAL : Module act_gact not found in directory /lib/modules/ 4.19.81 . The operating system version : minikube version : v 1.7.2 commit : 50d543b5fcb0e1c0d7c27b1398a9a9790df09dfb	2	1
1 1 1.2 1.0 1.0 1.0 0.85 1.0 1.3333333333333333 1.0 1.2285714285714286 35 1.0 1 8 18 52	Running minikube on IBM ( s390x , ppc64le ) , use cases . There's been some requests for minikube binaries for kubernetes IBM Z/POWER platforms : < URL > ( : s390x . ) < URL > ( : ppc64le . ) It would be nice with some more background information , how minikube is used there ... So far the main use case is running the : none . driver in CI , for installing : kubeadm . . There is not really any development desktop support , and no available ISO or KIC .    	1	2
2 2 1.6 2.0 1.7 2.0 1.2 1.5 1.3333333333333333 2.0 1.1454545454545455 55 1.0 3 7 13 49	Upgrade Docker to 19.03.11 . Also containerd 1.2.13 and runc 1.0.0 -rc10 EDIT : These were already the current versions	0	1
1 1 1.4 2.0 1.4 1.5 1.3 1.0 1.0 1.0 1.2333333333333334 30 1.0 0 0 13 24	Investigate compression methods on the minikube . iso . The compression method to use is a trade-off between size and speed ... Size testing with Buildroot 2019.02 : : 498M rootfs . cpio 191M rootfs . cpio . lz4 165M rootfs . cpio . gz 149M rootfs . cpio . bz2 89M rootfs . cpio . xz . Speed findings from Ubuntu 19.10 : : 940m BZIP2 705m XZ 239m LZO 235m GZIP 184m LZ4 . We could investigate changing to a slightly bigger ISO , that boots faster .  	1	1
2 2 1.6 2.0 1.3 1.5 1.25 2.0 2.0 2.0 0.6153846153846154 13 0.0 2 3 15 55	podman : load kic base image from cache if available for offline mode . This should be easier to implement once < URL > is merged  	1	1
1 1 1.0 1.0 1.1 1.0 1.2 1.0 0.6666666666666666 1.0 0.6 5 0.0 3 4 8 38	docker env in multinode . We should support this properly  	1	1
1 1 1.0 1.0 0.9 1.0 1.0 1.0 1.0 1.0 2.0 1 2.0 0 1 6 23	9p2000 . L mount breaks symbolic link creation : Operation not permitted . Minikube version : v 1.2.0 Environment : OS : MacOS v 10.15.5 VM Driver : hyperkit Docker version ( e.g. docker -v ): 18.09.2 , build 6247962 Install tools : brew What happened : I received an error ' ln : failed to create symbolic link ' /test/bar ' : Operation not permitted ' What you expected to happen : Create a successful symbolic link How to reproduce it ( as minimally and precisely as possible ): : cd $HOME touch foo minikube start -- vm-driver hyperkit -- mount -- mount-string $HOME : /test minikube ssh -- ln -s /test/foo /test/bar . Anything else do we need to know : This is important to me because I try and do things like npm install inside the container ( for local development ) , and I'll get errors about it not being able to make symbolic links . This reopens issue #890 This is happening with the Hyperkit driver but I suspect may happen with other drivers as well .	2	0
2 2 1.4 2.0 1.5 2.0 1.3 2.0 1.6666666666666667 2.0 1.087378640776699 103 1.0 4 12 24 77	investigate if we can install docker in KicImage without bind to containerd . in VM world we install docker service without running containerd . in my PR i noticed our containerd-runtime is 22 seconds faster than the docker runtime which could be because our docker runtime only in KIC it runs TWO services ( docker and containerd ) and our containerd runtime only runs ( containerd service ) can we make our kic image be like our VM ?  	1	1
0 0 1.4 2.0 1.2 1.0 1.4 1.5 1.3333333333333333 2.0 1.2258064516129032 31 1.0 0 1 13 23	Add support for using the podman service within minikube . Currently we offer support for using the docker daemon remotely , through : docker-env . . < URL > It is possible to offer the same kind of support for podman-remote , using : podman-env . . < URL > This uses the < URL > protocol over ssh , in order to start a podman socket on the VM . Unlike Docker , there is no need to start any long-running daemon before connecting ... Currently the only supported podman commands are : : load . , : save . , : build . , : images . There are currently some issues with running them , so use Kubernetes for that instead . Any images built are immediately available for the CRI-O runtime , for rapid turnaround . When issuing the : podman-remote build . command , the build context is sent to the VM .	0	1
2 2 1.6 2.0 1.6 2.0 1.4 2.0 1.6666666666666667 2.0 1.1518987341772151 79 1.0 2 4 4 22	Add support for buildkit , for building to containerd . Currently we support either ' docker ' or ' podman ' for building , either locally on the VM or remotely through the ' env ' commands . < URL > When using the containerd container runtime , we currently don't have any support for building images - only for loading them ... We could include : buildkitd . , in order to provide that ? It is used in the same way as docker or podman ( through ssh ) , but with the : buildctl . client . Similar to when using containerd , the syntax is somewhat more low-level than the other tools : : buildctl build -- frontend = dockerfile . v0 -- local context = . -- local dockerfile = . . < URL >  	1	1
0 0 0.2 0.0 0.8 0.5 0.9 1.0 0.0 0.0 0.0 0 0.0 0 1 2 28	registry-mirror : Job for docker . service failed because the control process exited with error code . sudo minikube start -- registry-mirror registry.cn-hangzhou.aliyuncs.com/google_containers -- vm-driver = virtualbox Password : 棣冩 minikube v 1.0.1 on darwin ( amd64 ) 棣冦仚 Downloading Kubernetes v 1.14.1 images in the background ... 棣冩暉 Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... 棣冩懕 ' minikube ' IP address is 192.168.99.100 棣冩儞 Configuring Docker as the container runtime ... 棣冩寴 Failed to enable container runtime : command failed : sudo systemctl start docker stdout : stderr : Job for docker . service failed because the control process exited with error code . See ' systemctl status docker . service ' and ' journalctl -xe ' for details . : Process exited with status 1 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL >	2	0
2 2 1.4 2.0 1.0 1.0 0.95 1.0 1.3333333333333333 2.0 1.26 50 1.0 2 2 16 46	kic : create container should verify it is running . after we create a container we need to verify the status is ' running ' as opposed to ' starting ' before we proceed to next commands . this happens rarely but on podman I saw this : : Error : cannot exec into container that is not running : container state improper : exit status 126 . which means we tried too early to run commands inside the container .  	1	1
1 1 1.4 2.0 1.3 2.0 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 1 2 4 21	StartHost timeout should be configurable . < URL > mentions everything . My actual use case is on Hyper-V , when creating a disk image for 128 GBs takes more than 240 seconds ( the hardcoded timeout ) , and thus the whole : minikube start . fails . I guess , this might be two things One is to allow configuring the timeout . Second is figuring out why does the creation take so long , anyway . During the creation of the VM , it seems like the disk first gets created , then it gets converted . Not sure why would either of these operations take long time , since I am on NVMe PCIe3 SSD , and even filling the file with zeroes should be a matter of several seconds . I think if we have configurabl timeout , then I can live with this . BUt if there's a way to make the creationg faster ( Well withing the hard coded timeout ) , that would work too .  	1	1
0 0 1.0 1.0 1.1 1.0 1.15 1.0 1.3333333333333333 2.0 1.032520325203252 123 1.0 0 4 15 53	UI : upgrade k8s version message gets repeated 4 times . as seen here in this issue < URL > : Starting minikube 棣冩 minikube v 1.11.0 on Ubuntu 18.04 閴?Using the docker driver based on existing profile 棣冨晭 Kubernetes 1.18.3 is now available . If you would like to upgrade , specify : -- kubernetes-version = v 1.18.3 棣冨晭 Kubernetes 1.18.3 is now available . If you would like to upgrade , specify : -- kubernetes-version = v 1.18.3 棣冨晭 Kubernetes 1.18.3 is now available . If you would like to upgrade , specify : -- kubernetes-version = v 1.18.3 棣冨晭 Kubernetes 1.18.3 is now available . If you would like to upgrade , specify : -- kubernetes-version = v 1.18.3 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩敡 Restarting existing docker container for ' minikube ' ... 棣冩儞 Preparing Kubernetes v 1.17.5 on Docker 19.03.2 ... 閳?kubeadm . pod-network-cidr = 10.244.0.0 /16 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : default-storageclass , storage-provisioner 棣冩寴 failed to start node : startup failed : Wait failed : waiting for apps_running : checking k8s-apps to be running : timed out waiting for the condition 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > .	0	0
1 1 1.8 2.0 1.7 2.0 1.35 1.0 1.6666666666666667 2.0 0.8860103626943006 193 1.0 0 2 8 46	auto-pause : make time to pause configurable .  	1	1
0 0 1.0 1.0 1.2 1.5 1.25 1.5 1.0 1.0 0.0 0 0.0 0 0 1 16	Publish arm64 image for gcr.io/k8s-minikube/storage-provisioner . The exact command to reproduce the issue : : minikube start -- vm-driver = none -- kubernetes-version = v 1.16.2 . on an ARM machine The full output of the command that failed : The : storage-provisioner . pod is falied to start . The output of the : minikube logs . command : : Back-off pulling image ' gcr.io/k8s-minikube/storage-provisioner-arm64:v1.8.1 ' . The operating system version : Ubuntu 18.04 for aarch64	2	0
2 2 1.2 1.0 0.9 1.0 1.05 1.0 1.3333333333333333 2.0 1.2553191489361701 47 1.0 4 9 11 42	kic status : warn user if docker is too slow ! . while debugging minikube with docker driver on a slow windows machine I noticed we Error to user that docker is not ready . but the real issue was , the user had a lot of other docker containers running and their docker was super slow ! we could probably warn user that their docker is slow with a link to website how to make it faster !	0	1
1 1 0.8 1.0 0.6 0.0 0.6 0.0 0.3333333333333333 0.0 1.0 133 1.0 0 3 5 21	Don't spit out glog . Errorf in json output . I tried the new status command with json , and I saw a glog . Errorf $ minikube status -l = cluster -o = json | jq : E0806 13:41:23 . 881611 26067 status . go : 334 ] kubeconfig endpoint : got : 127.0.0.1 : 32828 , want : 127.0.0.1 : 32832 { ' Name ' : ' minikube ' , ' StatusCode ' : 100 , ' StatusName ' : ' Starting ' , ' Step ' : ' Preparing Kubernetes ' , ' StepDetail ' : ' * Preparing Kubernetes v 1.18.3 on Docker 19.03.8 ... ' , ' BinaryVersion ' : ' v 1.12.2 ' , ' Components ' : { ' kubeconfig ' : { ' Name ' : ' kubeconfig ' , ' StatusCode ' : 500 , ' StatusName ' : ' } } , ' Nodes ' : [ { ' Name ' : ' minikube ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' , ' Components ' : { ' apiserver ' : { ' Name ' : ' apiserver ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' } , ' kubelet ' : { ' Name ' : ' kubelet ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' } } } ] } .	0	0
1 1 1.2 1.0 1.1 1.0 1.05 1.0 1.6666666666666667 2.0 0.8215767634854771 241 1.0 1 1 3 13	  add FAQ how to turn off kubernetes part of minikube ( to use it just for docker ) . that would be ' minikube pause '	0	2
0 0 1.0 1.0 1.2 1.5 1.15 1.5 0.3333333333333333 0.0 0.75 4 0.5 1 2 4 25	Flake rate reporting not commenting . Flake rates are not being commented about .  	1	0
0 0 1.0 1.0 0.7 0.5 0.95 1.0 0.6666666666666666 1.0 0.0 0 0.0 3 5 11 54	  docker driver page missing usage instructions . < URL > lacks a number of sections found in , e.g. , < URL > particularly the Usage section .	0	2
0 0 0.6 0.0 0.8 0.5 0.9 0.5 0.6666666666666666 0.0 0.5555555555555556 18 0.0 0 0 1 16	Using ` ~ ` in ` -- ssh-key` causes `SSH key does not exist` . : $minikube start ... -- ssh-key =~/ . ssh/id_rsa ... 棣冦亞 StartHost failed , but will try again : creating host : create : precreate : SSH key does not exist : ' ~ / . ssh/id_rsa ' . The weird quirk is this only occurs if you use an : = . between the key pair , if you use a space your terminal will rewrite to the absolute path . : $ echo ~ / . ssh /Users/user/ . ssh $ echo -- ssh-key =~/ . ssh -- ssh-key =~/ . ssh . We should update the ssh-key value with the absolute path if we detect a : ~ . .	0	0
1 1 0.8 1.0 1.2 1.0 1.05 1.0 0.6666666666666666 1.0 0.0 0 0.0 1 1 2 5	macOS mount : mounted directory is empty . Is this a BUG REPORT or FEATURE REQUEST ? Bug report Please provide the following details : Environment : Minikube version : v 0.25.0 - OS : macOS 10.13.2 - VM Driver : hyperkit - ISO version : v 0.25.1 - Others : kubernetes v 1.7.5 What happened : Tried to mount a host directory into a container . The directory inside the container is empty . What you expected to happen : The directory inside the container should give access to the mounted directory on the host . At least , that was the behavior when I was using minikube and VirtualBox ! How to reproduce it : : $ eval $(minikube docker-env ) $ mkdir test ; cd test ; echo ' hello ' > tes t.t xt $ docker run -- rm -it -v $(pwd ): /testmount alpine ( now inside alpoine container ) / # cd testmount /testmount # ls -l total 0 . Anything else do we need to know : Is this not possible when using hyperkit ? If not , what is the alternative ? I noticed some options relating to NFS , but I didn't dig too deeply . I'd prefer to just use the standard Docker volume mount mechanism if possible . Thank you !  	1	0
1 1 1.2 1.0 1.0 1.0 1.1 1.0 0.6666666666666666 1.0 0.0 0 0.0 3 4 6 27	Respect the XDG Base Directory Specification . Under Linux/Unix systems , there is the < URL > that states where applications should store user-related data . For example , the : config . json . currently resides in : . minikube/config/config . json . , but it should be in : $XDG_CONFIG_HOME/minikube/config . json . ( which would be : . config/minikube/config . json . most of the time ) .	2	1
1 1 1.2 1.0 1.1 1.0 1.15 1.0 1.0 1.0 1.1044776119402986 67 1.0 2 6 9 32	Build ISO minikube image for ARM ( aarch64 ) . If we want to run any hypervisor driver on arm64 , we need a new ' minikube . iso ' that works on the architecture . Currently the image is full of amd64 binaries that we download from other places , so it does not build from source . Buildroot does support the ARM architectures ( armv7 , arm64 ) . For instance the Raspberry Pi OS still uses 32-bit by default ...  	1	1
2 2 1.6 2.0 1.2 1.5 0.9 1.0 1.6666666666666667 2.0 0.0 0 0.0 2 2 4 19	Unable to pull `dashboard` & `metrics-scraper` images from ` registry.cn-hangzhou.aliyuncs.com/google_containers` . Steps to reproduce the issue : 1 . I executed minikube start successfully to start minikube , but it failed to start minikube dashboard : Exiting due to SVC_URL_TIMEOUT : < URL > is not accessible : Temporary Error : unexpected response code : 503 . Run : minikube logs -- file = logs . txt . and drag and drop the log file into this issue Full output of failed command if not : minikube start . : The following is the log record of : minikube dashboard . [ minikube_dashboard_1989b4f49854eaf6b3ed7671224c8f15e3a568a9_0 . log ]( < URL > [ minikube_dashboard_27014b127e5c6c9c95792ad751b8db0a7d7a54aa_0 . log ]( < URL > [ minikube_dashboard_0e027cdabd32d18e1757ed354235ff3cd1e2bdbc_0 . log ]( < URL >	1	0
0 0 1.0 1.0 1.0 1.0 1.1 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 3 6 39	Improve error handling for ' VirtualBox won't boot a 64bits VM when Hyper-V is activated ' . Shows the error bellow : ! Unable to start VM : create : precreate : This computer is running Hyper-V . VirtualBox won't boot a 64bits VM when Hyper-V is activated . Either use Hyper-V as a driver , or disable the Hyper-V hypervisor . ( To skip this check , use -- virtualbox-no-vtx-check )	2	0
0 0 0.0 0.0 0.4 0.0 0.75 0.5 0.0 0.0 0.0 0 0.0 3 3 4 15	Check ErrInsufficientDockerStorage before preload completed . Check : pErr = = oci . ErrInsufficientDockerStorage . before : waitForPreload . Wait () . < URL > < URL >  	1	0
2 2 1.2 1.0 1.1 1.0 0.9 1.0 1.0 1.0 1.5 2 1.5 0 2 5 37	Addon to configure internal registry aliases . Based on the discussion on #4604 , we need an addon that can configure the registry aliases . The addon will do the following : Deploy a ConfigMap that can define the aliases Deploy a Daemonset that can update the minikube node /etc/hosts Deploy a Job that can patch the coreDNS for aliases rewrite rules SA and ClusterRoleBinding to run the Job  	1	1
2 2 1.6 2.0 1.4 1.5 1.45 2.0 1.6666666666666667 2.0 0.0 5 0.0 1 6 11 50	Remove ' use PORT instead of 5000 ' message when disabling registry addon . Steps to reproduce : 1 . : minikube start -- driver = docker . 2 . : minikube addons enable registry . 3 . : minikube addons disable registry . Output may include ( not always ) the following message : : 棣冩寱 Registry addon on with docker uses 56124 please use that instead of default 5000 . This should only be shown on enable , not disable .	2	1
0 0 1.2 2.0 1.1 1.5 1.2 1.0 1.3333333333333333 2.0 0.5 4 0.0 1 3 11 49	CoreDNS : Unable to scale down deployment ' coredns ' in namespace ' kube-system ' to 1 replica . since we've moved closer to the point when coredns is deployed , there could be a race condition that needs to be handled and operation retried full error : : Unable to scale down deployment ' coredns ' in namespace ' kube-system ' to 1 replica : deployment rescale : Operation cannot be fulfilled on deployments . apps ' coredns ' : the object has been modified ; please apply your changes to the latest version and try again . /assign	0	0
0 0 0.8 0.0 0.9 0.5 0.9 1.0 0.0 0.0 0.0 0 0.0 0 2 2 16	Feature request : command to recreate certificates . While developing I need to work out of the office as well . The problem is that when I connect to the different network , the hyper-v minikube machine get a different address that can not ( easily ) be fixed . The problem after that is that when I run the : kubectl . commands I get the error : : Unable to connect to the server : x509 : certificate is valid for 192.168 . XXX . YYY , 10.96.0.1 , 10.0.0.1 , not 192.168 . XXX . ZZZ . Currently the only work around is to stop the minikube machine and restart it again so it can go through the recreation of the certificates . That also requires that all my deployments have to restart to , what is undesirable . I know of the : -- apiserver-ips . option but that only seems to work when you know what ip address the vm will be assigned . The solution would be a command like : minikube renew-certs . or something similar .	2	1
1 1 1.2 1.0 1.2 1.5 1.1 1.0 1.0 1.0 1.0 34 1.0 0 2 10 28	minikube should warn about out-of-date VM's . Occassionally , using minikube from an old VM can cause issues . For instance , an unexpectedly old version of Docker . We should detect if the image version is out of date , and warn appropriately . Two ideas : older than the major release of minikube older than X days	2	1
2 2 1.4 1.0 1.4 1.5 1.15 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 3 17	change registery_mirror to registery-mirror . in this line : : registryMirror = viper . GetStringSlice('registry_mirror ' ) . the flag string should be : registry-mirror . < URL > It is initially defined as : startCmd . Flags () . StringSliceVar(搴恑stryMirror , ' registry-mirror ' , nil , ' Registry mirrors to pass to the Docker daemon ' ) . < URL >	0	0
2 2 1.2 1.0 1.4 2.0 1.2 1.5 1.0 1.0 0.8318181818181818 220 1.0 1 4 10 34	`minikube cp` should support copying from minkube to user's host . currently minikube cp does NOT support doing this . : minikube cp minikube : /var/lib/minikube/certs/etcd/peer . crt /tmp . current workaround by @tstromberg : minikube ssh ' cat /var/lib/minikube/certs/etcd/peer . crt ' > /tmp/peer . cert .  	1	1
2 2 1.2 2.0 1.2 1.5 1.05 1.0 1.3333333333333333 2.0 0.9407407407407408 135 1.0 3 13 17 42	minikube unable to recover from system prune : DRV_CP_ENDPOINT : failed to get API Server URL : failed to parse ip for ' . To reproduce : : minikube start -- driver = docker docker system prune -a minikube start -- driver = docker -- delete-on-failure = true . Output : : 棣冩 minikube v 1.17.1 on Debian rodete 閴?Using the docker driver based on existing profile 棣冩啢 Starting control plane node minikube in cluster minikube 棣冨籍 Updating the running docker ' minikube ' container ... 棣冩儞 Preparing Kubernetes v 1.20.2 on Docker 20.10.2 ... 閴?Exiting due to DRV_CP_ENDPOINT : failed to get API Server URL : failed to parse ip for ' 棣冩寱 Suggestion : Recreate the cluster by running : minikube delete -- profile = minikube minikube start -- profile = minikube . Expectation : minikube deletes the broken cluster and recreates it . Verbose logs : < URL >   	1	0
2 2 1.2 2.0 1.1 1.5 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 1 6 36	Documentation - mount - do mounts persist ? . This is related to the documentation for minikube . I'm trying to figure out if mounts persist across : minikube start/stop . . I'm automating the startup of a collection of deployments , and they need a local directory to be mounted . I understand that the : minikube mount . command accomplishes this . I can't find any documentation on removing mount points or on what happens to the mount points when you : minikube stop . . I'd like to return the minikube state back to the same as it was prior to executing this script , so I just need to know either : Do mount points disappear after you shutdown minikube ( and not come back when you start again ) Or , is there a way to remove a mount point after you add one ?  	2	2
0 0 1.4 2.0 1.5 2.0 1.45 2.0 1.3333333333333333 2.0 0.9916666666666667 120 1.0 0 0 1 18	gcp-auth plugin should support image pulls from gcr.io . As reported in #Slack and seen within two friction logs .	0	1
1 1 0.4 0.0 1.0 1.0 1.1 1.0 0.6666666666666666 1.0 1.1322314049586777 121 1.0 0 2 4 30	Repository and tag for CoreDNS changed in 1.8.0 ( Kubernetes 1.21 ) . : Error response from daemon : manifest for k8s.gcr.io/coredns:1.8.0 not found : manifest unknown : Failed to fetch ' 1.8.0 ' from request ' /v2/coredns/manifests/ 1.8.0 ' . . < URL > : --- a/cmd/kubeadm/app/constants/constants . gopkg/minikube/bootstrapper/images/images . go +++ b/cmd/kubeadm/app/constants/constants . go @@ -323 , 7 +323 , 7 @@ const ( CoreDNSDeploymentName = ' coredns ' // CoreDNSImageName specifies the name of the image for CoreDNS add-on - CoreDNSImageName = ' coredns ' + CoreDNSImageName = ' coredns/coredns ' // KubeDNSConfigMap specifies in what ConfigMap in the kube-system namespace the kube-dns config should be stored KubeDNSConfigMap = ' kube-dns ' @@ -344 , 7 +344 , 7 @@ const ( KubeDNSVersion = ' 1.14.13 ' // CoreDNSVersion is the version of CoreDNS to be deployed if it is used - CoreDNSVersion = ' 1.7.0 ' + CoreDNSVersion = ' v 1.8.0 ' // ClusterConfigurationKind is the string kind value for the ClusterConfiguration struct ClusterConfigurationKind = ' ClusterConfiguration ' . So this change needs to be reflected in the minikube code as well . : func coreDNS(v semver . Version , mirror string ) string .	0	0
2 2 1.6 2.0 1.3 2.0 1.15 1.0 2.0 2.0 0.0 0 0.0 0 2 5 29	Cannot disable default storage class . Steps to reproduce : 1 . Start minikube ( from scratch ) 2 . Disable addon : : > minikube addons disable default-storageclass 閴?' default-storageclass ' was successfully disabled . 3 . List storage classes : : > kubectl get storageclasses.storage.k8s.io NAME PROVISIONER AGE standard ( default ) k8s.io/minikube-hostpath 4m14s . Bug : : standard . storage class is still marked as default . Actually , the storage class is then annotated with both : storageclass.kubernetes.io/is-default-class : ' true ' . ( from the addon ) and : storageclass.beta.kubernetes.io/is-default-class : ' false ' . ( added with : minikube addons disable default-storageclass . ): : apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : annotations : storageclass.beta.kubernetes.io/is-default-class : ' false ' storageclass.kubernetes.io/is-default-class : ' true ' creationTimestamp : ' 2019-11-20T 12:40:29 Z ' labels : addonmanager.kubernetes.io/mode : EnsureExists name : standard resourceVersion : ' 600 ' selfLink : /apis/ storage.k8s.io/v1/storageclasses/standard uid : cdef4691-5dc5-42c5-8e02-db45e4b4265c provisioner : k8s.io/minikube-hostpath reclaimPolicy : Delete volumeBindingMode : Immediate . Minikube version : v 1.5.2 Kubernetes version : v 1.16.2  	1	0
1 1 1.2 1.0 1.3 1.5 1.3 1.5 1.3333333333333333 1.0 1.0 3 1.0 4 6 9 23	Create Node operations for multinode . : minikube node add minikube node start minikube node stop minikube node delete . Implement those 4 operations and all the config nonsense that goes along with it . Make sure to fix MachineConfig so we can refactor all the node specific stuff . ref #94	0	1
1 1 1.0 1.0 1.1 1.0 1.15 1.0 1.0 1.0 0.9802631578947368 152 1.0 3 4 6 25	implement ' -- log_file ' and ' -- log_dir ' for klog . I hardcoded this in our code to force log file to be to a specific file , : if err : = goflag . Set('log_file ' , ' medya_1 . txt ' ) ; err ! = nil { . and makes the log file to append to medya_1 . txt we just need to parse this from the flags however if I pass the same flag to the minikube binary it will not do anything ( without that line above ) : minikube status -- log_file = medya2 . txt . same with one dash : minikube status -log_file = medya2 . txt . ```	0	1
2 2 1.2 1.0 1.2 1.0 0.85 1.0 1.3333333333333333 1.0 0.9171270718232044 181 1.0 2 3 8 31	auto-pause prototype feature .	0	1
2 2 1.4 2.0 1.4 1.5 1.15 1.0 2.0 2.0 0.0 0 0.0 2 4 7 25	Unable to start minikube on Ubuntu 21.04 . Ubuntu cannot start with weird message today : :/ : + sudo -E minikube start -- vm-driver = none -- apiserver-ips 127.0.0.1 -- apiserver-name localhost -- kubernetes-version = stable -- extra-config = apiserver . enable-admission-plugins = NamespaceExists -- extra-config = kubelet . resolv-conf = /run/systemd/resolve/resolv . conf -- extra-config = kubelet . cgroup-driver = systemd -- addons = pod-security-policy -- container-runtime = containerd -- network-plugin = cilium 棣冩 minikube v 1.22.0 on Ubuntu 21.04 閳?MINIKUBE_WANTREPORTERRORPROMPT = false 閳?KUBECONFIG = /home/superstar/ . kube/config 閳?MINIKUBE_HOME = /home/mati 閳?MINIKUBE_WANTUPDATENOTIFICATION = false 閴?Using the none driver based on user configuration 閴?Using the ' containerd ' runtime with the ' none ' driver is an untested configuration ! 棣冩啢 Starting control plane node minikube in cluster minikube 棣冦仚 Running on localhost ( CPUs = 4 , Memory = 15872MB , Disk = 223813MB ) ... 閳╃櫢绗?OS release is Ubuntu 21.04 閴?Exiting due to GUEST_START : Invalid character(s ) found in prerelease ' 0ubuntu1 ~ 21 ' .	2	0
0 0 1.0 1.0 0.8 1.0 0.95 1.0 0.6666666666666666 0.0 1.0422535211267605 71 1.0 3 3 6 19	Support a start command that does not reconfigure the existing cluster . : minikube start . currently reconfigures a cluster . We should support an explicit command for configuration , and an explicit one for startup .   	1	1
2 2 1.4 2.0 1.3 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 2 12	sometimes starting an uncleanly stopped instance results /var/lib/docker mounted on rootfs . BUG REPORT Environment : Minikube version ( use : minikube version . ): v 0.29.0 - OS ( e.g. from /etc/os-release ): MacOSX - VM Driver : virtualbox - ISO version : v 0.29.0 What happened : The minikube VM was shutdown in an unclean way as my machine restarted . After that : minikube start . hangs at ' Starting cluster components ... ' . checked where : /var/lib/docker . was mounted and it was on rootfs . : $ cd /var/lib/docker $ df -h . Filesystem Size Used Avail Use% Mounted on rootfs 0 0 0 - / . What you expected to happen : Normally : /var/lib/docker . is mounted on : /dev/sda1 . : : $ cd /var/lib/docker $ df -h . Filesystem Size Used Avail Use% Mounted on /dev/sda1 17G 1.3 G 14G 9% /var/lib/docker . How to reproduce it ( as minimally and precisely as possible ): Not sure . It sometimes happens , and sometimes it doesn't but repro would be probably something like this : 1 . minikube start 2 . unclean shutdown ( kill vm ) 3 . minikube start Anything else do we need to know : Based on my research this is also what's happening when you run : minikube start . again after minikube is started ( < URL > This behavior stops docker from creating new containers due to lack of disk space .	2	0
1 1 1.4 2.0 1.3 2.0 1.45 2.0 1.0 1.0 0.0 0 0.0 0 0 0 8	timed out waiting to elevate kube-system RBAC privileges : Post : Service Unavailable . Hello , I am trying to start local cluster on Windows 10 using minikube ( v 0.28.2 ) . Everything runs fine until the step Starting cluster components ... where I stumbled on the following issue : E0727 15:00:05 . 218387 15960 start . go : 300 ] Error starting cluster : timed out waiting to elevate kube-system RBAC privileges : creating clusterrolebinding : Post < URL > Service Unavailable Do you know what might be the reason ? Best regards  	1	0
0 0 1.2 2.0 0.9 0.5 1.15 1.0 1.3333333333333333 2.0 1.015625 128 1.0 1 2 8 29	kic : fix var race condition .	0	0
2 2 1.8 2.0 1.3 2.0 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 3 5 7 51	Exiting due to RSRC_INSUFFICIENT_SYS_MEMORY : System only has 0MiB available , less than the required 1800MiB for Kubernetes . Steps to reproduce the issue : I am doing according to this instruction exactly < URL > Then I am failed at this step : minikube start Full output of failed command : 棣冩 minikube v 1.17.1 on Ubuntu 20.04 ( vbox/amd64 ) 閴?Automatically selected the docker driver 閴€?Exiting due to RSRC_INSUFFICIENT_SYS_MEMORY : System only has 0MiB available , less than the required 1800MiB for Kubernetes Full output of : minikube start . command used , if not already included : 棣冩 minikube v 1.17.1 on Ubuntu 20.04 ( vbox/amd64 ) 閴?Automatically selected the docker driver 閴€?Exiting due to RSRC_INSUFFICIENT_SYS_MEMORY : System only has 0MiB available , less than the required 1800MiB for Kubernetes Optional : Full output of : minikube logs . command :	2	0
1 1 1.4 1.0 1.1 1.0 1.1 1.0 1.3333333333333333 1.0 0.3333333333333333 3 0.0 0 2 8 25	Prototype Shared Storage between host/cluster container runtimes . Share Storage between host/cluster閳ユ獨 container runtime for building images as fast as docker-compose in local kubernetes  	1	1
0 0 0.8 1.0 0.8 1.0 0.8 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 0 0 35	Feature request : multiplatform hosts manager as minikube addon . Would be nice to implement support for something like < URL > as minikube addon ( < URL > ) . It has few bugs and seems to be unmaintained , but there is Linux , Windows , MacOS support and it might eventually work . Another related tools that I found : < URL > < URL > These things needs to be fixed : - < URL > - < URL > Thanks	2	1
2 2 1.0 1.0 1.1 1.5 1.2 1.0 1.0 1.0 0.0 0 0.0 1 4 5 19	how to use minikube addons open . The exact command to reproduce the issue : The full output of the command that failed : The output of the : minikube logs . command : [ root @localhost ~ ]# minikube addons open dashboard X This addon does not have an endpoint defined for the ' addons open ' command . You can add one by annotating a service with the label kubernetes.io/minikube-addons-endpoint:dashboard : [ root @localhost ~ ]# kubectl describe svc kubernetes-dashboard -n kubernetes-dashboard Name : kubernetes-dashboard Namespace : kubernetes-dashboard Labels : addonmanager.kubernetes.io/mode=Reconcile k8s-app = kubernetes-dashboard kubernetes.io/minikube-addons=dashboard kubernetes.io/minikube-addons-endpoint=dashboard Annotations : kubectl.kubernetes.io/last-applied-configuration : {' apiVersion ' : ' v1 ' , ' kind ' : ' Service ' , ' metadata ' :{ ' annotations ' :{ } , ' labels ' :{ ' addonmanager.kubernetes.io/mode ' : ' Reconcile ' , ' k8s-app ' : ' kubern ... kubernetes.io/minikube-addons-endpoint : dashboard label : kubernetes.io/minikube-addons-endpoint:dashboard Selector : k8s-app = kubernetes-dashboard Type : ClusterIP IP : 10.96.84.166 Port : < unset > 80/TCP TargetPort : 9090/TCP Endpoints : 172.17.0.6 : 9090 Session Affinity : None Events : < none > . The operating system version :	2	2
2 2 1.0 1.0 1.3 2.0 1.1 1.0 0.6666666666666666 0.0 0.882051282051282 195 1.0 2 4 10 48	auto-pause : support non-docker runtimes .	0	1
2 2 1.2 1.0 1.4 1.5 1.2 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 1 6 25	Ability to set a mirror for downloading kubectl , kubelet , & kubeadm . This is partly related to #11934 in that I'm trying to get minikube to work in an offline environment , or at least without direct internet connectivity . I have Artifactory configured as a Docker registry so assuming that issue is fixed , I don't need direct internet access to download the container images . However minikube still needs internet access to download the three : kube * . binaries . If I configure proxy access then there's nothing really to stop the container images being pulled through the proxy as well , which I'd like to avoid as it circumvents our internal policies and checks . It would be handy if I could supply an alternative URL that could be used to download the three : kube * . binaries . I can then set up an internal mirror of the < URL > content . Steps to reproduce the issue : : minikube start -- download-only -- image-repository = docker.artifactory.example.com . Containers are ( mostly , see linked issue ) downloaded , but it times out downloading : kubelet . , etc . There is obviously the : -- image-repository . option already , as well as the : -- iso-url . option that could be used to point to an alternative location for the minikube ISO ( I don't need/use that particular option ) , so it seems odd that there's no way to specify an alternative location for the remaining : kube * . binaries .  	1	1
2 2 1.4 2.0 1.1 1.0 1.15 1.0 2.0 2.0 0.5 2 0.5 0 2 8 28	 Build kvm driver for different architectures . See < URL > for details This issue is to track the KVM part	0	1
2 2 1.6 2.0 1.1 1.0 1.05 1.0 1.3333333333333333 2.0 0.0 0 0.0 2 2 6 22	Error downloading kubeadm v 1.13.3 : checksum validation failed . Facing error while starting minikube OS : Windows 10 Tool used : Windows Powershell Minikube version : v 0.34.1 kubectl verison : v 1.13.0 Command Used : . /minikube start -- vm-driver='virtualbox ' Output of command : o minikube v 0.34.1 on windows ( amd64 ) Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... - ' minikube ' IP address is 192.168.99.100 - Configuring Docker as the container runtime ... - Preparing Kubernetes environment ... @ Downloading kubeadm v 1.13.3 @ Downloading kubelet v 1.13.3 ! Failed to update cluster : downloading binaries : downloading kubeadm : Error downloading kubeadm v 1.13.3 : failed to download : failed to download to temp file : checksum validation failed Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL >	2	0
1 1 1.2 1.0 1.0 1.0 0.9 1.0 1.0 1.0 0.0 0 0.0 5 9 12 48	  Addons - Ingress DNS documentation is missing . < URL > Returns a 404 and shows a ' Not found ' page  	1	2
1 1 1.2 1.0 1.4 2.0 1.35 2.0 1.0 1.0 0.0 0 0.0 0 0 2 2	Convenience Method For Mapping GCP creds into pods To Simulate Running In That Env . From : < URL > This command : minikube auth gcp . would be a convenience function for mapping application default credentials into every pod so that the cluster operates similar to how it would in GCE/GKE with no additional yaml modifications required by the user .	2	1
0 0 0.2 0.0 1.0 1.0 1.05 1.0 0.3333333333333333 0.0 1.6923076923076923 13 2.0 1 5 9 26	Integration : automate download of periodic clean up script from github . We need to make our agents to download the clean up script for agents . the clean up script should do : download latest version of clean up code if failed to download or not changed , use the previous file . check if it is added to the cronjob if not add it .  	1	1
2 2 1.2 2.0 1.4 2.0 1.45 2.0 1.3333333333333333 2.0 1.6666666666666667 12 2.0 1 5 9 24	Integration Scripts : install consistent version of tools in integration tests . We need to have same version for the following tools for integration tests : golang kubectl version virtualbox docker libvirt	2	1
2 2 1.4 2.0 1.3 2.0 1.15 1.5 2.0 2.0 0.7647058823529411 17 1.0 2 6 15 50	  site : Add release schedule to documentation . We release minikube once per month at the end of the month , with a beta prior . Let's add a more detailed release schedule so that people won't be surprised by a new release .	0	2
1 1 1.4 2.0 1.5 2.0 1.3 2.0 1.0 1.0 1.619047619047619 21 2.0 1 3 5 33	fix cobra wrong help ' use optoins for list of .... ' . : $ minikube profile -- help profile sets the current minikube profile , or gets the current profile if no arguments are provided . This is used to run and manage multiple minikube instance . You can return to the default minikube profile by running `minikube profile default` Available Commands : list Lists all minikube profiles . Usage : minikube profile [ MINIKUBE_PROFILE_NAME ] . You can return to the default minikube profile by running `minikube profile default` [ flags ] [ options ] Use ' minikube < command > -- help ' for more information about a given command . Use ' minikube profile options ' for a list of global command-line options ( applies to all commands ) . . the last line is wrong : Use ' minikube profile options ' for a list of global command-line options ( applies to all commands )  	1	0
1 1 1.2 1.0 1.3 1.5 1.15 1.0 1.6666666666666667 2.0 0.0 1 0.0 2 6 9 41	cache with quay.io issues : Docker-Content-Digest and 405 Method Not Allowed . Hello , With minikube 1.0.0 on windows , I am not able to cache images from quay.io docker repository . : $ minikube cache add quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.23.0 2019/03/28 14:30:05 No matching credentials were found , falling back on anonymous ! Failed to cache and load images : caching images : caching image C : /Programs/Cygwin/home/cvila/ . minikube/cache/images/ quay.io/kubernetes-ingress-controller/nginx-ingress-controller_0.23.0 : manifest digest : ' sha 256:603 b7018c941b1117da4eb980c8043992ad37abdde3636fcbae1f34bcfff443f ' does not match Docker-Content-Digest : ' sha256 : e1292564ba5f1fd75005a4575778523d3309fb5d5d57f6597234c0b1567641f6 ' for ' quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.23.0 ' * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL > . From what I understood , it comes from < URL > which has been recently fixed ( beg March ) by < URL > Maybe we just need to make sure we compile minikube with the latest release of go-containerregistry ? Side question , is there a way to avoid the : No matching credentials were found , falling back on anonymous . , when we put in cache a public image ( which does not require any credentials to be passed ) Thanks !	2	0
1 1 0.8 1.0 0.9 1.0 0.95 1.0 1.0 1.0 0.0 0 0.0 1 5 9 33	examples doc uses deprecated ' kubectl run ' cmd . The exact command to reproduce the issue : kubectl run hello-minikube -- image = k8s.gcr.io/echoserver:1.4 -- port = 8080 From < URL > The full output of the command that failed : kubectl run -- generator = deployment/apps . v1 is DEPRECATED and will be removed in a future version . Use kubectl run -- generator = run-pod/v1 or kubectl create instead . deployment . apps/hello-minikube created The output of the : minikube logs . command : The operating system version : Darwin macintosh . home 18.7.0 Darwin Kernel Version 18.7.0 : Thu Jun 20 18:42:21 PDT 2019 ; root : xnu- 4903.270.47 ~ 4/RELEASE_X86_64 x86_64 Suggested fix : kubectl run hello-minikube -- generator = run-pod/v1 -- image = k8s.gcr.io/echoserver:1.4 -- port = 8080     	1	2
0 0 1.0 1.0 1.0 1.0 0.95 1.0 0.3333333333333333 0.0 0.8277310924369747 238 1.0 1 3 4 10	delete older generation of preloaded_kubernetes images from minikube cache . if user has been arround for a long time , they might accumulate many older version fo preload tarball , minikube should at least Clean up the Older Geneations and also Warn the user to delete the ones that has not been access in more than 6 months . : $ ls -lah ~ / . minikube/cache/preloaded-tarball/ total 4476968 drwxr-xr-x 10 medya primarygroup 320B Sep 22 17:19 . drwxr-xr-x 7 medya primarygroup 224B Sep 20 13:21 .. -rw-r -- r -- 1 medya primarygroup 585M Sep 20 14:32 preloaded-images-k8s-v10-v 1.20.2 -cri- o-o verlay-arm64 . tar . lz4 -rw-r -- r -- 1 medya primarygroup 16B Sep 20 14:32 preloaded-images-k8s-v10-v 1.20.2 -cri- o-o verlay-arm64 . tar . lz4 . checksum -rw-r -- r -- 1 medya primarygroup 515M Apr 21 14:23 preloaded-images-k8s-v10-v 1.20.2 -docker-overlay2-arm64 . tar . lz4 -rw-r -- r -- 1 medya primarygroup 16B Apr 21 14:23 preloaded-images-k8s-v10-v 1.20.2 -docker-overlay2-arm64 . tar . lz4 . checksum -rw-r -- r -- 1 medya primarygroup 545M Sep 22 17:19 preloaded-images-k8s-v12-v 1.22.1 -docker-overlay2-arm64 . tar . lz4 -rw-r -- r -- 1 medya primarygroup 16B Sep 22 17:19 preloaded-images-k8s-v12-v 1.22.1 -docker-overlay2-arm64 . tar . lz4 . checksum -rw-r -- r -- 1 medya primarygroup 541M Sep 20 13:16 preloaded-images-k8s-v13-v 1.22.2 -docker-overlay2-arm64 . tar . lz4 -rw-r -- r -- 1 medya primarygroup 16B Sep 20 13:16 preloaded-images-k8s-v13-v 1.22.2 -docker-overlay2-arm64 . tar . lz4 . checksum . for example in the above case we should clean up all the pre-V13 ones and also clean up any V13 that has not been ' accessed ' in 6 months	0	1
0 0 1.0 1.0 1.0 1.0 1.1 1.0 1.0 1.0 0.0 0 0.0 3 12 30 61	Skip preload download if ` -- image-repository` is set . 闁插秶骞囬梻顕€顣介幍鈧棁鈧惃鍕嚒娴?閿涙inikube start -- cpus = 2 -- memory='2000mb ' -- vm-driver hyperkit -- image-mirror-country='cn ' -- image-repository='registry . cn-hangzhou.aliyuncs.com/google_containers ' 婢惰精瑙﹂惃鍕嚒娴犮倗娈戠€瑰本鏆ｆ潏鎾冲毉 閿?棣冩 minikube v 1.9.2 on Darwin 10.15.4 閴?Using the hyperkit driver based on existing profile 閴?Using image repository registry.cn-hangzhou.aliyuncs.com/google_containers 棣冩啢 Starting control plane node m01 in cluster minikube 棣冩崙 Downloading Kubernetes v 1.18.0 preload ... > preloaded-images-k8s-v2-v 1.18.0 -docker-overlay2-amd64 . tar . lz4 : 131.59 MiB : minikube logs . 閸涙垝鎶ら惃鍕翻閸?閿?娴ｈ法鏁ら惃鍕惙娴ｆ粎閮寸紒鐔哄閺?閿涙acOS 10.15.4	0	0
1 1 1.2 1.0 1.2 1.0 1.15 1.0 1.0 1.0 0.0 0 0.0 4 4 7 20	start deletes existing VM on error starting VM . : minikube start . deletes an existing VM ( without any prompt ) if there is a problem starting the VM . This was observed using the VirtualBox driver . This should not happen as it can cause data loss and configures the new VM differently than the original VM . In this particular case , there was a transient failure starting the VM as the VirtualBox version was out of sync with its kernel modules . The exact command to reproduce the issue : : minikube start . ( My default profile is : work-minikube . ) . The full output of the command that failed : 棣冩 [ work-minikube ] minikube v 1.6.1 on Gentoo 2.6 閴?Selecting ' virtualbox ' driver from existing profile ( alternates : [ kvm2 ]) 棣冩啢 Kubernetes 1.17.0 is now available . If you would like to upgrade , specify : -- kubernetes-version = 1.17.0 棣冩敡 Starting existing virtualbox VM for ' work-minikube ' ... 棣冩敡 Retriable failure : start : Unable to start the VM : /usr/bin/VBoxManage startvm work-minikube -- type headless failed : VBoxManage : error : The virtual machine ' work-minikube ' has terminated unexpectedly during startup with exit code 1 ( 0x1 ) VBoxManage : error : Details : code NS_ERROR_FAILURE ( 0x80004005 ) , component MachineWrap , interface IMachine Details : 15:01:51 . 198451 Console : Machine state changed to ' PoweredOff ' 棣冩暉 Deleting ' work-minikube ' in virtualbox ... 棣冩暉 Creating virtualbox VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... The operating system version : Gentoo , Minikube 1.6.1 , VirtualBox 6.0.14	2	0
0 0 0.6 1.0 0.4 0.0 0.7 0.5 0.6666666666666666 1.0 1.1063829787234043 94 1.0 2 6 16 65	consider using kubectl inside for WaitForNode . we coud save seconds or network latency ( probably minimal ) if we wait for things with a client inside minikube cluster as opposed to getting and IP and PORT ( expensive in case of docker driver because needs to caculacate port forward which could take a 1 each time -would add up to 4-5 seconds in total-) alternatively we could use the kube client inside the minikube Node , to bypass network . we use the inside minikube kubectl smartly in some of the funcs : // elevateKubeSystemPrivileges gives the kube-system service account cluster admin privileges to work with RBAC . func ( k * Bootstrapper ) kelevateKubeSystemPrivileges(cfg config . ClusterConfig ) error { start : = time . Now () // Allow no more than 5 seconds for creating cluster role bindings ctx , cancel : = context . WithTimeout(context . Background () , 5*time . Second ) defer cancel () rbacName : = ' minikube-rbac ' // kubectl create clusterrolebinding minikube-rbac -- clusterrole = cluster-admin -- serviceaccount = kube-system : default cmd : = exec . CommandContext(ctx , ' sudo ' , kubectlPath(cfg ) , ' create ' , ' clusterrolebinding ' , rbacName , ' -- clusterrole = cluster-admin ' , ' -- serviceaccount = kube-system : default ' , fmt . Sprintf('--kubeconfig=%s ' , path . Join(vmpath . GuestPersistentDir , ' kubeconfig ' ))) rr , err : = k . c . RunCmd(cmd ) if err ! = nil { // Error from server ( AlreadyExists ): clusterrolebindings.rbac.authorization.k8s.io ' minikube-rbac ' already exists if strings . Contains(rr . Output () , fmt . Sprintf('Error from server ( AlreadyExists )')) { glog . Infof('rbac %q already exists not need to re-create . ' , rbacName ) return nil } } .  	1	1
2 2 1.0 1.0 1.0 1.0 1.35 2.0 1.0 1.0 0.0 0 0.0 0 1 10 27	Storage provisioner broken for multinode mode . It seems that the hostpath provisioner is completely unsuitable for multinode installations . It must create and prepare directories on the exact host on which a pv is requested . In addition , it must add nodeaffinity for every hostpath pv exactly the same as it happens at < URL > Steps to reproduce the issue : minikube create -n 3 install any statefulset that requests pv and has pod anti affinity rule minikube version : v 1.22.0 storage-provisioner : image : ' gcr.io/k8s-minikube/storage-provisioner:v5 '	2	0
2 2 1.2 2.0 1.1 1.0 1.15 1.0 2.0 2.0 0.0 0 0.0 0 1 11 34	regression : Docker v 1.5.0 ISO has new docker uid . Hello , the image : minikube-v 1.5.0 . iso . contains the : docker . user with different id/group numbers compared with the previous versions . This cause issues for people relying on the default ( home ) shared volume because now it is owned by a different user . With : minikube . versions : 1.2.0 . , : 1.3.0 . , : 1.4.0 . : : $ minikube ssh ' cat /etc/passwd ' [ ... ] docker : x : 1000:1000 : - : /home/docker : /bin/bash [ ... ] . But with : minikube . version : 1.5.0 . : : $ minikube ssh ' cat /etc/passwd ' [ ... ] docker : x : 1009:1017 : - : /home/docker : /bin/bash [ ... ] . This causes permission issues for people using the default ( home ) shared folder and relying on : minikube . default user id . The exact command to reproduce the issue : The full output of the command that failed : The output of the : minikube logs . command : The operating system version :	0	0
2 2 1.8 2.0 1.4 2.0 1.2 1.5 1.6666666666666667 2.0 1.105263157894737 57 1.0 0 0 5 32	Add warning when trying to run with cgroups v2 enabled . Until Kubernetes supports running with cgroups v2 enabled , we should add a warning when starting ... This affects the VM-less drivers , such as ' none ' , ' ssh ' , ' docker ' or ' podman ' , when running under Fedora 31 and later : < URL > < URL > The choices are either using the libvirt driver (' kvm2 ' ) , or changing back to cgroups v1 and rebooting . Currently we get a lot of subtle errors later on , when it fails to find required files under : /sys/fs/cgroup/ . .  	2	2
1 1 1.4 1.0 1.5 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 7 25 64	Document support for non-systemd-based init systems . Based on @medyagh's comment < URL > , there have been some feature additions to support init systems other than systemd . The docs specify in multiple places that a systemd based init system is required for minikube . This issue is to fix that in docs .  	2	2
1 1 0.8 1.0 1.0 1.0 1.05 1.0 1.0 1.0 1.5 4 1.5 0 1 12 40	Support for multiple architectures ( beyond amd64 ) . Back in ' 16 when the project was started , there was discussions about multiple architectures ( #5 ) But in the the end it was decided to focus on : amd64 . , mosty because of the hypervisors ( ~ #955 ~ ) . Other architectures were better off handled by kubeadm , than supported by minikube ( localkube ) Now things have changed a bit , and we might consider more architectures - like e.g. ARM ? Kubernetes supports : * amd64 * arm * arm64 * ppc64le * s390x for the binaries . I'm not sure which of these make sense as desktop systems , so I think it will be mostly : arm64 . . I added support for cross-compiling for < URL > Go architecture in #3887 - just like current OS cross . If we get some hypervisor-less installation going , we could consider supporting another arch . But for now , you are still better off using : kubeadm . directly on these - use them with : kubectl . . Added this story , since there were some discussions ongoing in older PR : #2171 #2455 #2881 The main issue is how to test anything but amd64 in CI , given the current existing infrastructure . Before that testing is in place , there can't really be any official support for any other architecture .	2	1
2 2 1.6 2.0 1.3 1.5 1.25 1.5 1.6666666666666667 2.0 1.1477272727272727 88 1.0 5 6 10 41	Starting minikube requires sudo due to error with certs copy . Something changed in : copyHostCerts . , that runs locally instead of on the remote ? : sudo cp -a /tmp/minikube123456789 ~ / . minikube/ca . pem . This fails , and is of course wrong to start with . It should not have needed : sudo . ... Using the virtualbox driver , other commands ran properly .	0	0
0 0 0.4 0.0 0.8 0.5 1.05 1.0 0.0 0.0 0.0 0 0.0 2 3 12 30	Don't spam multiple ' docker has taken an unusually long time ' messages during the same command . If docker is slow during a command it may repeat the message multiple times during the same command , it should only show the message once during the command . : minikube start 棣冩 minikube v 1.17.1 on Darwin 11.1 閴?Automatically selected the docker driver . Other choices : hyperkit , parallels , virtualbox , ssh 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 4000MB ) ... 棣冩儞 Preparing Kubernetes v 1.20.2 on Docker 20.10.2 ... 閳?Generating certificates and keys ... 閳?Booting up control plane ... 閳?Configuring RBAC rules ... 棣冩敺 Verifying Kubernetes components ... 閴?Executing ' docker container inspect minikube -- format ={{ . State . Status }}' took an unusually long time : 2.186437127 s 棣冩寱 Restarting the docker service may improve performance . 閴?Executing ' docker container inspect minikube -- format ={{ . State . Status }}' took an unusually long time : 3.383615222 s 棣冩寱 Restarting the docker service may improve performance . 閴?Executing ' docker container inspect minikube -- format ={{ . State . Status }}' took an unusually long time : 3.5012923 s 棣冩寱 Restarting the docker service may improve performance . 棣冨皞 Enabled addons : storage-provisioner , default-storageclass 棣冨及 Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default .	0	0
1 1 1.4 2.0 1.3 1.5 1.45 2.0 1.0 1.0 0.0 0 0.0 1 4 6 12	vbox/macos : ' service hello-minikube ' does not return . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : Minikube version ( use : minikube version . ): minikube version : v 0.33.1 - OS ( e.g. from /etc/os-release ): macos 10.14.2 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): virtutalbox - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.33.1 - Install tools : - Others : What happened : minikube service hello-minikube does not return What you expected to happen : I expected it to return with the service How to reproduce it ( as minimally and precisely as possible ): install minikube , start it run the hello-minikube deployment Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	0
0 0 0.6 0.0 1.0 1.0 1.1 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 3 6 27	Minikube hello world instructions don't include what you should see if it works . Steps to reproduce the issue : Follow instructions at c < URL > up through port forwarding command : a . minikube start b . minikube dashboard c . kubectl create deployment hello-minikube -- image = k8s.gcr.io/echoserver:1.4 d . kubectl expose deployment hello-minikube -- type = NodePort -- port = 8080 e . kubectl get services hello-minikube f . minikube service hello-minikube g . kubectl port-forward service/hello-minikube 7080:8080 Access < URL > What should this page look like ? The instructions should show that . It does not say ' hello world ' , so I'm left wondering if it worked or not . The output is this instead : CLIENT VALUES : client_address = 172.17.0.1 command = GET real path =/ query = nil request_version = 1.1 request_uri = < URL > SERVER VALUES : server_version = nginx : 1.10.0 - lua : 10001 HEADERS RECEIVED : accept = text/html , application/xhtml+xml , application/xml ; q = 0.9 , / ; q = 0.8 accept-encoding = gzip , deflate accept-language = en-us connection = keep-alive host = 127.0.0.1 : 51597 upgrade-insecure-requests = 1 user-agent = Mozilla/ 5.0 ( Macintosh ; Intel Mac OS X 10_15_7 ) AppleWebKit/ 605.1.15 ( KHTML , like Gecko ) Version/ 14.1.1 Safari/ 605.1.15 BODY : -no body in request-    	1	2
0 0 0.8 1.0 0.8 1.0 1.0 1.0 0.6666666666666666 0.0 1.09375 64 1.0 0 2 11 24	Podman v2 breaks the podman driver . The podman driver is still broken , with v 2.0.4 and 2.0.5 : : new client : new client : Error creating new ssh host from driver : Error getting ssh port for driver : get ssh host-port : convert host-port ' /x00 ' to number : strconv . Atoi : parsing ' : invalid syntax . This is a regression/follow-up from previous #8587 ( 2.0.1 ) The workaround is still to use the v 1.9.3 release instead . The goal is to remove the need for current warning #8783 : 閴?Using podman 2 is not supported yet . your version is ' 2.0.5 ' . minikube might not work . use at your own risk . . Note that podman 1 is no longer supported upstream : < URL > We also plan to drop support from the RHEL 8.4 release , spring 2021 And that RHEL 8 does not support docker , only podman . < URL > For RHEL 8 , Docker is not included and not supported by Red Hat This basically means that you can not run KIC on Red Hat Enterprise Linux . Fedora still includes Docker ( Moby ) , but doesn't work because of cgroups v2 .   	1	0
2 2 1.8 2.0 1.4 1.5 1.4 2.0 2.0 2.0 0.8 5 0.0 4 12 14 46	Skip loading images if using preloaded image with kic . This should also save time on : minikube start . . Note : only do this for the docker container runtime	0	1
0 0 1.2 2.0 1.2 2.0 1.1 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 3 40	Docs for Minikube DNS integration on MacOS . Apologies if I'm missing a link somewhere . Are there docs around how to set up DNS on MacOS to use the cluster's coredns ? I've found < URL > which seems to be the right approach , though it's mostly covering stuff that is superseded by : minikube tunnel . . < URL > gives a lot of detail on : minikube tunnel . , but doesn't mention DNS .  	2	2
1 1 1.2 1.0 1.4 1.5 1.1 1.0 1.3333333333333333 1.0 1.0521739130434782 115 1.0 3 3 6 54	site : get rid of redirect on index page . minikube site . has a redirect on the first page from < URL > to < URL > in site/content/en/_index . html we have this : : < a href='/docs'>redirecting to /docs < /a > < script language='javascript'> window . location . replace('/docs ' ) ; < /script > . instead of re-direct , it should try to load the /docs you can use help form hugo site < URL > please only take this task if you are fimmilar with Hugo and willing to try it locally using : make site . and try in your local browser that works  	2	2
2 2 1.2 1.0 1.3 1.0 1.05 1.0 1.0 1.0 1.25 4 1.5 0 0 0 1	Brew Formula for Minikube . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): feature request We currently have a brew cask installer , which installs from a pre-built binary . However , we can create a real brew formula for minikube now that we no longer include ( and therefore don't cross build ) localkube with go-bindata . I think the formula should be pretty straightforward and would be very similar to existing go formula .	2	1
0 0 1.2 1.0 1.3 1.5 1.1 1.0 1.0 1.0 1.75 4 2.0 2 5 9 39	How to provide an image pull secret when using ' -- image-repository ' with an internal registry that does not allow anonymous pull requests ? . After a bit of a hiatus , I have been revisiting the whole topic of running minikube from purely local sources ( i.e. , any images should come from our own private registry ) . I revisted the issue < URL > that I created in June . The first problem I am running into is that our internal registry does not support anonymous pull access . In the above mentioned issue , @tstromberg mentioned two sources on how to provide a pull secret - but all of the already require a running cluster , as they both boil down to enabling the registry-creds plugin : : minikube addons configure registry-creds . But at that point all the infrastructure images would already have to be there , no ? Further , this add-on only allows me to specify creds for ECR , GCR , Dockerhub or ACR - what about my own registry ?	2	1
2 2 1.4 2.0 1.6 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 3 7 38	Include registry mirror if specified in addon template yamls . Steps to reproduce the issue : Create minikube with $ minikube start -- vm = true -- image-mirror-country = cn -- registry-mirror = < URL > $ minikube addons enable ingress 棣冩敺 Verifying ingress addon ... 閴?Exiting due to MK_ENABLE : run callbacks : running callbacks : [ waiting for app.kubernetes.io/name=ingress-nginx pods : timed out waiting for the condition ] 棣冩▼ If the above advice does not help , please let us know : 棣冩啝 < URL > The reason is in < URL > Some image from Docker Hub is also using the mirror repo : image : {{ default ' jettech ' . ImageRepository}}/kube-webhook-certgen : v 1.3.0 . Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : Current logic for handle Docker Hub image seems incorrect and not leveraging -- registry-mirror Thanks a lot  	1	0
0 0 1.2 2.0 0.8 0.0 1.05 1.0 0.6666666666666666 0.0 0.2857142857142857 7 0.0 1 1 6 32	Putting flag as first argument causes unintended log file names . Current : : $ minikube -p m1 status Log file created is : /tmp/minikube_-p_ < hash > . log . Expected : : $ minikube -p m1 status Log file created is : /tmp/minikube_status_ < hash > . log .	0	0
1 1 0.4 0.0 0.5 0.0 0.85 1.0 0.3333333333333333 0.0 0.8181818181818182 242 1.0 2 2 4 14	add new flag -- no-kubernetes . it would start a minikube VM without starting any kubernetes in it ( for people interested in only using minikube as a docker desktop )	0	1
2 2 1.0 1.0 1.2 1.5 1.35 2.0 1.0 1.0 0.0 0 0.0 0 1 11 38	Changing a Service from type : LoadBalancer to type : ClusterIP doesn't remove status . loadBalancer . ingress [] . Steps to reproduce the issue : minikube start -- memory 8192 -- cpus 8 -- disk-size 80g minikube tunnel Create a Service my-svc with : type : LoadBalancer . Observe : kubectl get service my-svc -o yaml . shows : status . loadBalancer . ingress[0 ] . ip . with the service's clusterIP kubectl patch service my-svc -- type = merge -p ' spec : { type : ClusterIP }' Actual : : status . loadBalancer . ingress[0 ] . ip . remains . Expected : : status . loadBalancer . ingress . should be removed . This would help automated tests that need to work with both minikube and cloud k8s clusters like GKE , that test adding and removing external access on Service . We took a look at LoadBalancerEmulator , and saw that it has a method : cleanupService () . . However , it is only called on Services with type : LoadBalancer , and the cleanup methods are only called during shutdown . Ideally , it seems like PatchServices should reconcile all Services to add or remove status . loadBalancer . ingress as appropriate .  	1	0
2 2 0.8 0.0 0.8 0.0 1.1 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 1 4 20	Possibility to configure pvDir for storage-provisioner addon . I have a huge minikube cluster for local development with many stateful services(like mysql , rabbitmq , etc ) . If I use docker driver in case of minikube profile recreation(for example , upgrading k8s version or changing cluster resources or something else ) I will lose all of my persistent data , because in current implementation pvDir is a constant - : /tmp/hostpath-provisioner . . It would be nice to have possibility to configure pvDir option for storage-provisioner , like : : minikube addons configure storage-provisioner . , then I could set pvDir to host mounted directory , create necessary pv on cluster setup and all my persistent data would be available on profile recreation . What do you think about it ? P.S. Now , I use my own version of storage-provisoner to protect my data from losing .	2	1
1 1 1.2 1.0 1.0 1.0 0.95 1.0 0.6666666666666666 1.0 1.1348314606741574 89 1.0 0 4 11 42	Don't duplicate the images in the preload for containerd runtime . Currently we are unpacking the images in the preloaded tarball for the containerd runtime . This causes it to be much larger to download than the files for the other container runtimes . 0 ) Small size ( 211M ) - all images , compressed as a whole : 948M images . tar 297M images . tgz 211M images . txz . 1 ) Cache images ( 315M ) - all images , compressed individually : 90M images/ docker.io/kubernetesui 11M images/ gcr.io/k8s-minikube 215M images/ k8s.gcr.io . 2 ) Preload images ( 491M ) - all images ( unpacked ) + all binaries : 903M preloaded-tarball/preloaded-images-k8s-v7-v 1.20.0 -containerd-overlay2-amd64 . tar . lz4 556M preloaded-tarball/preloaded-images-k8s-v7-v 1.20.0 -cri- o-o verlay-amd64 . tar . lz4 491M preloaded-tarball/preloaded-images-k8s-v7-v 1.20.0 -docker-overlay2-amd64 . tar . lz4 . : 38M linux/v 1.20.0 /kubeadm 39M linux/v 1.20.0 /kubectl 109M linux/v 1.20.0 /kubelet . It would be nice if there was a way to not have to store both the blobs and the snapshots ? This way they will only be stored once , like for the other container runtimes ( and not twice ) . /lib/containerd/io . containerd . content . v1 . content/blobs/ . /lib/containerd/io . containerd . snapshotter . v1 . overlayfs/snapshots/  	1	0
1 1 0.8 1.0 0.6 0.5 0.75 1.0 0.6666666666666666 1.0 0.8686868686868687 198 1.0 5 7 13 51	TryMacStudium VMs for integration tests .	0	1
1 1 1.4 1.0 1.3 1.0 1.2 1.0 1.6666666666666667 2.0 1.2093023255813953 43 1.0 0 1 9 34	make it obvious in terminal for user if using docker-env . I was thinking as part of the eval $(minikube docker-env ) we could pass something that changes the terminal Prompt so it is obvious for the user they are usinf minikube's docker not their own docker . maybe : something like : : medya@ ~ $ PS1='$PS1'(using minikube dockerd ) $:' . that will try turn into : medya@ ~ $ ( using minikube dockerd ) $ : . the minikube word would be profile name , this will also make it clear which profile's docker-env we are using . helps with < URL > helps with < URL > also we could explicity set a ENV that says : USES_MINIKUBE_DOCKERD = PROFILE_NAME .  	2	2
1 1 1.2 1.0 1.3 1.0 1.0 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 1 4 25	Publish additional ports and IPs when running minikube start . When : minikube start . command runs using the Docker driver it tells Docker to publish some ports , but it binds to the localhost ip only , thus not giving a chance to access the cluster from another machine using the : kubectl . for example . Another issue is if one wants to access a Service via an Ingress , in that case we need the same solution , which is to add some entries to the iptables as following : : iptables -I DOCKER-USER 1 ! -i docker0 -o docker0 -p tcp -j ACCEPT -d $(minikube ip ) -- dport 443 iptables -t nat -A DOCKER ! -i docker0 -p tcp -j DNAT -- dport 443 -- to-destination $(minikube ip ): 443 . I could also execute those command to bind the 8443 port to 0.0.0.0 to use the kubectl outside the local minikube machine . Does minikube provide a better way of doing that or is that an inexistent feature ?  	2	2
1 1 0.4 0.0 0.9 1.0 1.0 1.0 0.3333333333333333 0.0 1.1636363636363636 110 1.0 0 2 3 30	Alternative Buildroot 2021.02 image , using Linux version 5 . The current 2020.02 LTS series are now end of life , we should upgrade to 2021.02 LTS if we want to keep it . While doing so , we should make yet another attempt at upgrading from Linux version 4 to Linux version 5 ... : # From < URL > sha256 930ae76b9a3b64b98802849aca332d17a706f20595de21e1ae729b55ee461add linux- 5.10.25 . tar . xz sha256 1c3cef545f366b56332c11c28d074c9d9148c28059a970ec8710826652237560 linux- 5.4.107 . tar . xz # From < URL > sha256 05db750ba01ad557bef50835c253894fad9fb0db2224f0e803b25e2ff7ab2365 linux- 4.19.182 . tar . xz sha256 7adc041af81424ff8d68affe3005fa9e5babc4e84e0b07e4effdf54225ba9426 linux- 4.14.226 . tar . xz . This relates to #9992 and #10501  	1	1
2 2 1.4 2.0 1.0 1.0 0.9 0.5 2.0 2.0 1.25 8 1.5 1 10 19 57	Default vmdriver selection on existing node shouldn't override existing driver . I would expect minikube to just simply start an existing minikube cluster by : : minikube start -p < clustername > . However this is not the case when the default vm driver is different from the vmdriver of the node , I saw this error message when restarting my existing docker2 node . : minikube start -p docker2 1194ms 椤?Thu Feb 27 10:07:32 2020 棣冩 [ docker2 ] minikube v 1.7.3 on Darwin 10.15.3 閴?Automatically selected the hyperkit driver 棣冩寽 The existing ' docker2 ' VM that was created using the ' docker ' driver , and is incompatible with the ' hyperkit ' driver . 棣冩啝 To proceed , either : 1 ) Delete the existing ' docker2 ' cluster using : ' minikube -p profile delete ' * or * 2 ) Start the existing ' docker2 ' cluster using : ' minikube -p profile start -- vm-driver = docker ' 棣冩寴 Exiting . .	0	0
2 2 0.8 0.0 1.2 1.5 0.95 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 3 5 45	The ' openrc ' sysinit should probably verify that it is actually using OpenRC . minikube start * minikube v 1.11.0 on Amazon 2018.03 ( xen/amd64 ) * Using the none driver based on existing profile * Starting control plane node minikube in cluster minikube * Restarting existing none bare metal machine for ' minikube ' ... * OS release is Amazon Linux AMI 2018.03 * Preparing Kubernetes v 1.18.3 on Docker 19.03.6 -ce ... * X Failed to update cluster : updating node : sudo service kubelet start : exit status 127 stdout : stderr : /etc/init . d/kubelet : line 9 : start-stop-daemon : command not found * * minikube is exiting due to an error . If the above message is not useful , open an issue :	2	1
0 0 0.0 0.0 0.2 0.0 0.45 0.0 0.0 0.0 0.42857142857142855 7 0.0 0 4 11 55	kvm : add ' Troubleshooting KVM networks ' section with commands to docs . page : < URL > ref : < URL > /assign    	1	2
2 2 1.2 1.0 1.2 1.5 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 2 6 32	Docs : Host access . From the docs : : Prerequisites : The service running on your host must either be bound to all IP閳ユ獨 ( 0.0.0.0 ) and interfaces , or to the IP and interface your VM is bridged against . If the service is bound only to localhost ( 127.0.0.1 ) , this will not work . . Is it possible to add on ' how does one achieve that ' ? Thank you    	1	2
2 2 1.4 2.0 1.1 1.0 0.95 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 0 1	Add multi-node support . To provide a complete kubernetes experience , user might want to play & experience with features like scheduling and daemon set , etc . If minikube can emulate multiple kubernetes nodes , users can then use most of the kubernetes features . This is probably not necessary in the first few versions of minikube . Once the single node setup is stable , we can look at emulating multiple nodes . Up votes from users on this issue can be used as a signal to start working on this feature .	0	1
1 1 0.4 0.0 0.5 0.5 0.65 1.0 0.6666666666666666 1.0 0.9805194805194806 154 1.0 0 1 7 23	delete config after failed start . example windows with windows container on data center windows minikbue skips docker driver since os type is windows then tries hyperv and it is failed better to delete that config so when user switches os type to linux , they can re-use docker driver not be stuck with hyperv	0	0
2 2 1.6 2.0 1.2 1.0 1.05 1.0 1.6666666666666667 2.0 0.9570552147239264 163 1.0 3 4 7 34	Add static IP for KVM driver . similar to our docker and podman driver implementation	0	1
1 1 1.0 1.0 0.8 1.0 1.15 1.0 1.3333333333333333 1.0 1.0285714285714285 35 1.0 1 3 10 29	Add NFS support to ' minikube mount ' . ' minikube mount ' only supports 9pfs at the moment . It wouldn't be difficult to add NFS support , which is much faster . On platforms where NFS is supported , it should be the default , as 9p has many issues .  	1	1
0 0 1.0 1.0 1.3 2.0 1.05 1.0 0.3333333333333333 0.0 1.0149253731343284 67 1.0 2 4 7 35	none : ' start ' may declare success prematurely immediately after ' stop ' . With the none driver , if you run : ' minikube start ' ' minikube stop ' ' minikube start ' You may run into a situation where the apiserver health check passes , but found the old apiserver process instead , that is subsequently restarted , so a call to the apiserver fails . This mostly manifests with unreliable integration testing .  	1	0
1 1 1.0 1.0 0.8 1.0 0.9 1.0 1.3333333333333333 1.0 1.2777777777777777 36 1.0 2 2 4 25	kic : run docker container without -- privileged . Thanks to @josedonizetti who first noticed we could improve this . current docker run looks like -- privileged -- security-opt seccomp = unconfined we could get rid of privilged and pass in the selective whitelist of features we need .  	1	1
0 0 1.4 2.0 1.4 2.0 1.4 2.0 1.3333333333333333 2.0 0.0 0 0.0 2 4 18 46	minikube kvm driver constant writes to disk . Steps to reproduce the issue : 1 . minikube start -- driver = kvm2 -- cni = calico -- nodes 2 When we start minikube with kvm driver a constant stream of io writes is executed due to etcd . Inside the vm the io is some kb/s but it is amplified on the host , causing bottlenecks . This problem is minimized when starting minikube with : minikube start -- driver = kvm2 -- cni = calico -- nodes 2 -- extra-config = etcd . backend-batch-limit = 1000 -- extra-config = etcd . backend-batch-interval = 60s thus forcing etcd to write less frequently ( but at the risk of consistency ) . Based on the nature of this issue , I assume it could affect all vm drivers . Does it makes sense to change the default etcd parameters at least for VM drivers ? If not , this info could be documented ?  	1	0
2 2 1.4 1.0 1.2 1.0 1.2 1.0 1.3333333333333333 1.0 1.04 75 1.0 0 0 2 23	ssh with kic : driver does not have SSHHostName . I expected this to work with Docker for Desktop on macOS , but it didn't . : minikube start -- vm-driver = docker minikube ssh . message : : ssh : Creating ssh client : driver does not have SSHHostName . If it isn't possible , we should display a nicer error message .	0	0
0 0 1.0 1.0 1.3 2.0 1.25 2.0 0.6666666666666666 0.0 0.0 0 0.0 2 5 7 23	Trandalation file load error . The exact command to reproduce the issue : Any minikube command for example : : minikube profile . All minikube commands are throwing the error : : E0621 12:57:44 . 947127 1086 translate . go : 80 ] Failed to load transalation file for [ locale ]: open pkg/minikube/translate/translations/[respective_locale_file ] . json : no such file or directory . . before proceeding further . Is there not a better way to deal with this error rather than displaying it with every minikube command . The operating system version : ubuntu 18.04	0	0
0 0 1.2 1.0 0.9 1.0 0.8 1.0 1.3333333333333333 2.0 1.2142857142857142 14 1.5 1 1 5 9	  Rewrite GitHub README.md to point to official docs & mailing lists . The minikube README.md needs to be streamlined . These are the two highest priority issues : Installation documentation should be de-duped and point to < URL > Mailing lists & meeting details should be front and center	0	2
0 0 0.6 0.0 0.6 0.0 1.0 1.0 0.0 0.0 0.0 0 0.0 0 0 6 13	Feature Request : Istio addon . FEATURE REQUEST Istio is becoming very popular for it's service mesh and ingress features . It is now available on GKE as a managed service . It would be a really nice addon for minikube .  	1	1
2 2 0.8 0.0 0.4 0.0 0.7 0.0 0.6666666666666666 0.0 0.0 0 0.0 3 4 7 21	Add problem ID and exit code for ' Ensure your system has enough CPUs . The minimum allowed is 2 CPUs ' . Steps to reproduce the issue : Run : minikube start . on a default Google Cloud Compute VM with 1 cpu with : -- output = json . Get the error : {' data ' :{ ' exitcode ' : ' 64 ' , ' message ' : ' Ensure your system has enough CPUs . The minimum allowed is 2 CPUs . /n ' } , ' datacontenttype ' : ' application/json ' , ' id ' : ' 507528d8-0e85-47bb-a2d1-08b41d868683 ' , ' source ' : ' < URL>' , ' specversion ' : ' 1.0 ' , ' type ' : ' io . k8s . sigs . minikube . error ' } . : gcloud . needs a machine readable error code that matches 1-to-1 with the message for this so it can understand what the error was without parsing the message .	0	1
0 0 1.0 1.0 1.4 2.0 1.2 1.0 0.6666666666666666 0.0 1.1388888888888888 72 1.0 1 2 11 56	preload should not load wrong arch images . per comment in this issue : < URL > and someone tried to start on arm64 arch and minikube tried to load preloaded images for amd64 arch . we need to make our tar files Per architecture , and if if there is no preload for arm64 we should not load wrong architecture we should also provide a flag , -- preload = false	0	0
1 1 1.0 1.0 0.8 0.5 1.2 2.0 1.0 1.0 0.0 0 0.0 1 2 8 21	Update ingress-nginx to latest release . Minikube currently uses < URL > . In this release the : x-forwarded-prefix . annotation is broken ( see < URL > This should be fixed with : 0.24.0 . ( see < URL > The above fix is not backwards compatible and requires modifying the annotation , i.e. , pass the actual prefix instead of a boolean ' true ' , but I guess this is better than not working at all , right ? Is there any reason not to update ingress-nginx to the latest release , i.e. , < URL > ?	2	0
0 0 0.2 0.0 0.4 0.0 1.0 1.0 0.3333333333333333 0.0 1.0919540229885059 87 1.0 2 13 29 58	kic on GCE : docker : Windows does not support privileged mode . trying windows VMs in google cloud with docker driver : I bet I need to start the vms that support nested virtualization . ( they support windows too ) but if anyone interested to take this task . and do experiment with Windows VMs on google cloud . so we can add them to jenkins for testing : docker ' minikube ' container is missing , will recreate . * Creating Kubernetes in docker container with ( CPUs = 2 ) ( 4 available ) , Memory = 38 00MB ( 15358MB available ) ... * X Failed to start docker container . ' minikube start ' may fix it . : recreate : crea ting host : create : creating : create kic node : create container : failed args : [ ru n -d -t -- privileged -- security-opt seccomp = unconfined -- tmpfs /tmp -- tmpfs /run -v /lib/modules : /lib/modules : ro -- hostname minikube -- name minikube -- label cre ated_by . minikube.sigs.k8s.io=true -- label name.minikube.sigs.k8s.io=minikube -- l abel role.minikube.sigs.k8s.io= -- label mode.minikube.sigs.k8s.io=minikube -- vol ume minikube : /var -- cpus = 2 -- memory = 3800mb -- expose 8443 -- publish = 127.0.0.1 :: 84 43 -- publish = 127.0.0.1 :: 22 -- publish = 127.0.0.1 :: 2376 gcr.io/k8s-minikube/kicbase : v 0.0.8 @sha256 : 2f3380ebf1bb0c75b0b47160fd4e61b7b8fef0f1f32f9def108d3eada50a7a81 ] output : docker : Error response from daemon : Windows does not support privileged mode . See ' docker run -- help ' . : exit status 125 .  	1	0
2 2 1.2 2.0 1.2 2.0 1.1 1.0 2.0 2.0 0.0 0 0.0 0 0 3 19	Driver timeouts should be configurable or disablable . On typically configured systems , : libvirt . should be able to be used as a regular user by way of PolicyKit . The ' two second or we kill your attempt ' provision disallows any kind of such usage when PolicyKit prompts the user for authentication , as even copying your password to the clipboard and just being very quick on the paste keys can fail and should not be the expected use case . Thus I propose allowing < URL > to be configured to a longer time span or disabled all together , so that a regular user still is able to type in passwords without finding minikube already having abandoned any attempt .	2	1
2 2 1.8 2.0 1.5 2.0 1.3 1.5 2.0 2.0 0.0 0 0.0 3 5 6 23	`minikube docker` subcommand . Steps to reproduce the issue : This is not an issue but a feature request . It would be nice to have a : minikube docker . subcommand that shells out to : docker . with the command's : DOCKER_* . env vars set to env vars output by : minikube docker-env . . This is similar to : minikube kubectl . which runs : kubectl . against the minikube cluster . Currently , in order to not to pollute my current shell , I'm running : sh -c ' eval $(minikube docker-env ) && docker build ... ' . to self-contain any docker commands against the minikube cluster . This is not as convenient . Full output of failed command : N/A Full output of : minikube start . command used , if not already included : N/A Optional : Full output of : minikube logs . command : N/A	2	1
0 0 1.0 1.0 1.4 2.0 1.1 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 0 6 35	minikube_latest_amd64 . deb accidentally included in Github release assets . When checking through the release assets of the last release ( < URL > I found that there are two . deb files for the same thing : - minikube_latest_amd64 . deb - minikube_ 1.13.1 -0_amd64 . deb IMO the : latest . one shouldn't be there , since the path to it is still versioned as : < URL > . . Since I haven't found the corresponding pipeline so far , maybe you could/should look into this .	2	0
2 2 1.4 2.0 1.3 1.5 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 1 1	Proposal : ' restart ' command . Often times when my laptop goes to sleep , I encounter issues with my minikube setup , such as sometimes it just hangs , sometimes a time drift occurs between VM and the host and I end up just doing : : minikube stop ; minikube start . It would be great if there was a : minikube restart . . Similarly , : docker-machine . also has a ' restart ' subcommand . It's useful when things just go wrong and you quickly want to fix it . Thoughts ? I can volunteer to implement this .	2	1
0 0 1.0 1.0 1.1 1.0 1.05 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 2 6 23	update readme with clear instructions to run site locally . readme does not state clearly the steps to run site locally . < URL >  	2	2
0 0 1.4 2.0 1.5 2.0 1.35 2.0 1.0 1.0 1.0236220472440944 127 1.0 2 3 11 39	none driver : setting CNI times out the tests . after this PR < URL > was merged the none driver been timing out . and we ended up up skipping it in none driver , we should figure out if it is not usable for none driver , then not allow the user to set it .	0	0
2 2 1.6 2.0 1.6 2.0 1.55 2.0 2.0 2.0 0.0 0 0.0 4 5 8 35	Minikube start with multi node failed with default name . minikube start failed with multi nodes and default name , but it won't fail if i specify -p . /out/minikube start -n 2 -- alsologtostderr Please find the full log in the attachment . < URL > . /out/minikube start -n 2 -p p1 works fine for me .  	1	0
2 2 2.0 2.0 1.4 2.0 1.2 1.5 2.0 2.0 1.1652173913043478 115 1.0 1 1 5 21	Move podman CNI config to different directory . Kubernetes has a problem to handle third-party packages using CNI : ~~< URL >~~ Any installed config files will be used if present , in alphabetical order ... There is no way to select a specific config , especially one appearing later . Since < URL > Podman now has a configuration option to select a different directory : /etc/containers/containers . conf : # The network table contains settings pertaining to the management of # CNI plugins . [ network ] # Path to directory where CNI plugin binaries are located . # # cni_plugin_dirs = ['/usr/libexec/cni ' ] # The network name of the default CNI network to attach pods to . # default_network = ' podman ' # Path to the directory where CNI configuration files are located . # # network_config_dir = ' /etc/cni/net . d/' . : network . network_config_dir . Changing this to a different directory , is the easiest way to fix kubeadm . /etc/cni/net . d/87-podman-bridge . conflist Another option would be to delete the file , and use : -- network = host . . But that would require any podman users to change , breaking some . : Error : error configuring network namespace for container f56bea2ef5b840309583da9c1b18b416f94c750d9b30a0036e02a49622b653e6 : CNI network ' podman ' not found . Podman has the opposite side , they don't normally install Kubernetes . So there is no incentive to change the podman default cni packaging . : # Path to the directory where CNI configuration files are located . # # network_config_dir = ' /etc/cni/net . d/' network_config_dir = ' /etc/containers/net . d/' .  	1	1
2 2 1.2 2.0 0.9 0.5 1.2 1.5 1.3333333333333333 2.0 0.0 0 0.0 1 1 1 4	minikube should not start knfsd by default . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.28.2 - OS ( e.g. from /etc/os-release ): Fedora 28 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): kvm2 - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.28.1 On startup , minikube starts up knfsd ( via the nfs-server . service under systemd ) , even though it serves no exports . This makes it somewhat useless -- it should not start the nfs server unless it has something to serve : : 閳?nfs-server . service - NFS server and services Loaded : loaded (/usr/lib/systemd/system/nfs-server . service ; enabled ; vendor preset : enabled ) Active : inactive ( dead ) since Thu 2018-09-20 13:34:48 UTC ; 7min ago Process : 2886 ExecStopPost = /usr/sbin/exportfs -f ( code = exited , status = 0/SUCCESS ) Process : 2883 ExecStopPost = /usr/sbin/exportfs -au ( code = exited , status = 0/SUCCESS ) Process : 2880 ExecStop = /usr/sbin/rpc . nfsd 0 ( code = exited , status = 0/SUCCESS ) Main PID : 1781 ( code = exited , status = 0/SUCCESS ) Sep 20 12:06:54 minikube systemd[1 ]: Starting NFS server and services ... Sep 20 12:06:54 minikube systemd[1 ]: Started NFS server and services . Sep 20 13:34:48 minikube systemd[1 ]: Stopping NFS server and services ... Sep 20 13:34:48 minikube systemd[1 ]: Stopped NFS server and services . . When constructing the ISO , could we add in a call to ' systemctl disable nfs-server ' to ensure that it's not started by default ?	2	1
0 0 1.2 2.0 1.3 2.0 1.25 1.5 1.3333333333333333 2.0 0.0 3 0.0 1 5 17 49	Create fix and write tests for Snap package manager bug . Background info : The original issue was that starting minikube installed via Snap ( package manager ) would result in the error : : prepare kic ssh : copying pub key : docker copy /tmp/tmpf-memory-asset150446131 into minikube : /home/docker/ . ssh/authorized_keys , output : lstat /tmp/tmpf-memory-asset150446131 : no such file or directory This is due to Snap only allowing packages to see their own view of the /tmp folder . . In this case minikube copies a memory asset to the /tmp folder for Docker to later copy . Snap prevents Docker from seeing minikubes file and the above error occurs . This issue was attempted to be fixed in #10042 , the correct way to solve this issue is to check if the binary is installed via Snap , and if it is , writes the memory asset to the users home directory instead . However , there was a bug included in the PR where instead of setting the tmp directory to the users home directory , it would try to find an env variable by the name of the users home directory and set that as the tmp directory . This env was not found and would return an empty string , passing an empty string to the function would fallback to using the default tmp directory and would fail .	0	0
1 1 0.6 1.0 0.5 0.0 0.9 1.0 0.6666666666666666 1.0 0.0 0 0.0 5 9 16 52	  Documentation on enabling CNI For using Pod Network Policies . It looks like minikube supports network policies using -- network-plugin = cni : : minikube start -- vm-driver virtualbox -- network-plugin = cni -- cpus 4 -- memory 8192s . However I don't find any documentation concerning that feature , and how to enable a network plugin for instance calico .   	1	2
2 2 1.8 2.0 1.4 2.0 1.35 1.5 1.6666666666666667 2.0 1.1515151515151516 99 1.0 2 12 16 41	Make the getting started / installation page look nice again . After adding architectures to the ' Installation ' section , it's an even bigger mess than it was before ... Now it separates both on OS and on CPU , and it can be confusing to know what to actually download : < URL > ( 1 ) Installation * Linux * x86 * Binary download * Debian package * RPM package * ARM * Binary download * Debian package * RPM package * macOS * Brew package manager * x86 * Binary download * ARM * Binary download * Windows * Windows Package Manager * Chocolatey * Stand-alone Windows Installer Hopefully there is some nice web design that could be done , especially for the Linux tab ? I described three different workarounds ( in text ) , but am looking for a better looking solution .  	2	2
1 1 0.6 1.0 1.0 1.0 1.25 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 2 3 24	cache add : transferring cached image : Process exited with status 137 from signal KILL . problem I am adding offline ~ 2G image to minikube with : cache add . command but it always failed . When I ssh login VM I found a ~ 400M image file in : /tmp . folder . I try a lot of times it seems it is always failed at ~ 400M data transfered . Small size image has no such problems . The strange is when I put this image file in : files . folder it is always transferred to VM successfully . And also I can run : docker load -i xxx . successfully in VM . logs ! Failed to cache and load images : loading cached images : loading image { MINI_HOME}/cache/images/xxxx-web_ 2.89.27 : transferring cached image : Process exited with status 137 from signal KILL environment : windows 10 + virtualbox minikube 1.0.0	1	0
0 0 0.6 0.0 0.9 0.5 1.25 2.0 1.0 1.0 0.9120879120879121 182 1.0 3 4 9 32	Add solution message for slowdown in k8s 1.20.3 and above . .	0	1
2 2 1.2 1.0 1.1 1.0 1.0 1.0 1.6666666666666667 2.0 0.0 0 0.0 5 10 17 50	Get docker.pkg.github.com/v2/kubernetes/minikube/kicbase/mainfests/v0.0.10 : no basic auth credentials . Steps to reproduce the issue : Copy < URL > to browse Install minikube-installer . exe in window 10 Start your cluster by run : minikube start Full output of failed command : stderr : Template parsing error : tempate : : 1:8 : executing ' at < . State . Status > : map has no entry for key ' State ' stderr : Unable to find image ' docker.pkg.github.com/kubernetes/minikube/kicbase:v0.0.10 ' locally docker : Error response from daemon : Get < URL > no basic auth credentials Full output of : minikube start . command used , if not already included : stderr : Unable to find image ' docker.pkg.github.com/kubernetes/minikube/kicbase:v0.0.10 ' locally docker : Error response from daemon : Get < URL > no basic auth credentials	0	0
2 2 0.8 0.0 0.8 1.0 0.8 1.0 1.3333333333333333 2.0 1.5 14 2.0 1 5 12 31	Add a button to copy the command to the browser on the docs site . I think that it would be good user experience if we can add a button besides each code snippet which should be copied to the clipboard like several sites do it .  	2	2
1 1 1.4 1.0 1.1 1.0 1.15 1.0 1.6666666666666667 2.0 0.9655172413793104 58 1.0 1 2 4 20	Add ` -- addons` flag to `minikube start` . It would be nice if users could specify which addons a cluster will have , especially as this could in the future inform the default resource footprint for a cluster . For example : : minikube start -- addons helm-tiller . Related : #4227 #5302  	1	1
2 2 1.2 1.0 0.9 1.0 0.85 1.0 1.0 1.0 0.8373205741626795 209 1.0 3 6 10 49	UI : make it hard to miss Important notices . I noticed many ppl miss the notice to use different Port for Mac Os for registry addon : $ minikube addons enable registry 棣冩寱 Registry addon on with docker uses 55012 please use that instead of default 5000 棣冩憣 For more information see : < URL > 閳?Using image registry : 2.7.1 閳?Using image gcr.io/google_containers/kube-registry-proxy:0.4 棣冩敺 Verifying registry addon ... 棣冨皞 The ' registry ' addon is enabled . maybe we can have a new style that creates a Index Pointer Emoji in a Box with one line empty above and bellow so it is hard to miss : $ minikube addons enable registry 棣冩啝 ------------------------------------------------------------------------------------------ 棣冩啝 Registry addon on with docker uses 55012 please use that instead of default 5000 . 棣冩啝 ------------------------------------------------------------------------------------------ 棣冩憣 For more information see : < URL > 閳?Using image registry : 2.7.1 閳?Using image gcr.io/google_containers/kube-registry-proxy:0.4 棣冩敺 Verifying registry addon ... 棣冨皞 The ' registry ' addon is enabled .  	1	1
0 0 1.2 2.0 1.1 1.5 1.1 1.0 1.3333333333333333 2.0 0.8225108225108225 231 1.0 2 3 4 22	  add beta releases to our website installation instructions . < URL >   	1	2
0 0 0.6 0.0 0.7 0.5 0.8 0.5 0.0 0.0 0.0 1 0.0 0 2 5 19	Unable to parse ' : Version string empty error . Minikube 1.6.2 fails to start : : $ minikube start 棣冩 [ operator-minikube ] minikube v 1.6.2 on Darwin 10.15.1 閳?MINIKUBE_HOME = /opt/minikube 閴?Selecting ' virtualbox ' driver from user configuration ( alternates : [ hyperkit ]) 棣冩寴 Unable to parse ' : Version string empty . Having set this config : : + PROFILE = operator-minikube + VMDRIVER = virtualbox + minikube profile operator-minikube 閴?open /opt/minikube/ . minikube/profiles : no such file or directory 閴?profile ' operator-minikube ' not found 閴?Created a new profile : operator-minikube 閴?minikube profile was successfully set to operator-minikube + minikube config set memory 4096 閳跨媴绗?These changes will take effect upon a minikube delete and then a minikube start + minikube config set cpus 4 閳跨媴绗?These changes will take effect upon a minikube delete and then a minikube start + minikube config set disk-size 10GB 閳跨媴绗?These changes will take effect upon a minikube delete and then a minikube start + [[ -n virtualbox ]] + echo ' Using VM driver ' /''virtualbox'/''' Using VM driver ' virtualbox ' + minikube config set vm-driver virtualbox 閳跨媴绗?These changes will take effect upon a minikube delete and then a minikube start . I've worked around by specifying the kubernetes version : : $ minikube start -- kubernetes-version = v 1.17.0 棣冩 [ operator-minikube ] minikube v 1.6.2 on Darwin 10.15.1 閳?MINIKUBE_HOME = /opt/minikube 閴?Selecting ' virtualbox ' driver from user configuration ( alternates : [ hyperkit ]) 棣冩崚 Downloading VM boot image ... .	2	0
0 0 0.6 0.0 0.8 0.5 1.05 1.0 0.6666666666666666 0.0 1.1649484536082475 97 1.0 3 6 10 22	Try to make the podman driver less ' experimental ' , for podman version 3.0 . There is a new version of podman being prepared : : v 3.0.0 . < URL > < URL > This is the release track that will go into next RHEL ( previously based on first 1.6.4 and then 2.0.5 ) < URL > < URL > Hopefully this means that the podman driver is finally stable enough to be made non-experimental . The main features are support for the docker socket and for docker compose , so nothing breaking ? Support for podman has already been merged , and both KIC and ISO are currently running : v 2.2.1 . From the < URL > post : We are anticipating a final release in several weeks . We will publish a second release candidate next Monday , and if no major bugs are identified , the 3.0.0 final release will be the subsequent Monday , February 1 .  	1	1
0 0 1.0 1.0 0.9 1.0 0.9 1.0 1.0 1.0 1.75 4 2.0 2 5 8 26	Add documentation for qemu uri on kvm2 driver . it would be nice to have a doc , showing minikube running with qemu uri . would be even better to have an example showing it working on the cloud . Related : < URL >  	2	2
1 1 1.2 1.0 0.9 1.0 0.7 0.5 1.3333333333333333 1.0 1.6666666666666667 3 2.0 0 0 1 4	kvm2 : VM forcibly paused and became unusable with minikube . minikube 0.31.0 driver : kvm2 No idea how the VM got in ' Paused ' state but minikube can't start or stop it since then : : $ minikube status minikube : Paused cluster : kubectl : $ minikube start Starting local Kubernetes v 1.10.0 cluster ... Starting VM ... E1219 14:48:14 . 263048 106732 start . go : 168 ] Error starting host : Error starting stopped host : Error creating VM : virError(Code = 55 , Domain = 10 , Message='Requested operation is not valid : domain is already running ' ) . Retrying . E1219 14:48:14 . 263711 106732 start . go : 174 ] Error starting host : Error starting stopped host : Error creating VM : virError(Code = 55 , Domain = 10 , Message='Requested operation is not valid : domain is already running ' ) $ minikube status minikube : Paused cluster : kubectl : $ minikube stop Stopping local Kubernetes cluster ... Error stopping machine : Error stopping host : minikube : stopping vm : virError(Code = 55 , Domain = 10 , Message='Requested operation is not valid : domain is not running ' ) $ minikube status minikube : Paused cluster : kubectl : .	2	0
0 0 1.4 2.0 1.7 2.0 1.35 2.0 1.3333333333333333 2.0 0.75 4 0.5 0 0 2 10	Driver execution errors are not shown to the user ( libmachine limitation ? ) . When starting from ChromeOS : : $ minikube start -- vm-driver = kvm2 Starting local Kubernetes v 1.10.0 cluster ... Starting VM ... E1002 02:32:51 . 769734 2219 start . go : 168 ] Error starting host : Error creating new host : dial tcp : missing address . Retrying . E1002 02:32:51 . 770952 2219 start . go : 174 ] Error starting host : Error creating new host : dial tcp : missing address . It looks the kvm driver has an error , but we fail to surface it : : Found binary path at /usr/local/bin/docker-machine-driver-kvm2 Launching plugin server for driver kvm2 () DBG | /usr/local/bin/docker-machine-driver-kvm2 : error while loading shared libraries : libvirt-lxc . so . 0 : cannot open shared object file : No such file or directory Plugin server listening at address .	0	0
2 2 1.2 1.0 1.4 1.5 1.45 2.0 1.0 1.0 0.0 0 0.0 3 3 5 43	profile name should not have ' _' . : Invalid value : ' vm_0_10_centos ' : a DNS-1123 subdomain must consist of lower case alphanumeric character .	0	0
1 1 1.4 2.0 1.2 1.0 1.3 1.5 1.6666666666666667 2.0 1.0 1 1.0 1 7 8 37	multinode fails conformance test with kindnet ( but calico works ) . Run the conformance test with default kindnet cni . Tests failed due to coreDNS can't resolve service name on the non-mater nodes . Run the same test for calico cni for both docker and containerd runtime . All passed . There are two things we could do : 1 . Change default cni to a more lightweight one which pass all the conformance tests . 2 . Investigate the kindnet dns issue . The attachment are the tests log for calico with docker and containerd runtime . < URL > < URL >	1	0
0 0 1.2 2.0 1.2 1.5 1.45 2.0 1.3333333333333333 2.0 0.6 5 0.0 1 5 8 30	Implement command to get cpu usage benchmark . /kind feature Currently , minikube has several driver choices . The drivers have each characteristics and they affect local machine's performance differently . Of course , low CPU load and high performance are desirable , but it is important to know each characteristics and performance values 閳ュ鈧獪f each drivers . In the past , Thomas took cpu usage benchmark using < URL > on September 2020 . < URL > This benchmark compared minikube's drivers and similer products(Kind , K3d , Docker for Desktop ) . Now if we'd like to take cpu usage benchmark , It takes long time and to summarize manually . So , in this issue , I'll aim to implement the : make XXXX . command to do to reduce mannual costs .	2	1
0 0 1.2 1.0 1.2 1.5 1.25 2.0 1.0 1.0 1.3 10 1.0 0 1 4 20	Show information about the linux distribution running . When provisioning a host with libmachine , or running locally with the none driver , it can be useful to show information about which linux distribution is running . There is a file in : systemd . , that contains this information : : /etc/os-release . . And libmachine already has functions to parse this and to retrieve it from host . < URL > We are probably just going to use the : PRETTY_NAME . parameter , for simplicity . Examples : ' Provisioned with Buildroot 2018.05.3 ' ' Running on Ubuntu 16.04.6 LTS ' We could show the kernel version ( : uname . ) as well , but GOOS just says ' linux ' . Examples : Linux 4.15.0 x86_64 Linux 4.4.0 -150-generic x86_64	0	1
2 2 1.0 1.0 1.2 1.0 1.1 1.0 1.0 1.0 0.8529411764705882 204 1.0 1 2 16 54	benchmark different image build/push ways in minikube . generate charts across 100 runs for docker-env , image load , and registry and podman-env addon for docker , containerd , crio and compare the build image and the time that image was availalbe inside minikube . for different docker files small layers , few small layers , a lot big layers , few big l ayers , a lot duplicate layers	0	1
0 0 1.4 2.0 1.2 1.5 1.1 1.0 1.3333333333333333 2.0 0.9575757575757575 165 1.0 6 7 9 40	add multinode integration test two pods should get two different ips on different nodes . add a subtest deploy an app with more than 1 replica on multi node their pods ips should be different < URL >	0	1
2 2 1.6 2.0 1.2 2.0 1.1 1.5 1.3333333333333333 2.0 1.1666666666666667 96 1.0 0 0 3 32	Add functional test for system versions of container engines . ( This is issue is about KIC on Linux ) Currently we are testing with the vendor versions of the container engines : docker.com < URL > podman.io < URL > But maybe we also need to test with the system versions , in the distributions : debian < URL > < URL > ubuntu < URL > < URL > fedora < URL > ( Provides : docker ) < URL > centos docker ( el7 : 1.13 ) podman ( el8 : 2.0 ) Or at least add some documentation , as to which ' variants ' are supported . Currently users expect to be able to install the packages from anywhere ... Note that this issue is about the container engine to be used for the node : Docker Podman It's not about the container runtime to be used inside the container node : Docker ( dockershim/cri-dockerd ) CRI-O containerd For those we still only support the vendor versions , as per < URL > . There are system versions in some other dedicated Kubernetes operating systems . But we don't use those , so it is up to each Kubernetes distribution to support them . Example is our own custom Buildroot distribution for the ISO , with system packages .  	1	1
2 2 1.2 2.0 1.1 1.0 1.3 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 0 0 0	inotify doesn't work on 9p filesystem mounts . Bug Report Minikube version : v 0.19.0 Environment : - OS : Ubuntu 16.04.2 LTS - VM Driver : kvm - ISO version : listed as 0.18.0 , but was built from master - Install tools : - Others : What happened : inotify does not trigger on mounted files . What you expected to happen : inotify works , allowing hot reload How to reproduce it ( as minimally and precisely as possible ): Minikube mount a directory : : minikube mount . : /mount-9p . Mount that into a container : : volumeMounts : - mountPath : /app/src name : src volumes : - name : src hostPath : path : /mount-9p/src . kubectl exec into that container , : apt-get install inotify-tools inotifywait -m /app/src/index . js . Edit that file on the host , and note that inotifywait never outputs anything . Note that the reverse works fine , touching the file from inside the container triggers inotify events on the host . Even more strange , touching the file from inside the container doesn't trigger inotify events in the container . Touching a file that isn't shared triggers events , but touching a shared file doesn't . Anything else do we need to know :	2	1
1 1 0.8 1.0 0.8 1.0 1.05 1.0 1.3333333333333333 1.0 1.125 8 1.0 0 3 6 31	kindnet CNI fails to connect pods to services on different nodes . Reproduction : : minikube -p coredns start -- nodes = 2 minikube kubectl -p coredns -- apply -f test/integration/testdata/multinodes/multinode-pod-dns-test . yaml minikube kubectl -p coredns -- get pods -- all-namespaces -o wide BB_WITH_CD='<copy busybox name on same node as coredns pod>' BB_NO_CD='<copy busybox name on diff node as coredns pod>' # Succeeds minikube kubectl -p coredns -- exec $BB_WITH_CD -- nslookup kubernetes.io # Fails minikube kubectl -p coredns -- exec $BB_NO_CD -- nslookup kubernetes.io . Related to #12208 . Upstream issue : < URL > Specifying : -- cni = calico . fixes this issue as far as I can tell , haven't tested other CNIs  	1	0
0 0 1.6 2.0 1.6 2.0 1.45 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 0 0 0	hyperkit clock falls behind : failed to write or validate certificate : the certificate is not valid yet . Minikube version : v 0.18.0 Environment : - OS : MacOS Sierra ( 10.12.4 ( 16E195 )) - VM Driver : xhyve - ISO version : v 0.18.0 What happened : When I wake my laptop after sleep , the clock of the Minikube VM lags behind . This causes problems with the registry credentials plugin , since AWS rejects credentials requests with an invalid timestamp . What you expected to happen : The clock should sync after computer wakeup . How to reproduce it ( as minimally and precisely as possible ): See above .	2	0
2 2 1.6 2.0 1.6 2.0 1.35 2.0 1.3333333333333333 1.0 1.6428571428571428 14 2.0 1 2 6 29	Site : add link to external tutorials based on minikube . There are many good blogs posts and tutorials that are based on minikube . @afbjorklund mentioned this in the slack channel : < URL > I create this issue to collect most interesting tutorials , and I would accept PRs that add links to these tutorials to somewhere in the site .  	2	2
0 0 1.4 2.0 1.3 1.5 1.25 1.5 1.0 1.0 0.0 0 0.0 2 2 6 54	none with containerd : make it possible to run without the `docker` CLI . Steps to reproduce the issue : This is the current behavior regarding initizliation with none driver with containerd runtime . : # minikube start -- driver = none -- container-runtime='containerd ' 棣冩 minikube v 1.18.1 on Ubuntu 20.04 ( amd64 ) 閴?Using the none driver based on user configuration 棣冦仐 Exiting due to PROVIDER_NONE_NOT_FOUND : The ' none ' provider was not found : exec : ' docker ' : executable file not found in $PATH 棣冩寱 Suggestion : Install docker 棣冩憣 Documentation : < URL > root @minikube : ~ # minikube start -h | grep provider root @minikube : ~ # minikube start -- driver = none -- container-runtime='containerd ' 棣冩 minikube v 1.18.1 on Ubuntu 20.04 ( amd64 ) 閴?Using the none driver based on user configuration 棣冦仐 Exiting due to PROVIDER_NONE_NOT_FOUND : The ' none ' provider was not found : exec : ' docker ' : executable file not found in $PATH 棣冩寱 Suggestion : Install docker 棣冩憣 Documentation : < URL > . Since Docker is no longer default runtime , I think even none driver should consider the runtime option if specified . Full output of failed command : Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command :  	1	0
0 0 1.2 1.0 1.3 1.0 1.2 1.0 1.0 1.0 0.5454545454545454 11 0.0 0 2 7 43	Set the -- force-systemd true or false automatically ( by detecting the cgroups ) . Look into if we should be setting : -- force-systemd = true . by default , and if this results in any performance improvement documentation says we need to use same as your system if your system uses systemd , you should use systemd	2	1
2 2 0.8 0.0 1.0 1.0 0.9 1.0 0.6666666666666666 0.0 0.0 0 0.0 2 3 7 23	mounts using go9p report 0 byte capacity ( breaking Persistent Local Volumes ) . When using OSX and mounting directories into Minikube , the capacity/size of the mounts is reported as 0 by df : [ x ] How to replicate the error , including the exact command-lines used . mkdir ~ /mymount minikube start minikube mount ~ /mymount : /data/mymount & minikube ssh df /data/mymount [ x ] The full output of the command that failed df /data/mymount/ Filesystem 1K-blocks Used Available Use% Mounted on 192.168.99.1 0 0 0 - /data/mymount The background is that I'm trying to use Persistent Local Volumes with Minikube and OSX , with the static provisioner to create the PVs . This provisioner fails to create the PVs because the capacity of the mounts is seen by Minikube as being 0 . ( StackOverflow link to issue here : < URL > I get the same error if I use : : minikube start -- mount -- mount-string='~/mymount:/data/mymount ' . [ x ] The operating system name and version used OSX Mojave ( 10.14.3 ) Minikube v 0.34.1 Kubernetes v 1.13.3	2	0
2 2 1.4 2.0 1.5 2.0 1.2 1.0 2.0 2.0 0.0 0 0.0 1 1 6 28	none driver doesn't start if /bin/bash doesn't exist . . Command : : minikube -- logtostderr start -p sentinel -- vm-driver none . Output : < URL > Minikube logs : < URL > My os-release : < URL > In NixOS , there is no /bin/bash so starting the none driver fails to function . A simple fix for this error would be to use /usr/bin/env to invoke bash by following the path rather than assuming that it's hard coded , but I'm not sure what other assumptions that might break . This solution should work because NixOS ensures a symlink exists at /usr/bin/env that points at the executable and most distributions that I'm aware of ship with /usr/bin/env . If that sounds like a reasonable solution , I can work on figuring out a pull request ( unless someone can get to it faster ) .	2	0
2 2 1.0 1.0 1.3 2.0 1.15 1.5 1.0 1.0 0.0 0 0.0 0 0 0 3	read logs via kubectl when journald log-driver is configured . This is a Feature Request Currently running : minikube version : v 0.24.1 ' DriverName ' : ' virtualbox ' , ' Boot2DockerURL ' : ' file :// /Users/me/ . minikube/cache/iso/minikube-v 0.23.6 . iso ' , What happened : Unable to read logs via : kubectl logs . with journald log-driver configured . Error message is : : Error response from daemon : configured logging driver does not support reading . What you expected to happen : Logs would be able to be read via : kubectl logs . [ note that they are available via journalctl ] How to reproduce it ( as minimally and precisely as possible ): : minikube start -- docker-opt log-driver = journald . Then launch a pod that has data go to stdout and issue : kubectl logs $pod . : kubectl logs foo-76f44c5545-mbqqd Error response from daemon : configured logging driver does not support reading .	2	1
2 2 1.8 2.0 1.6 2.0 1.5 2.0 2.0 2.0 0.0 0 0.0 1 5 11 60	docker-env with crio : Error getting service status : Process exited with status 3 . The exact command to reproduce the issue : Make sure you're running Docker on Mac and run in a terminal : : $ minikube start -- cpus = 4 -- memory = 4g -- container-runtime = cri-o -- vm-driver = virtualbox $ minikube docker-env . I guess it is due to the use of VirtualBox ? With hyperkit docker-env works , but hyperkit has some issues with the network . The full output of the command that failed : : $ minikube docker-env 棣冩寴 Error getting service status : Process exited with status 3 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > . The output of the : minikube logs . command : There is none regarding this issue . The operating system version : macOS 10.14.6 Docker 19.03.8 minikube 1.8.2 VirtualBox 6.0.18	2	0
0 0 1.0 1.0 1.3 2.0 1.2 1.5 1.0 1.0 0.0 0 0.0 1 6 10 27	  Add `minikube unpause` next to `minikube pause` in documentation . Under Manage Cluster at < URL > it would be nice to reference minikube unpause in conjunction with minikube pause .	0	2
0 0 1.0 1.0 1.0 1.0 1.0 1.0 0.3333333333333333 0.0 1.0 1 1.0 0 1 8 24	Add Signing Process for Windows Installer . As of now , whenever the minikube installer is run , the following screen comes up which makes it look as if the executable is malicious to a user - This also revolves around having infrastructure/process to sign the executables/installers so that they can be verified . Not sure if we need to have for other operating systems as well . -Pranav	2	1
2 2 1.2 1.0 1.2 1.0 1.0 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 4 27	kvm2 driver : check if user in ' libvirt ' group and exit if they are not . Please , add that user starting minikube should be in ' libvirt ' group . Because without adding user in ' libvirt ' group will be an error ' authentification unavailable '	2	1
0 0 1.0 1.0 1.1 1.0 1.05 1.0 1.0 1.0 0.8865979381443299 194 1.0 1 3 9 47	auto-pause : make it memory-leak proof . each call of : time . After . creates a new timer instance which is not released until the timer fired . If we have high traffic in : incomeCh . we'll end up with a lot of pending timers < URL > Originally posted by @ilya -zuyev in < URL > < URL >	0	1
2 2 1.2 1.0 1.1 1.0 1.1 1.0 1.0 1.0 0.0 0 0.0 1 3 8 29	reuse vbox : ' minikube start ' failed at ' Waiting for pods : apiserver ' ( timeout finally ) . Environment : macOS Mojave 10.14.4 , VirtualBox 6.0.6 $ minikube start 棣冩 minikube v 1.0.0 on darwin ( amd64 ) 棣冦仚 Downloading Kubernetes v 1.14.0 images in the background ... 棣冩寱 Tip : Use ' minikube start -p ' to create a new cluster , or ' minikube delete ' to delete this one . 棣冨籍 Re-using the currently running virtualbox VM for ' minikube ' ... 閳?Waiting for SSH access ... 棣冩懕 ' minikube ' IP address is 192.168.99.100 棣冩儞 Configuring Docker as the container runtime ... 棣冩儞 Version of container runtime is 18.06.2 -ce 閳?Waiting for image downloads to complete ... 閴?Preparing Kubernetes environment ... 棣冩 Pulling images required by Kubernetes v 1.14.0 ... 棣冩敡 Relaunching Kubernetes v 1.14.0 using kubeadm ... 閳?Waiting for pods : apiserver 棣冩寴 Error restarting cluster : wait : waiting for component = kube-apiserver : timed out waiting for the condition 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > 閴?Problems detected in ' kube-addon-manager ' : error : no objects passed to apply error : no objects passed to apply error : no objects passed to apply	0	0
2 2 1.0 1.0 1.1 1.5 1.25 1.5 1.0 1.0 0.0 0 0.0 0 0 0 1	hyperv : Default to disabling dynamic memory ( pass -- hyperv-disable-dynamic-memory to driver ) . I keep running into issues the first time I do minikube start with kubeadm and hyper-v . It seems to be releated to dynamic memory . I can stop things , disable dynamic memory , then restart but I think it's messed up by then . I'm tyring to work though it now .  	1	1
2 2 1.4 2.0 1.2 1.5 1.4 2.0 1.6666666666666667 2.0 1.121212121212121 66 1.0 0 3 9 57	Make website documentation versioned . we recently updated docs that -- vm-driver to -- driver and that was true for head ! but not for the binary that people donwload and use . so we need versioned documentation for HEAD and also for releases .  	2	2
2 2 1.0 1.0 0.9 1.0 1.15 1.0 1.6666666666666667 2.0 0.8333333333333334 6 0.5 0 1 2 11	listpullreqs . go ( release notes generator ) misses PR's merged during a release . The 0.30.0 release notes missed PR #3148 - which was merged after v 0.29.0 was built , but before the github release was created . The bug can be seen here : : lastReleaseTime : = * releases[0 ] . PublishedAt fmt . Println(fmt . Sprintf('Collecting pull request that were merged since the last release : %s (%s )' , * releases[0 ] . TagName , lastRelease Time )) .	2	0
2 2 1.4 2.0 1.5 2.0 1.15 1.5 1.3333333333333333 2.0 1.6 20 2.0 1 2 5 32	fix minikube service retry logic ( takes 35s ) . : minikube service list . takes 0.138 s ( less than a 1 second ) see : : $ time minikube service list | ----------------------|--------------------------- | --------------|----- | | NAMESPACE | NAME | TARGET PORT | URL | | ----------------------|--------------------------- | --------------|----- | | default | kubernetes | No node port | | kube-system | kube-dns | No node port | | kubernetes-dashboard | dashboard-metrics-scraper | No node port | | kubernetes-dashboard | kubernetes-dashboard | No node port | | ----------------------|--------------------------- | --------------|----- | real 0m 0.164 s user 0m 0.068 s sys 0m 0.065 s . but if you try to access a Not-existing service : time minikube service newservice -n unknown -- url real 0m 35.885 s user 0m 0.115 s sys 0m 0.092 s . it takes 35 seconds ! that is a flaw ! we could easily search for the list in list of services first and then if it exists go through the Retry logic . I also noticed the : service . package needs a lot of clean up and refactor !	2	1
2 2 1.4 2.0 1.1 1.5 1.0 1.0 1.3333333333333333 2.0 1.0289855072463767 69 1.0 0 0 4 16	Move triage documentation to official website . < URL >  	2	2
0 0 0.4 0.0 0.7 1.0 0.75 1.0 0.3333333333333333 0.0 0.953125 128 1.0 1 1 6 37	Add -- delete-on-failure integration test if existing config references unusable driver . We need an integration test to assert that -- delete-on-failure properly handles the following scenario : Existing config references a driver that's been uninstalled or is otherwise unavailable minikube should delete the cluster and pick a new default driver  	1	1
1 1 1.2 1.0 1.1 1.0 1.15 1.0 1.0 1.0 0.0 0 0.0 0 0 0 0	kubelet and docker fail to start up again after VM restart . Author of < URL > here ; it would streamline my testing enormously if , after the minikube VM is rebooted via : systemctl reboot . , kubelet etc came back up automatically without having to manually run : minikube start . again from the host . Is there a reason it doesn't do that , other than there's not really been a need for it previously ?  	1	0
1 1 0.2 0.0 0.8 1.0 0.7 0.0 0.3333333333333333 0.0 0.8394495412844036 218 1.0 1 4 9 27	minikube image build test Flake . example < URL >	0	0
0 0 1.2 2.0 0.8 0.5 1.1 1.0 0.6666666666666666 0.0 0.5 24 0.0 6 7 12 35	Add timeout to `minikube delete` . If we haven't successfully deleted our cluster in 3 minutes , move on to the next cluster Hanging on : minikube delete . is a poor experience for the user and has been forcing our integration tests to timeout .	0	1
1 1 1.2 1.0 1.0 1.0 1.1 1.0 1.0 1.0 2.0 1 2.0 0 1 2 17	Add support for qemu :// /session ( currrent error : no network with matching name ' default ' ) . Goal Support QEMU/KVM User Session as a backend , to enable a rootless Minikube . QEMU/KVM is already supported , but User Session isn't . Since QEMU/KVM User Session doesn't require root , this would be the simplest solution for a local rootless cluster .	2	1
1 1 0.2 0.0 0.7 0.0 0.9 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 3 7 20	kvm2 driver : Allow use of storage pools ( to allow storing disks on ZFS ) . < URL > the kvm2 driver uses an unconfigurable file path inside the : MINIKUBE_HOME . as the persistent storage . Please consider allowing the use of libvirt storage pools . This would allow use of ZFS zvol block devices as the storage backend for Minikube . < URL > .	2	1
2 2 1.6 2.0 1.0 1.0 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 3 10 20	vhost-net support in minikube iso for kubevirt . i d like to have vhost-net support in minikube iso as it's needed in kubevirt in order to use minikube	2	1
2 2 1.2 2.0 1.0 1.0 1.15 1.0 1.3333333333333333 2.0 2.0 1 2.0 0 2 7 22	minikube status should show status for all profiles . minikube status shows ' Stopped ' when starting a minikube in a different profile . : $ minikube start -p p1 $ minikube status host : Stopped kubelet : apiserver : kubectl : $ minikube status -p p1 host : Running kubelet : Running apiserver : Running kubectl : Correctly Configured : pointing to minikube-vm at 192.168.99.132 . To recreate : - Start minikube default profile : minikube start . - Stop minikube : minikube stop . - Start minikube in a different profile : minikube start -p p1 . - Get miniube status : minikube status . Expected : I would like minikube status , show status of all profiles , if I don't provide a profile name .  	1	1
2 2 0.8 1.0 1.1 1.0 1.25 1.5 1.3333333333333333 1.0 1.1139240506329113 79 1.0 3 6 9 45	hyperkit driver should be happy with current minimum verison . we have not updated the hyperkit driver in a year and we still each time we update minikube we redownload it and we ask user to give sudo persmiion for the user group . currnetly with each new version of minikube , we re-download the hyperkit driver . even if the code for it was not touched . the hyperkit driver udpate should not download the driver if the checksum is not changed . simmilar to preload or other things in minikube that do not redownload .	0	1
0 0 0.6 1.0 0.9 1.0 0.85 1.0 0.3333333333333333 0.0 0.0 0 0.0 2 3 3 18	minikube tunnel should have background mode . It would be useful if Minikube tunnel had a background/daemon mode . It would also be nice if it could be started whenever Minikube starts without having to do anything .	2	1
2 2 1.6 2.0 1.4 2.0 1.3 2.0 2.0 2.0 1.0256410256410255 39 1.0 2 3 4 28	Add support for VM-free deployments using Docker . This will give a much better user experience for folks in nested VM's than the ' none ' driver currently provides . If possible , this should be done in collaboration with the fine folks at < URL >  	1	1
2 2 1.2 1.0 1.2 1.0 1.25 1.5 1.3333333333333333 1.0 1.5555555555555556 9 2.0 2 2 5 26	hyperv : GetVMHostIP fails if default switch is used . The exact command to reproduce the issue : : . /minikube-windows-amd64 . exe start -- vm-driver = hyperv -- alsologtostderr -- v = 9 . The cluster starts fine but when I was trying to refactor the CIFS mounts on Windows + Hyper-V , I started getting - : PS C : /utilities > . /minikube-windows-amd64 . exe mount -- type = cifs ' E :/ : /home/docker/yay ' -- alsologtostderr -- v = 9 W1020 23:09:03 . 205756 18752 root . go : 240 ] Error reading config file at C : /Users/bluee/ . minikube/config/config . json : open C : /Users/bluee/ . minikube/config/config . json : The system cannot find the file specified . W1020 23:09:03 . 214757 18752 exit . go : 101 ] Error getting the host IP address to use from within the VM : ip for interface (): Error finding IPV4 address for vEthernet () * X Error getting the host IP address to use from within the VM : ip for interface (): Error finding IPV4 address for vEthernet () * * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : . After checking , the config file inside : machines/minikube . , I saw that the : VSwitch . field is empty . Meanwhile , alternatively , I can specify the cluster start like this and it will start fine - : . /minikube-windows-amd64 . exe start -- vm-driver = hyperv -- alsologtostderr -- v = 9 -- hyperv-virtual-switch='Default Switch ' . /assign  	1	0
1 1 1.0 1.0 1.4 1.5 1.0 1.0 0.6666666666666666 1.0 1.042857142857143 70 1.0 1 1 5 17	Use a CNI by default in minikube ( flannel ? ) . For consistency and to prepare for multi-node .  	1	1
2 2 1.2 1.0 1.1 1.0 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 0 0 1 4	cache add : Error caching and loading images : Process exited with status 1 . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG Please provide the following details : Environment : : - Minikube version : v 0.25.0 - OS : Windows 10 - VM Driver : virtualbox - ISO version : v 0.25.1 - Install tools : - Others : . What happened : When I run : minikube cache add ubuntu : 16.04 . it gives following error : - : Error caching and loading images : loading cached images : loading image C : /Users/Abhinav Chittora/ . minikube/cache/images/ubuntu : loading docker image : /tmp/ubuntu : Process exited with status 1 . What you expected to happen : It should cache the given image . How to reproduce it ( as minimally and precisely as possible ): Install minikube , run : minikube start . and run the command : minikube cache add ubuntu : 16.04 . Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	0
2 2 1.2 2.0 1.0 1.0 1.25 1.0 1.3333333333333333 2.0 0.0 0 0.0 2 4 8 32	Windows 10 + VirtualBox + Minikube CPU optimization . I stumbled upon that VirtualMachinePlatform-Feature-Related causes more CPU useless computing when I tried < URL > . Even if this feature is not enabled , it can result in kube-apiserver taking up 5 to 10 times more CPU resources than normal . You need to manually enable and disable it again , and the CPU will return to normal . After some commands and reboots , I found that my CPU was green again : : # windows powershell Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform Disable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform .  	2	2
2 2 0.6 0.0 0.9 1.0 1.05 1.0 1.0 1.0 1.5 2 1.5 1 2 7 24	Generate performance metric for minikube start/stop/delete speed on different drivers . it would be a great thing to have a script to run on each release , that does the following : measure how long minikube start , start again , stop , delete takes on each different drivers such as , virtualbox , hyperkit , hyperv and none . the script should generate the a dirver_performance . md file in the /docs automatically on each release and purpose a PR . The script could run as an integration test . the same way that our e2e tests run .	2	1
2 2 0.8 0.0 1.0 1.0 1.05 1.0 0.6666666666666666 0.0 0.896551724137931 29 1.0 4 5 9 32	Add fast-path for restarting a cluster when the configuration is unchanged . If the specified configuration is identical to the current configuration : If VM is running , skip setup and block until pods are healthy . This should be instantanteous . If VM is not running , start VM , reconfigure any IP's necessary , and block until pods are healthy . 3293 was trying to solve this , but the logic to assert that the configuration was the same was never implemented . I feel that this would be a huge performance benefit for our users .	2	1
1 1 0.8 1.0 1.3 1.0 1.2 1.0 1.0 1.0 1.0 13 1.0 0 2 3 24	Provide tarballs for releases . In addition to the currently existing binaries , we should provide tarballs for smaller downloads . Doesn't have to be anything fancy , just add . tar . gz with the same files . Kubernetes does this .	2	1
1 1 1.6 2.0 1.5 2.0 1.55 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 2 5	-- nfs-share : Don't start if there is export rule with subnet . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.27.0 - OS ( e.g. from /e	2	0
0 0 1.2 2.0 0.9 1.0 0.95 1.0 1.3333333333333333 2.0 1.1081081081081081 74 1.0 1 3 9 53	Run StopContainers in parallel . for kic and none , we stop the containers on stop . we can speed up the stop processes by doing this in parallel : : // StopContainers stops a running container based on ID func ( r * Docker ) StopContainers(ids [] string ) error { if len(ids ) = = 0 { return nil } glog . Infof('Stopping containers : %s ' , ids ) args : = append([]string{'stop ' } , ids ... ) c : = exec . Command('docker ' , args ... ) if _ , err : = r . Runner . RunCmd(c ); err ! = nil { return errors . Wrap(err , ' docker ' ) } return nil } .  	1	1
2 2 1.6 2.0 1.4 2.0 1.3 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 4 4 16	minikube docker-env incorrectly recognizes shell type in PowerShell 7.1.5 . Steps to reproduce the issue : Open PowerShell 7.1.5 Run : minikube docker-env . Minikube displays : : SET DOCKER_TLS_VERIFY = 1 SET DOCKER_HOST = tcp :/ / 192.168.98.100 : 2376 SET DOCKER_CERT_PATH = C : /Users/Wojtek/ . minikube/certs SET MINIKUBE_ACTIVE_DOCKERD = minikube REM To point your shell to minikube's docker-daemon , run : REM @FOR /f ' tokens=*' %i IN (' minikube -p minikube docker-env ' ) DO @%i . as if it was run in CMD instead of Powershell . If the : minikube docker-env . is started in Windows 10 built-in PowerShell it works correctly . If I execute : & minikube -p minikube docker-env | Invoke-Expression . command in PowerShell 7.1.5 it returns error : : You : The term ' You ' is not recognized as a name of a cmdlet , function , script file , or executable program . Check the spelling of the name , or if a path was included , verify that the path is correct and try again . Invoke-Expression : Cannot bind argument to parameter ' Command ' because it is an empty string . Invoke-Expression : Line | 1 | & minikube -p minikube docker-env | Invoke-Expression | ~~~~~~~~~~~~~~~~~ | The string is missing the terminator : ' . . The same command in built-in Powershell works without flaws . Temporary workaround is pointing to the correct version of shell : : & minikube -p minikube docker-env -- shell powershell | Invoke-Expression .	0	0
0 0 0.6 0.0 0.6 0.0 0.75 0.5 0.0 0.0 0.0 1 0.0 1 1 3 26	kicbase/entrypoint : zfs workaround is broken . < URL > The zfs workaround above is broken , because : /etc/containerd/config . toml . is overriden by : pkg/minikube/cruntime . Containerd . Enable () . < URL >	0	0
2 2 1.4 2.0 1.7 2.0 1.5 2.0 1.0 1.0 1.608695652173913 23 2.0 2 2 5 17	Performance Test : measure start a stopped minikube . while working on this PR < URL > I noticed my mprovements might have added 10 seconds to minikube ' re-start ' but to verify I had to do manual testing . I purpose we measure , how a PR affects miikube start after a stop . to do that : - minikube start ; measure as before - minikube stop measure stop also - minikube start measure the start existing  	1	1
1 1 1.4 2.0 1.5 2.0 1.2 2.0 1.6666666666666667 2.0 1.0526315789473684 38 1.0 1 2 3 28	  Migrate user docs to Hugo-based site . The minikube docs directory has been good enough up until now , but it lacks structure and isn't localizable . See request for sub-project website here : < URL >	0	2
1 1 1.4 1.0 1.4 1.0 1.45 2.0 1.3333333333333333 1.0 1.0 1 1.0 1 3 9 44	-- help : bool flags which default to ' false ' yield confusing output . Steps to reproduce the issue : minikube service -- help Take notice of : options : -- https = false . . The description of this option doesn't make sense . If : -- https . is a bool 	2	0
2 2 0.6 0.0 0.9 0.5 1.05 1.0 1.0 1.0 0.0 0 0.0 1 2 9 19	none : minikube should be able to be run by a non-root user ( use sudo when necessary ) . If this is a bug report , please include : I tried starting minikube with the following command minikube start -- vm-driver = none after downloading minikube v 0.34.1 from this command curl -Lo minikube < URL > && chmod +x minikube && sudo cp minikube /usr/local/bin/ && rm minikube I got the output that minikube crashed I am on ubuntu 18.04 LTS machine   	1	1
0 0 1.6 2.0 1.3 2.0 1.2 1.5 1.3333333333333333 2.0 0.96 100 1.0 1 10 20 73	Make Hyper-V usable by members of Hyper-V admins group . This would require moving from : Get-WindowsOptionalFeature . to : HypervisorPresent . , as mentioned here : < URL > @gbraad mentioned that perhaps we should just call all of the checks here before trying to use the Hyper-V driver .	0	0
1 1 1.6 2.0 1.3 1.5 1.55 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 1 6	Document ' none ' driver behavior of overwriting kubeadm . BUG REPORT : Please provide the following details : Environment : Ubuntu 18 Minikube version ( use : minikube version . ): v 0.31.0 - OS ( e.g. from /etc/os-release ): Ubuntu - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): none - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): - Install tools : - Others : What happened : I already have version 1.13 of kubeadm in my $PATH . specifically /snap/bin/kubeadm , because of ubuntu snap packages . However , minikube insists on downloading a raw binary of kubeadm .. 1.10 and putting it in /usr/bin Even if I have a SYMLINK /usr/bin/kubeadm -> /snap/bin/kubead , minikube blows it away and puts the older binary there . What you expected to happen : Don't download anything ?? How to reproduce it ( as minimally and precisely as possible ): minikube start Output of : minikube logs . ( if applicable ) : Anything else do we need to know :  	2	2
0 0 1.0 1.0 1.0 1.0 0.95 1.0 0.6666666666666666 0.0 2.0 3 2.0 1 8 14 52	Trouble setting up minikube in Alpine Linux . Can you please help me with installing minikube on Alpine Linux ? I am following instruction from < URL > Steps to reproduce the issue : Have docker installed ( < URL > Install curl : sudo apk add curl . Download minikube : curl -LO < URL > . Add minikube to path : sudo install minikube-linux-amd64 /usr/local/bin/minikube . Execute any command with minikube for example : : minikube . or : minikube start . or : minikube start -- driver = docker . Full output of failed command : : /usr/local/bin/minikube : line 1 : ELF : not found /usr/local/bin/minikube : line 2 : @$ : not found /usr/local/bin/minikube : line 1 : % : not found /usr/local/bin/minikube : line 3 : can't open : no such file /usr/local/bin/minikube : line 1 : % : not found /usr/local/bin/minikube : line 4 : o%X : not found /usr/local/bin/minikube : line 5 : x not found /usr/local/bin/minikube : line 5 : Y渚?: not found /usr/local/bin/minikube : line 6 : syntax error : unexpected ' )' /usr/local/bin/minikube : line 5 : xYY : not found . Full output of : minikube start . command used , if not already included : Completely same as above Optional : Full output of : minikube logs . command : Completely same as above	2	2
1 1 0.8 1.0 1.0 1.0 1.05 1.0 0.6666666666666666 1.0 0.25 8 0.0 0 1 3 16	Multi-node label lost on minikube restart . Received feedback that this is an issue . Need to verify this is indeed an issue and then fix it .   	1	0
2 2 2.0 2.0 2.0 2.0 1.55 2.0 2.0 2.0 2.0 1 2.0 1 4 15 33	Not all parameters available in ' minikube start ' have a corresponding setting in ' minikube config ' , and for those that do , some don't seem to work . The problem in this issue was brought up by me in < URL > but as requested I am opening a new issue for that . When trying to replace a ' minikube start ' command with a lot of params with a simple ' minikube start ' ( i.e. , without params ) plus setting ' minikube config ' accordingly , I noticed that not all parameters available in ' minikube start ' have corresponding settings in ' minikube config ' , e.g. -- extra-config -- apiserver-{name|names|port|ips } ( Not that I stictly needed those for my use case , but someone might ... ) Further , some settings that do exist in ' minikube config ' don't seem to work , e.g. , : $ minikube config set memory 12G * X Set failed : [ memory : strconv . Atoi : parsing ' 12G ' : invalid syntax ] . while : -- memory 12g . worked fine . Same problem with : $ minikube config set insecure-registry myinsecureregistry : 12345 * X Set failed : [ Cannot enable/disable invalid addon insecure-registry ] . As before , the corresponding ' minikube start ' switch : minikube config set insecure-registry myinsecureregistry : 12345 . works . Maybe I am using ' minikube config ' incorrectly , but other settings ( cpus , disk-size ) worked when used that way , e.g. $ minikube config set disk-size 30G ! These changes will take effect upon a minikube delete and then a minikube start $ minikube config set cpus 4 ! These changes will take effect upon a minikube delete and then a minikube start  	1	1
2 2 1.6 2.0 1.3 1.5 1.25 1.5 2.0 2.0 0.0 0 0.0 5 10 20 56	kic : minikube tunnel won't work if a node reuses an ip existing on ~ / . ssh/known_hosts . The PR that removed the need to ask permission when creating the ssh tunnel , created a side effect . If the user created a cluster before , and docker used the same IP for that container , the new cluster won't be able to create the tunnel .	0	0
1 1 1.8 2.0 1.5 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 1 17	Cannot enable ingress add-on with K8s v 1.18.20 and Minikube v 1.23.2 . With Minikube v 1.23.2 running on macOS v 11.6 installed via homebrew ( commit : 0a0ad764652082477c00d51d2475284b5d39ceed ) , I get the following error when I try to enable the ingress add-on with an old version of Kubernetes : minikube start -- kubernetes-version = v 1.18.20 minikube addons enable ingress : error : unable to recognize ' /etc/kubernetes/addons/ingress-deploy . yaml ' : no matches for kind ' IngressClass ' in version ' networking.k8s.io/v1 ' . I don't think that : networking.k8s.io/v1 . is part of the v 1.18.2 k8s API , which may be the source of the issue ?	0	0
2 2 1.2 1.0 1.2 1.5 1.05 1.0 1.0 1.0 0.0 0 0.0 1 1 3 18	VMWare driver mount broken in 1.23 . Upgraded to 1.23 and driver mounts in VMWare Fusion appear to be broken . The : /mnt/hgfs . folder is entirely missing , and : vmhgfs-fuse . utility is missing . Steps to reproduce the issue : : minikube start -- iso-url = < URL > -- driver = vmware . : minikube ssh . : ls /mnt/hgfs . returns : No such file or directory . : vmhgfs-fuse . doesn't exist in : /usr/bin/ . Exists and works in v 1.22 . While this seems potentially related to < URL > (/cc @afbjorklund ) , it's not obvious to me why : vmhgfs-fuse . went missing . : BR2_PACKAGE_OPENVMTOOLS = y . is still set , so perhaps it's something in < URL > or < URL > itself . I'm not able to successfully build the ISO on macOS otherwise I'd investigate further .   	1	0
2 2 1.2 1.0 1.5 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 11 15 40	Accessing apps documentation issue . Section from Documentation : Getting the NodePort using the service command : minikube service -- url $SERVICE . Problem : It's not clear from the documentation how to use this command . After experimentation , I believe the usage would look something like this : : minikube service -- url hello-minikube1 . Why is this confusing ? Because in the documentation in the section just before , the convention is to use : <> . tags to represent a dynamic value , example : : minikube addons enable < name > . There should be one convention for representing dynamic values . I'm happy to raise a PR with these changes if you agree with my reasoning  	2	2
2 2 1.2 2.0 0.9 0.5 1.05 1.0 2.0 2.0 0.5555555555555556 18 0.0 1 4 9 33	Add -- cancel-scheduled-stop flag to cancel all existing scheduled stops . Related to < URL >	0	1
1 1 0.8 1.0 0.9 1.0 0.95 1.0 1.0 1.0 0.0 0 0.0 0 2 3 27	Dashboard does not show CPU or memory usage . why i cant see any cpu or memory usage graph on dashboard ?	2	0
2 2 1.2 2.0 1.4 2.0 1.1 1.0 2.0 2.0 1.1372549019607843 102 1.0 3 4 16 56	  Document how to use BuildKit remotely , for building images . Currently we only have instructions on how to use it locally . : sudo buildctl build . We should also add instructions on how to to use it remotely . : buildctl -- addr unix :/ /buildkitd . sock build . Unfortunately it is currently still too complicated for a : buildkit-env . There is no support for : ssh :// . addresses , and no systemd support . But there are manual workarounds meanwhile , like a ssh : -L . tunnel The daemon also needs to be started manually , with the right config .	0	2
0 0 1.6 2.0 1.5 2.0 1.5 2.0 1.3333333333333333 2.0 2.0 2 2.0 2 4 5 25	`minikube kubectl -- profile` does not switch context . Steps to reproduce the issue : Provision multiple minikube clusters , e.g. cluster-a & cluster-b Change context to cluster-a with : kubectl config use-context cluster-a . Get namespaces in cluster-b with : minikube kubectl -- profile cluster-b -- get ns . , context isn't switched from cluster-a to cluster-b and cluster-a namespaces are shown . The reason is that < URL > doesn't consider : -- profile . . Full output of failed command : N/A Full output of : minikube start . command used , if not already included : N/A Optional : Full output of : minikube logs . command :	0	0
1 1 1.4 2.0 1.2 1.0 1.2 1.0 1.0 1.0 0.0 0 0.0 3 11 28 59	feature request : mount host volumes into docker driver via `docker -v` . Steps to reproduce the issue : minikube config set vm-driver docker minikube start This is a feature request ; there's no failed command here to document . That said : It would be really nice if the docker vm-driver allowed mapping paths from the local filesystem into the minikube container , using docker's volume binding functionality . E.g. , : minikube config set vm-driver docker minikube start -- docker-volume = /home/ptramsey : /home/ptramsey . ... which would internally call something like , : docker run -v /home/ptramsey : /home/ptramsey gcr.io/k8s-minikube/kicbase:v0.0.8 /usr/local/bin/entrypoint . As it is , despite running without virtualization on linux ( which has a noticeable performance advantage ) , we're stuck using nfs or 9p to mount host paths inside minikube , both of which are slower ( the latter of which is much slower ) than just mapping the path directly via docker .  	1	1
0 0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0 0.0 1 5 10 28	mac m1 minikube start error . Steps to reproduce the issue : brew install minikube minikube start Full output of : minikube logs . command : Exiting due to GUEST_PROVISION : Failed to start host : recreate : creating host : create : creating : setting up container node : preparing volume for minikube container : docker run -- rm -- name minikube-preload-sidecar -- label created_by . minikube.sigs.k8s.io=true -- label name.minikube.sigs.k8s.io=minikube -- entrypoint /usr/bin/test -v minikube : /var gcr.io/k8s-minikube/kicbase:v0.0.20@sha256:0250dab3644403384bd54f566921c6b57138eecffbb861f9392feef9b2ec44f6 -d /var/lib : exit status 125 stdout : Full output of failed command : docker : Error : error contacting notary server : unknown : Project ' project : gcr.io:k8s-minikube ' not found or deleted . See ' docker run -- help ' .	2	0
1 1 0.6 1.0 0.9 1.0 0.95 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 5 5 19	Corporate firewall issues . Our corporate firewall only allows access to dockerhub . I have tried the standard gcr as well as the chinese alternative . : 棣冩暉 Creating podman container ( CPUs = 2 , Memory = 292500MB ) ... 棣冦亞 StartHost failed , but will try again : creating host : create : creating : setting up container node : preparing volume for minikube container : sudo -n podman run -- rm -- name minikube-preload-sidecar -- label created_by . minikube.sigs.k8s.io=true -- label name.minikube.sigs.k8s.io=minikube -- entrypoint /usr/bin/test -v minikube : /var gcr.io/k8s-minikube/kicbase:v0.0.26 -d /var/lib : exit status 125 stdout : stderr : Trying to pull gcr.io/k8s-minikube/kicbase:v0.0.26 ... Get ' < URL>': dial tcp 173.194.76.82 : 443 : i/o timeout Error : Error initializing source docker :/ / gcr.io/k8s-minikube/kicbase:v0.0.26 : error pinging docker registry gcr.io: Get ' < URL>': dial tcp 173.194.76.82 : 443 : i/o timeout . any chance the PowerPC images can be uploaded to dockerhub ?	2	1
2 2 0.8 0.0 0.9 0.5 0.9 1.0 0.6666666666666666 0.0 0.0 0 0.0 5 9 12 32	gluster link is 404 . This link is broken : < URL > in this page : < URL >  	2	2
0 0 1.2 1.0 1.5 2.0 1.45 2.0 1.3333333333333333 2.0 1.5833333333333333 24 2.0 1 3 4 18	improve message when trying to cache non existing image . when I try to cache an image that doesn't exist I get a crash : : $ . /out/minikube cache add busyboxy E1210 12:03:49 . 628653 96092 cache_images . go : 82 ] CacheImage busyboxy -> /Users/medya/ . minikube/cache/images/busyboxy failed : UNAUTHORIZED : ' authentication required ' 棣冩寴 Failed to cache and load images : caching images : caching image /Users/medya/ . minikube/cache/images/busyboxy : UNAUTHORIZED : ' authentication required ' 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > . we could be nicer and say , we couldn't find the image in all the auhtoirzed registeries we have access to  	1	0
2 2 1.6 2.0 1.7 2.0 1.3 2.0 2.0 2.0 0.0 0 0.0 1 6 11 47	minikube addons enable logviewer command does not apply YAML . . When I am trying to enable the logviewer add-on on minikube v 1.11.0 , the YAML is not applied . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Steps to reproduce the issue : Installed minikube ( version 1.11.0 ) After successful install tried to start by running : minikube start . : minikube addons enable logviewer -- alsologtostderr . Full output of failed command : : gashirar : ~ $ minikube addons enable logviewer -- alsologtostderr I0605 00:48:22 . 944397 83419 addons . go : 50 ] Setting logviewer = true in profile ' minikube ' I0605 00:48:22 . 944436 83419 addons . go : 92 ] Writing out ' minikube ' config to set logviewer = true ... ?? The ' logviewer ' addon is enabled . In the case of other addons , the log of applying YAML is displayed , but it is not displayed in the case of logviewer . I think it's because the callbacks is not set in logviewer . < URL >	0	0
1 1 1.4 2.0 1.3 1.5 1.25 1.5 1.0 1.0 1.0 1 1.0 3 4 7 18	Minikube stop - correct the number of stopped nodes . Steps to reproduce the issue : minikube start minikube stop When there is a single node stopped , the plural word ' nodes ' is used instead of singular which can be confusing . I find this particularly to be a simple fix within the next file on the line 123 : < URL > Screenshot : Full output of : minikube logs . command : No applicable , UI bug . Full output of failed command : No applicable , UI bug .  	1	0
0 0 0.8 0.0 1.0 1.0 0.9 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 2 5 38	none : waiting for k8s-app = kube-proxy : timed out waiting for the condition . Ubuntu 18.04.1 LTS Release : 18.04 Codename : bionic : user @ubuntu : ~ /Downloads/minikube$ sudo minikube start -- vm-driver = none o minikube v 0.35.0 on linux ( amd64 ) > Configuring local host environment ... ... : Restarting existing none VM for ' minikube ' ... : Waiting for SSH access ... - ' minikube ' IP address is 192.168.100.131 - Configuring Docker as the container runtime ... - Preparing Kubernetes environment ... - Pulling images required by Kubernetes v 1.13.4 ... : Relaunching Kubernetes v 1.13.4 using kubeadm ... : Waiting for pods : apiserver proxy ! Error restarting cluster : wait : waiting for k8s-app = kube-proxy : timed out waiting for the condition .	2	0
2 2 1.0 1.0 1.0 1.0 1.05 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 4 6 23	Host access Documentation Problem . Hi , the procedure outlined for connecting to the host in the documentation at : < URL > does not work because neither : ping . nor : telnet . are installed on the minikube docker image : docker @minikube : ~ $ ping host . minikube . internal -bash : ping : command not found docker @minikube : ~ $ telnet host . minikube . internal -bash : telnet : command not found .    	1	2
2 2 0.8 0.0 1.0 1.0 1.2 1.5 0.6666666666666666 0.0 0.0 0 0.0 0 0 0 3	minikube should fail early if executed as root . This is a bad practice , and could address some awkward connectivity and authentication issues , like #2548  	1	1
2 2 1.4 1.0 1.3 1.0 1.05 1.0 1.3333333333333333 1.0 1.0 81 1.0 1 5 8 60	Add `kubectl desc node -A` to `minikube logs` command . It should probably be at the top , as it's increasingly important to solve resource limitations .	0	1
2 2 0.6 0.0 0.9 1.0 0.85 1.0 0.6666666666666666 0.0 1.0420168067226891 119 1.0 2 6 14 46	KVM : provide suggestion when default network doesnt exist . I had accidently deleted my default network ( while debugging a kvm problem ) but minikube was dead and could never come back , till I created a default network : and the output of $ sudo virsh net-list -- all was empty < URL > : med @xmac : ~ /workspace/minikube ( static_ip_docker)$ . / run.sh 棣冩 [ p1 ] minikube v 1.10.1 on Ubuntu 20.04 閴?Automatically selected the kvm2 driver 棣冩啢 Starting control plane node p1 in cluster p1 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 3900MB , Disk = 20000MB ) ... 棣冦亞 StartHost failed , but will try again : creating host : create : Error creating machine : Error in driver during machine creation : creating network : network default doesn't exist : virError(Code = 43 , Domain = 19 , Message='Network not found : no network with matching name ' default '' ) 棣冩敡 Restarting existing kvm2 VM for ' p1 ' ... 棣冩▼ Failed to start kvm2 VM . ' minikube start -p p1 ' may fix it : driver start : creating network : network default doesn't exist : virError(Code = 43 , Domain = 19 , Message='Network not found : no network with matching name ' default '' ) 閴?Startup with kvm2 driver failed , trying with alternate driver docker : Failed to start host : driver start : creating network : network default doesn't exist : virError(Code = 43 , Domain = 19 , Message='Network not found : no network with matching name ' default '' ) . we could either do that for them , or send them to the documentation	0	0
0 0 1.4 2.0 0.8 0.5 1.1 1.0 1.0 1.0 0.0 0 0.0 0 1 8 38	console . OutStyle () arguments escaping . Hi , if you send an argument to console . OutStyle () that contains a ' %' sign , the sign will get interpreted as a formatting directive instead of being printed out . Example : : console . OutStyle('option ' , ' %s ' , ' HTTP_PROXY=<URL>') . will print out : HTTP_PROXY = < URL > . instead of : HTTP_PROXY = < URL > .	2	0
2 2 0.8 0.0 0.8 0.0 1.0 1.0 0.6666666666666666 0.0 0.0 0 0.0 2 4 8 25	xdg-open : no method available for opening on an Ubuntu machine . Steps to reproduce the issue : minikube service mongo-express-service 棣冨竴 Opening service default/mongo-express-service in default browser ... xdg-open : no method available for opening ' < URL > 閴?Exiting due to HOST_BROWSER : exit status 3 棣冩▼ If the above advice does not help , please let us know : Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command :  	2	2
2 2 1.4 2.0 1.2 1.5 1.2 1.5 1.3333333333333333 2.0 0.8730964467005076 197 1.0 4 6 12 50	auto-pause : add arm64 support . our makefile needs to build arm64 binary for arm64 kic image	0	1
0 0 1.2 1.0 1.2 1.0 1.1 1.0 1.0 1.0 0.9851851851851852 135 1.0 0 1 5 17	docker network create : Pool overlaps with other one on this address space . as seen here < URL > : I0822 18:04:37 . 920257 332897 cli_runner . go : 109 ] Run : docker network create -- driver = bridge -- subnet = 192.168.39.0 /24 -- gateway 192.168.39.1 offline-containerd-20200822180426-331183 W0822 18:04:37 . 975776 332897 kic . go : 85 ] unable to create docker network ; node ip may not be stable : error creating network : docker network create -- driver = bridge -- subnet = 192.168.39.0 /24 -- gateway 192.168.39.1 offline-containerd-20200822180426-331183 : exit status 1 stdout : stderr : Error response from daemon : Pool overlaps with other one on this address space .	0	0
1 1 1.4 2.0 1.2 1.5 1.2 1.0 1.6666666666666667 2.0 1.5 2 1.5 1 2 9 25	  Add Documentation on setting up minikube with Hyper-V & Virtualbox on Windows . Quite a lot of people who are on Windows are facing issues while trying to run minikube . As of now , the documentation we have on the website doesn't include any initial setup which is required to setup minikube with Hyper-V & Virtualbox . Relates with #4783 /assign	0	2
1 1 0.8 1.0 0.7 0.5 0.8 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 1 5 21	Add installation documentation for ARM64 . The guide here : < URL > seems for X86 only , the deb package I get is not supported on ARM64 , : # dpkg -i minikube_ 1.6.2 . deb dpkg : error processing archive minikube_ 1.6.2 . deb ( -- install ): package architecture ( amd64 ) does not match system ( arm64 ) Errors were encountered while processing : minikube_ 1.6.2 . deb . Is there any guide for installation for ARM64 and the deb is ready ? Thanks a lot !     	1	2
2 2 1.2 1.0 1.1 1.0 1.05 1.0 1.0 1.0 1.0 1 1.0 3 5 10 31	Add Operators OLM addon . /kind feature It will be nice to have the operator lifecycle manager installed in to minikube by default as that can support installing stack from < URL > If not default alteast an addon will help . e.g. For OLM install we need curl -sL < URL > | bash -s 0.13.0	2	1
2 2 0.6 0.0 0.8 1.0 1.15 1.0 1.0 1.0 0.9574468085106383 141 1.0 4 7 14 36	windows : minikube ssh taking extremely long . minikube ssh pwd takes 17 seconds on windows in Azure this is HyperV driver : PS C : /Users/jenkins > PS C : /Users/jenkins > Measure-Command { minikube . exe ssh pwd } Days : 0 Hours : 0 Minutes : 0 Seconds : 17 Milliseconds : 625 Ticks : 176255191 TotalDays : 0.000203999063657407 TotalHours : 0.00489597752777778 TotalMinutes : 0.293758651666667 TotalSeconds : 17.6255191 TotalMilliseconds : 17625.5191 . and for docker driver 4 seconds : PS C : /Users/jenkins > Measure-Command { minikube . exe ssh pwd } Days : 0 Hours : 0 Minutes : 0 Seconds : 4 Milliseconds : 693 Ticks : 46930265 TotalDays : 5.43174363425926 E-05 TotalHours : 0.00130361847222222 TotalMinutes : 0.0782171083333333 TotalSeconds : 4.6930265 TotalMilliseconds : 4693.0265 . This bug also makes minikube cache tests to fail too .	0	0
1 1 1.0 1.0 1.4 1.5 1.25 1.0 0.6666666666666666 1.0 0.9719626168224299 107 1.0 4 9 16 49	-- extra-config does not take effect on an existing cluster . For example , if you specify : -extra-config kubelet . kube-api-qps = 5 -- extra-config controller-manager . kube-api-qps = 5 . , and run : sudo ps -afe | grep controller . , you'll see that th	1	0
2 2 2.0 2.0 1.3 2.0 1.45 2.0 2.0 2.0 0.5714285714285714 7 1.0 1 3 5 15	Investigate running x86 kubernetes on M1 . Docker Desktop for M1 now supports running x86 containers : lyaz @host --- /tmp 绂?docker version Client : Cloud integration : 1.0.14 Version : 20.10.6 API version : 1.41 Go version : go 1.16.3 Git commit : 370c289 Built : Fri Apr 9 22:46:57 2021 OS/Arch : darwin/arm64 Context : default Experimental : true Server : Docker Engine - Community Engine : Version : 20.10.6 API version : 1.41 ( minimum version 1.12 ) Go version : go 1.13.15 Git commit : 8728dd2 Built : Fri Apr 9 22:44:13 2021 OS/Arch : linux/arm64 Experimental : true containerd : Version : 1.4.4 GitCommit : 05f951a3781f4f2c1911b05e61c160e9c30eaa8e runc : Version : 1.0.0 -rc93 GitCommit : 12644e614e25b05da6fd08a38ffa0cfe1903fdec docker-init : Version : 0.19.0 GitCommit : de40ad0 ilyaz @host --- /tmp 绂?arch arm64 ilyaz @host --- /tmp 绂?docker run -it -- platform linux/amd64 ubuntu root @29557202deef :/ # arch x86_64 root @29557202deef :/ # ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 1.6 0.1 222560 13832 pts/0 Ssl 04:35 0:00 /usr/bin/qemu-x86_64 /usr/bin/bash root 18 0.0 0.1 223956 8916 ? Rl+ Jun21 0:00 /usr/bin/ps aux root @29557202deef :/ # . We need to investigate if it's possible to run x86_64 kicbase there . If yes , we could give a lot of flexibility to our users , including building mixed arm/x86 minikube clusters It's qemu and probably is pretty slow , but worths a shot  	1	1
0 0 1.0 1.0 1.1 1.5 1.05 1.0 1.0 1.0 0.5 4 0.5 1 4 11 34	TestNetworkPlugins/group/auto/HairPin test fails . See < URL > for example	0	0
2 2 1.4 2.0 1.1 1.5 1.05 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 1 12	Website page for ' minikube cache ' has wrong description . < URL > describes : minikube cache . command . It says : Add , delete , or push a local image into minikube But the command has 3 sub-commands below : add delete reload So , it should be : Add , delete , or reload an image into minikube  	2	2
2 2 1.0 1.0 0.8 0.5 0.8 0.5 1.0 1.0 0.9603174603174603 126 1.0 1 4 5 39	start : Comma-delimited -- addons silently ignored . Oddly , our : -- addons . flag does not allow comma-delimited addons . Worse , it completely ignores your choice rather than returning an error : : . /out/minikube start -- addons = dashboard , efk , freshpod , istio , istio-provisioner , olm -- memory = 16384 -- driver = hyperkit 5.3 s 椤?Thu Sep 24 12:12:02 2020 棣冩 minikube v 1.13.1 on Darwin 10.15.6 閴?Using the hyperkit driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating hyperkit VM ( CPUs = 2 , Memory = 16384MB , Disk = 20000MB ) ... 棣冩儞 Preparing Kubernetes v 1.19.2 on Docker 19.03.12 ... 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : default-storageclass , storage-provisioner 閴?/usr/local/bin/kubectl is version 1.16.6 -beta . 0 , which may have incompatibilites with Kubernetes 1.19.2 . 棣冩寱 Want kubectl v 1.19.2 ? Try ' minikube kubectl -- get pods -A ' 棣冨及 Done ! kubectl is now configured to use ' minikube ' by default .  	1	0
1 1 0.8 1.0 0.7 0.5 0.85 1.0 0.6666666666666666 1.0 0.8405797101449275 207 1.0 2 8 21 60	add integration test for Auto-pause .  	1	1
2 2 1.4 1.0 1.3 1.0 1.0 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 10 13 32	start ' -- embed-certs ' fails to find certs . The exact command to reproduce the issue : As root : : . /minikube start -- vm-driver = none -- embed-certs . Run this on a clean CentOS-7 installation with Docker started/enabled . The full output of the command that failed : : . /minikube start -- vm-driver = none -- embed-certs 棣冩 minikube v 1.4.0 on Centos 7.6.1810 棣冦仚 Running on localhost ( CPUs = 2 , Memory = 3789MB , Disk = 40939MB ) ... 閳╃櫢绗?OS release is CentOS Linux 7 ( Core ) 棣冩儞 Preparing Kubernetes v 1.16.0 on Docker 1.13.1 ... 棣冩寴 Failed to setup kubeconfig : reading CertificateAuthority /root/ . minikube/ca . crt : open /root/ . minikube/ca . crt : no such file or directory 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > .  	1	0
1 1 1.0 1.0 1.1 1.0 1.35 1.5 1.0 1.0 0.9887640449438202 89 1.0 2 7 11 49	dashboard : fetch and run the built-in kubectl version . This would improves our getting started experience .   	1	1
0 0 0.4 0.0 0.7 0.0 0.9 1.0 0.0 0.0 0.9507042253521126 142 1.0 2 8 14 37	Dedicated network for Docker driver . dedicate network might solve a lot of CNI issues	0	1
0 0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0 0.0 5 6 8 56	kubelet resolv-conf is not overriding . Current status : OS : Ubuntu 18.0.4 Minikube version : 1.10 . x Driver : none On former versions , the minikube is auto setting extra-config to ' kubelet . resolv-conf = /run/systemd/resolve/resolv . conf ' only when kubelet . resolv . conf is not set . But once it is set , then auto setting is ignored , and the specified conf was overriding . On version 1.10 . x , the minikube is auto setting extra-config to ' kubelet . resolv-conf = /run/systemd/resolve/resolv . conf ' always . i.e. the specified resolv conf is appending to the systemd resolv conf . v 1.9.0 $minikube start : ... Preparing Kubernetes v 1.18.0 on Docker 19.03.8 ... - kubelet . resolv-conf = /run/systemd/resolve/resolv . conf ... . $minikube start -- extra-config = kubelet . resolv-conf = /home/zuul/ . minikube/k8s_resolv . conf : ... Preparing Kubernetes v 1.18.0 on Docker 19.03.8 ... - kubelet . resolv-conf = /home/zuul/ . minikube/k8s_resolv . conf ... . v 1.10 . x $minikube start : ... Preparing Kubernetes v 1.18.0 on Docker 19.03.8 ... - kubelet . resolv-conf = /run/systemd/resolve/resolv . conf ... . $minikube start -- extra-config = kubelet . resolv-conf = /home/zuul/ . minikube/k8s_resolv . conf : ... Preparing Kubernetes v 1.18.0 on Docker 19.03.8 ... - kubelet . resolv-conf = /home/zuul/ . minikube/k8s_resolv . conf - kubelet . resolv-conf = /run/systemd/resolve/resolv . conf ... . Expectation kubelet . resolv-conf would be overriding . So once that is specified , then the auto setting would be ignored .  	1	0
0 0 0.6 0.0 0.9 1.0 0.75 0.5 0.6666666666666666 0.0 0.0 0 0.0 0 1 10 42	Non-interactive minikube addons configure ? . I'm trying to fully script a minikube start command so I can share it between developers . I want to use metallb addon introduced recently , which requires LoadBalancer Start and End IPs ( I can construct it with output from minikube ip ) to be configured via minikube addons configure metallb . Can this be set non-interactively ? By default , it always spawns the input , and you have to input the addresses yourselves .	2	1
2 2 1.0 1.0 1.3 2.0 1.4 2.0 0.6666666666666666 0.0 0.9882352941176471 85 1.0 3 4 10 54	Fall-through to next driver if host creation fails . On some systems , the preferable driver may be broken in a way that we are unable to detect until host creation . If cluster creation fails , due to permissions issues for instance , we should delete the cluster and fall through to the next driver .  	1	1
2 2 2.0 2.0 1.8 2.0 1.55 2.0 2.0 2.0 0.0 0 0.0 0 0 0 0	hyperkit clock falls behind : failed to write or validate certificate : the certificate is not valid yet . Minikube version : v 0.18.0 Environment : - OS : MacOS Sierra ( 10.12.4 ( 16E195 )) - VM Driver : xhyve - ISO version : v 0.18.0 What happened : When I wake my laptop after sleep , the clock of the Minikube VM lags behind . This causes problems with the registry credentials plugin , since AWS rejects credentials requests with an invalid timestamp . What you expected to happen : The clock should sync after computer wakeup . How to reproduce it ( as minimally and precisely as possible ): See above .	0	0
2 2 1.4 2.0 1.3 1.5 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 2 5	hyperv : Too many retries waiting for SSH to be available / target machine actively refused it . . minikube version : v 0.28.2 kubectl version : Client Version : version . Info{Major:' 1 ' , Minor:' 10 ' , GitVersion:' v 1.10.3 ' , GitCommit:' 2bba0127d85d5a46ab4b778548be28623b32d0b0 ' , GitTreeState:' clean ' , BuildDate:' 2018-05-21T 09:17:39 Z ' , GoVersion:' go 1.9.3 ' , Compiler:' gc ' , Platform:' windows/amd64 ' } Unable to connect to the server : dial tcp [: : 1 ]: 8080 : connectex : No connection could be made because the target machine actively refused it . OS : windows 10 minikube logs : F0813 14:52:15 . 753035 14752 logs . go : 50 ] Error getting cluster bootstrapper : getting kubeadm bootstrapper : getting ssh client : Error creating new ssh host from driver : Error getting ssh host name for driver : IP not found docker : 18.06.0 ( Community edition for windows ) vm driver : Hyper-V Have followed the steps as per - < URL > Expecting the cluster to start running normally . But instead getting the following error : C : > minikube start -- vm-driver hyperv -- hyper v-v irtual-switch ' Primary Virtual Switch ' Starting local Kubernetes v 1.10.0 cluster ... Starting VM ... E0813 14:28:30 . 508122 22044 start . go : 174 ] Error starting host : Temporary Error : Error configuring auth on host : Too many retries waiting for SSH to be available . Last error : Maximum number of retries ( 60 ) exceeded . Retrying . E0813 14:33:32 . 996420 22044 start . go : 174 ] Error starting host : Temporary Error : Error configuring auth on host : Too many retries waiting for SSH to be available . Last error : Maximum number of retries ( 60 ) exceeded . Please help on how to resolve this , have checked various guides however none could help me resolve the issue .  	1	0
2 2 0.8 1.0 1.0 1.0 0.9 1.0 1.0 1.0 0.9811320754716981 53 1.0 2 2 3 22	gvisor addon crashloop : open /tmp/gvisor/gvisor-containerd-shim . toml : no such file or directory . While working on an integration test refactor , I noticed that the integration tests are broken at head . I was able to replicate the breakage using a basic command-line : : make % . /out/minikube start -- container-runtime = containerd -- docker-opt containerd = /var/run/containerd/containerd . sock 棣冩 minikube v 1.4.0 -beta . 0 on Debian rodete 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... Running pre-create checks ... Creating machine ... ... 棣冩憹 Preparing Kubernetes v 1.16.0 -beta . 1 on containerd 1.2.8 ... 閳?opt containerd = /var/run/containerd/containerd . sock 棣冩 Pulling images ... 棣冩畬 Launching Kubernetes ... 閳?Waiting for : apiserver etcd scheduler controller 棣冨及 Done ! kubectl is now configured to use ' minikube ' . Then : : minikube addons enable gvisor . Gives you a pod that looks like this : : kube-system gvisor 0/1 CrashLoopBackOff 4 2m14s . Pod logs show : : 2019/09/09 20:30:53 Storing default config . toml at /tmp/config . toml 2019/09/09 20:30:53 Copying gvisor-containerd-shim . toml ... 2019/09/09 20:30:53 copying config files : copying gvisor-containerd-shim . toml : getting contents of deploy/addons/gvisor/gvisor-containerd-shim . toml : open /tmp/gvisor/gvisor-containerd-shim . toml : no such file or directory . This host has no speciial cached images .. : minikube cache list . returns empty . As an aside , I think that minikube as a team is a bit confused about how to properly handle integration testing gvisor . I get the feeling that we may be pre-release testing gvisor , by virtue of the docker build that happens in : common.sh . . It feels wrong to me ? /cc @ianlewis	0	0
0 0 0.8 1.0 1.2 1.0 1.2 1.0 1.0 1.0 1.375 32 2.0 7 8 12 26	kic : add integration tests . same way we have for none , kvm , hyperkit ...	0	1
2 2 1.4 2.0 1.5 2.0 1.45 2.0 1.6666666666666667 2.0 0.0 0 0.0 3 4 6 55	Missing leading ' v ' for start command on new version available . Steps to reproduce the issue : : minikube start . on 1.10.0 : minikube start . on 1.10.1 suggests : Kubernetes 1.18.2 is now available . If you would like to upgrade , specify : -- kubernetes-version = 1.18.2 . : minikube start -- kubernetes-version = 1.18.2 . fails with the final line as : Suggestion : Check that your -- kubernetes-version has a leading ' v ' . For example : ' v 1.1.14 ' . Resolution : The message about updating the minikube cluster should be something like : Kubernetes 1.18.2 is now available . If you would like to upgrade , specify : -- kubernetes-version = v 1.18.2 . This could either be how the strings are printed , or the actual version itself . < URL > References : - occurrences : < URL >	2	0
2 2 1.4 2.0 1.5 2.0 1.5 2.0 1.3333333333333333 2.0 1.0865384615384615 104 1.0 4 8 22 77	preload hack script : puts binaries from other versions . I was looking at the jenkins logs for generating the binary I saw this : and some tar files are susopeously big : 17:43:53 . /lib/minikube/binaries/ 17:43:53 . /lib/minikube/binaries/v 1.15.7 / 17:43:53 . /lib/minikube/binaries/v 1.15.7 /kubeadm 17:43:53 . /lib/minikube/binaries/v 1.15.7 /kubelet 17:43:53 . /lib/minikube/binaries/v 1.15.7 /kubectl 17:43:54 . /lib/minikube/binaries/v 1.18.0 -alpha . 1/ 17:43:54 . /lib/minikube/binaries/v 1.18.0 -alpha . 1/kubeadm 17:43:54 . /lib/minikube/binaries/v 1.18.0 -alpha . 1/kubelet 17:43:54 . /lib/minikube/binaries/v 1.18.0 -alpha . 1/kubectl 17:43:54 . /lib/minikube/binaries/v 1.14.10 / 17:43:54 . /lib/minikube/binaries/v 1.14.10 /kubeadm 17:43:54 . /lib/minikube/binaries/v 1.14.10 /kubelet 17:43:55 . /lib/minikube/binaries/v 1.14.10 /kubectl 17:43:55 . /lib/minikube/binaries/v 1.16.4 / 17:43:55 . /lib/minikube/binaries/v 1.16.4 /kubeadm 17:43:55 . /lib/minikube/binaries/v 1.16.4 /kubelet 17:43:56 . /lib/minikube/binaries/v 1.16.4 /kubectl .	0	0
2 2 1.0 1.0 1.3 1.5 1.1 1.0 1.3333333333333333 1.0 0.0 0 0.0 2 4 6 25	add support of 802.1 Q kernel module . The current kernel build does not provide support for 802.1 Q module . This module is adding ability to configure VLAN sub-interfaces which are for networking use-cases more than required . Select this and you will be able to create 802.1 Q VLAN interfaces on your Ethernet interfaces . 802.1 Q VLAN supports almost everything a regular Ethernet interface does , including firewalling , bridging , and of course IP traffic . You will need the ' ip ' utility in order to effectively use VLANs . See the VLAN web page for more information : < URL > ~ greear/vlan . html < URL > : diff -- git a/deploy/iso/minikube-iso/board/coreos/minikube/linux_defconfig b/deploy/iso/minikube-iso/board/coreos/minikube/linux_defconfig index 1871f86d8 .. 5c8ce5465 100644 --- a/deploy/iso/minikube-iso/board/coreos/minikube/linux_defconfig +++ b/deploy/iso/minikube-iso/board/coreos/minikube/linux_defconfig @@ -273 , 6 +273 , 7 @@ CONFIG_BRIDGE_EBT_SNAT = m CONFIG_BRIDGE_EBT_LOG = m CONFIG_BRIDGE_EBT_NFLOG = m CONFIG_BRIDGE = m +CONFIG_VLAN_8021Q = m CONFIG_NET_SCHED = y CONFIG_NET_SCH_HTB = y CONFIG_NET_SCH_PRIO = y .	2	1
0 0 1.4 2.0 1.2 1.5 1.25 2.0 1.3333333333333333 2.0 1.1637931034482758 116 1.0 2 2 6 22	Automatically start buildkit for `minikube image build` for containerd . Trying to build an image for containerd will likely result in an error like : : error : failed to get status : rpc error : code = Unavailable desc = connection error : desc = ' transport : error while dialing : dial unix /run/buildkit/buildkitd . sock : connect : no such file or directory ' . This is because of the missing packaging and units , for buildkitd component . The workaround suggested by upstream is to start the daemon up manually : : sudo -b buildkitd -- oci-worker = false -- containerd-worker = true -- containerd-worker-namespace = k8s.io . It is documented when running : buildctl . , but not for : minikube image build . < URL > Eventually it is supposed to be started up with systemd socket-activation . : sudo systemctl start buildkit . socket . 9947 ~~< URL >~~ But it is not yet available in any released version of BuildKit , such as 0.8.2 10068 < URL >	0	1
0 0 0.6 0.0 0.6 0.0 0.85 1.0 0.6666666666666666 0.0 0.0 0 0.0 2 2 6 19	minikube v 1.12.3 faild to start on Centos 7.4.1708 . $ docker -v Docker version 17.03.2 -ce , build f5ec1e2 $ minikube start -- cpus = 4 -- memory = 4096mb -- driver = docker * minikube v 1.12.3 on Centos 7.4.1708 * Using the docker driver based on user configuration * Starting control plane node minikube in cluster minikube * Pulling base image ... * Creating docker container ( CPUs = 4 , Memory = 7900MB ) ... * Stopping node ' minikube ' ... * Powering off ' minikube ' via SSH ... * Deleting ' minikube ' in docker ... ! StartHost failed , but will try again : creating host : create : creating : prepare kic ssh : copying pub key : docker copy /tmp/tmpf-memory-asset543657973 into minikube : /home/docker/ . ssh/authorized_keys , output : unknown shorthand flag : ' a ' in -a See ' docker cp -- help ' . : exit status 125 * Creating docker container ( CPUs = 4 , Memory = 7900MB ) ... * Failed to start docker container . ' minikube start ' may fix it : creating host : create : creating : prepare kic ssh : copying pub key : docker copy /tmp/tmpf-memory-asset080548560 into minikube : /home/docker/ . ssh/authorized_keys , output : unknown shorthand flag : ' a ' in -a See ' docker cp -- help ' . : exit status 125	2	0
0 0 1.2 1.0 1.0 1.0 1.1 1.0 1.0 1.0 0.0 0 0.0 0 1 2 2	service : allow per-service configuration of protocol ( mongodb :// for example ) . This is a feature request . In summary I'd like to be able to configure the output of : minikube service . to allow more accurate information to be displayed for our developers . Environment : Minikube version ( use : minikube version . ): v 0.27.0 - OS : macOS 10.13.4 - VM Driver : virtualbox - ISO version : v 0.26.0 - Install tools : n/a - Others : n/a What happened : Using mongodb as an example , but this applies to any external service you'd like to expose that isn't http . Create a mongodb deployment Create a mongodb service Now list your running services as follows : : $ minikube service list -- namespace default | -------------|---------------------- | -------------------------------- | | NAMESPACE | NAME | URL | | -------------|---------------------- | -------------------------------- | | default | mongodb | < URL > | | -------------|---------------------- | -------------------------------- | . What you expected to happen : It'd be great if there were a way of configuring the service endpoint to display as : mongodb :// < ip > : < port > . and be configurable on a service-by-service basis . Very happy to submit a pull request for this although I could use some help in identifying an appropriate place to add it since this would be my first contribution .	2	1
1 1 1.0 1.0 0.8 1.0 0.85 1.0 1.0 1.0 0.8333333333333334 6 0.5 0 1 5 22	[ Ingress Addon ] Fix bug which the networking.k8s.io/v1 ingress is always rejected . What type of PR is this ? /kind bug /addon ingress /kind regression /priority important-soon What this PR does / why we need it : This PR will fix the bug which the : networking.k8s.io/v1 . ingress is alwalys rejected . As I described at < URL > upstream ingress-nginx(v 0.44.0 and v 0.45.0 ( latest )) doesn't allow : networking.k8s.io/v1 . ingress . Ingress-nginx admission controller only support(validate ) : networking.k8s.io/v1beta1 . ingress . : networking.k8s.io/v1 . ingress is not supported(validated ) . But our ValidatingWebhookConfiguration manifest check both : networking.k8s.io/v1beta1 . and : networking.k8s.io/v1 . . < URL > I removed : networking.k8s.io/v1 . check from ValidatingWebhookConfiguration manifest . And I add a integration test pattern to check if : networking.k8s.io/v1 . ingress works . Which issue(s ) this PR fixes : Fix #11121 Does this PR introduce a user-facing change ? Yes . This PR allows to create : networking.k8s.io/v1 . ingress . Before this PR : networking.k8s.io/v1 . ingress is always rejected by Validating Webhook . : ' failed to process webhook request ' err='rejecting admission review because the request does not contain an Ingress resource but networking.k8s.io/v1 , Kind = Ingress with name nginx-ingress in namespace default ' . After this PR : networking.k8s.io/v1 . ingress can be created . : ingress.networking.k8s.io/nginx-ingress created . Additional documentation e.g. , KEPs ( Kubernetes Enhancement Proposals ) , usage docs , etc . : : NONE .	0	0
0 0 0.8 1.0 1.0 1.0 1.05 1.0 1.0 1.0 1.3333333333333333 3 2.0 1 2 6 36	kic : Preload docker images into node . This should reduce start latency for the docker runtime by ~ 17 seconds .	0	1
0 0 0.4 0.0 0.7 0.0 0.85 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 5 30	Broken podman driver docs due to using templating in . inc file . < URL > docker link is broken . As far as I can tell this is due to using templating in an : . inc . file which does not seem to be permitted . < URL > Happy to open a PR pending some guidance on the solution that would be accepted . :)  	2	2
1 1 1.4 1.0 1.1 1.0 1.1 1.0 1.3333333333333333 1.0 0.0 0 0.0 3 5 6 26	Some integration tests are disabled on linux/arm64 . Currently some integration tests are skipped when running on arm64 . This issue is to track those and enable it back when blocking issues with dependencies are resolved . TestDownloadOnlyKic - preloads are not supported for arm64 yet TestPreload - preloads are not supported for arm64 yet TestOffline - crio and containerd are not fully supported on arm64 TestAddons - helm addon is not supported yet TestKVMDriverInstallOrUpdate - kvm is not supported on arm64 yet functional/validateMySQL - mysql image is not available for arm64   	1	0
0 0 1.4 2.0 1.1 1.0 1.2 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 3 7 33	sudo dashboard crash : Error getting machine status : state : machine does not exist . Trying to run sudo minikube dashboard ends up with output : 棣冩寴 Error getting machine status : state : machine does not exist 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : but it works alright with just minikube dashboard   	1	0
2 2 1.4 2.0 1.1 1.5 1.5 2.0 1.3333333333333333 2.0 0.9 190 1.0 1 2 5 48	improve error message for cgroup limit . 閴?Cgroup v2 does not allow setting memory , if you want to set memory , please modify your Grub as instructed in < URL > ernel-does-not-support-cgroup-swap-limit-capabilities	0	0
0 0 0.6 0.0 1.0 1.0 1.2 1.0 0.0 0.0 1.0125 80 1.0 0 4 7 59	minikube config set vm-driver ignored . Related to #7078 #7079	0	0
1 1 1.4 1.0 1.2 1.0 1.15 1.0 1.6666666666666667 2.0 1.2121212121212122 33 1.0 0 3 6 37	Upgrade CNI to support version 0.4.0 . : ERRO[0000 ] Error adding network : incompatible CNI versions ; config is ' 0.4.0 ' , plugin supports [' 0.1.0 ' ' 0.2.0 ' ' 0.3.0 ' ' 0.3.1 ' ] . We are running 0.6.0 ( Aug 2017 ) , should upgrade to 0.7.0 ( Apr 2019 ) - or 0.7.1 And we probably have to build it from source , in order to do so ( no more binaries ) Required for : podman run .  	1	1
1 1 1.0 1.0 1.2 1.0 1.25 1.0 1.0 1.0 0.0 0 0.0 2 5 8 41	allow downloading insecure , ( Fetch via HTTP if HTTPS is unavailable ) . Hi , minikube start should support insecure download of kubectl & co because of an horrible corporate LAN infrastructure . Is this possible ? Thanks ! logout output of minikube start Downloading kubeadm v 1.17.2 W0220 14:24:53 . 574297 13590 exit . go : 101 ] Failed to update cluster : downloading binaries : downloading kubeadm : Error downloading kubeadm v 1.17.2 : failed to download : failed to download to temp file : download failed : 5 error(s ) occurred : Temporary download error : Get < URL > x509 : certificate signed by unknown authority	2	1
2 2 1.4 1.0 1.1 1.0 1.3 2.0 1.6666666666666667 2.0 0.9273743016759777 179 1.0 2 2 11 29	running minikube . exe in WSL looks for home in C :/ ... . when minikube . exe runs in Powershell it will create a MINIKUBE_HOME in C : /Program ... but when the same binary is called in WSL ( windows sub linux ) it looks for minikube home in linux Path ~ / . minikube this causes confusion ! for that reason we need to either , not allow running . exe binary in WSL and make suer make a choice of downloading Linux binary on WSL or . exe binary needs to do the right thing if it is run in WSL I suggest as a start we disallow running the . exe binary WSL ( unless -- force is applied ) as seen in < URL >	0	0
0 0 1.2 2.0 1.2 1.5 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 2 2 6 32	status command should support json output . For upstream tooling , we need JSON output for commands like : minikube status . . The exact command to reproduce the issue : minikube status cc : @tstromberg   	1	1
0 0 1.2 1.0 1.4 1.5 1.1 1.0 1.0 1.0 0.9795918367346939 98 1.0 3 10 20 71	LoadBalancer external IP unroutable on master ( service-cluster-ip ? ) . : . /out/minikube start -- driver = hyperkit . : kubectl create deployment balanced -- image = k8s.gcr.io/echoserver:1.4 . : kubectl expose deployment balanced -- type = LoadBalancer -- port = 8081 . : . /out/minikube tunnel . The tunnel shows a route : -- service-cluster-ip-range='10 . 96.0.0 /12 ' . : Status : machine : minikube pid : 67162 route : 10.96.0.0 /12 -> 192.168.64.10 minikube : Running services : [ balanced ] errors : minikube : no errors router : no errors loadbalancer emulator : no errors . : kubectl get services balanced . shows an EXTERNAL-IP in the same range as CLUSTER-IP , rather than the range that the tunnel expects : : NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S ) AGE balanced LoadBalancer 10.100.105.183 10.100.105.183 8081:31739 /TCP 5m27s . This IP is not accessible : : ping -c1 10.100.105.183 PING 10.100.105.183 ( 10.100.105.183 ): 56 data bytes 36 bytes from 192.168.64.1 : Time to live exceeded Vr HL TOS Len ID Flg off TTL Pro cks Src Dst 4 5 00 5400 45a8 0 0000 01 01 ff3c 192.168.64.1 10.100.105.183 .	0	0
2 2 0.6 0.0 0.9 0.5 0.95 0.5 1.0 1.0 1.0941176470588236 85 1.0 6 15 27 58	Noisy ' Overriding stale ClientConfig host ' message . for docker/podman driver , on each hard start , we update the ip , port , currently it shows as an Error Log it would be nicer to show that as a out . T in a fashionable mannner . : Overriding stale ClientConfig host < URL > with < URL > . should be replace by something nicer : Icon Updating the API Server Endpoint in KubeConfig < URL > . : medmac@ ~ /workspace/minikube ( kic_stop_improve ) $ make && . /out/minikube start -- driver = docker -- container-runtime = containerd make : `out/minikube ' is up to date . 棣冩 minikube v 1.9.2 on Darwin 10.13.6 閴?Using the docker driver based on existing profile 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩 Pulling base image ... 棣冩敡 Restarting existing docker container for ' minikube ' ... 棣冩憹 Preparing Kubernetes v 1.18.0 on containerd 1.3.2 ... 閳?kubeadm . pod-network-cidr = 10.244.0.0 /16 E0408 23:16:35 . 865237 62250 kubeadm . go : 346 ] Overriding stale ClientConfig host < URL > with < URL > 棣冨皞 Enabling addons : default-storageclass , storage-provisioner 棣冨及 Done ! kubectl is now configured to use ' minikube ' .	0	0
2 2 1.0 1.0 1.2 1.0 1.0 1.0 1.0 1.0 0.625 8 0.0 4 5 16 58	Copy in k8s binaries to preloaded tarball so we can skip transfer .	0	1
2 2 1.2 2.0 0.9 0.5 1.05 1.0 0.6666666666666666 0.0 1.4333333333333333 30 2.0 5 6 10 24	  kic : clearify image build process for kic . for kic the minikube docker-env command wont work . we need to provide a good clear story for the users , how to use their images . minikube cache add is one way we need to explore the registry addons we have to see if they work as intended .   	1	2
2 2 0.8 1.0 1.1 1.0 1.1 1.0 1.0 1.0 0.4375 16 0.0 1 1 4 21	Fix building for ppc64le and arm(v7 ) in kicbase . Building for ppc64le in kicbase was failing so we removed it ( #12398 ) so we could release , ppc64le should be revisited and added back .	2	1
1 1 1.0 1.0 0.9 1.0 0.9 1.0 0.6666666666666666 1.0 1.3333333333333333 3 1.0 0 1 7 23	crio : Unable to pull images form internal registry . : minikube version : v 1.16.0 commit : 9f1e482427589ff8451c4723b6ba53bb9742fbb1 . Steps to reproduce the issue : Start Minikube as : minikube start -p $PROFILE_NAME / -- memory = $MEMORY -- cpus = $CPUS / -- disk-size = 50g / -- insecure-registry='10 . 0.0.0 /24 ' . Apply the registry and registry-aliases add-on Build and push an image to the Minikube registry as : example.com/demo/greeter . Deploy a pod using the command : kubectl run demo-greeter -n tektontutorial / -- generator='run-pod/v1 ' / -- image='example . com/demo/greeter ' && / kubectl expose pod demo-greeter -n tektontutorial -- port 8080 -- type = NodePort . The kubectl get events shows the crashed pod with the following logs : Full output of failed command : : 0s Warning Failed pod/demo-greeter Failed to pull image ' registry . minikube/rhdevelopers/tekton-tutorial-greeter ' : rpc error : code = Unknown desc = error pinging docker registry registry . minikube : Get < URL > dial tcp 10.101.166.84 : 443 : connect : connection refused . With : -- insecure-registry . specified , the pod still tries to resolve the url as : https . than : http . CC : @afbjorklund  	1	0
1 1 1.6 2.0 1.2 1.0 1.25 1.5 1.3333333333333333 1.0 0.0 0 0.0 0 1 3 15	CHANGE_MINIKUBE_NONE_USER is missing in docs/en v_v ars . md . BUG REPORT < URL > uses CHANGE_MINIKUBE_NONE_USER but docs/en v_v ars . md does not document what it actually does .  	2	2
0 0 0.4 0.0 0.7 0.0 0.75 0.0 0.0 0.0 0.0 0 0.0 3 3 5 26	allow specifying extra option for k8s's proxy component bind address 0.0.0.0 . Running < URL > on centos 8 VM ( with firewalld disabled ) and trying to make minikube start a proxy running on another address ( cause it will be nice ) with the extra config tag : proxy . BindAddress . ( also try with : proxy . bindAddress . ) and this is the result : [ centos @k8s ~ ]$ minikube start -- vm-driver = kvm2 -- extra-config = proxy . BindAddress='0 . 0.0.0 ' 棣冩 minikube v 1.6.2 on Centos 8.1.1911 閴?Selecting ' kvm2 ' driver from user configuration ( alternates : [ none ]) 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... 棣冩儞 Preparing Kubernetes v 1.17.0 on Docker ' 19.03.5 ' ... 閳?proxy . BindAddress = 0.0.0.0 棣冩寴 Failed to update cluster : generating kubeadm cfg : generating extra component config for kubeadm : unknown component map['apiserver ' : ' apiServer ' ' controller-manager ' : ' controllerManager ' ' kubeadm ' : ' kubeadm ' ' kubelet ' : ' ' scheduler ' : ' scheduler ' ] . valid components are : map[apiserver : apiServer controller-manager : controllerManager kubeadm : kubeadm kubelet : scheduler : scheduler ] 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > . So ... any idea to why is that happening ?	2	1
2 2 1.8 2.0 1.7 2.0 1.4 2.0 2.0 2.0 1.5 2 1.5 2 4 7 21	Create the Node interface for multinode . ref #94	0	1
1 1 1.0 1.0 1.0 1.0 0.85 1.0 0.3333333333333333 0.0 1.75 8 2.0 0 2 5 25	UI : Advice when user tries to set profile name to ' delete ' or ' start ' . to delete a minikube profile you need to do : minikube delete -p prof_name . but if you do : minikube profile delete . it will actually create a profile named delete . because that command sets profile name . we need to at lest add an advice to the user , that says Did you mean to delete a profile ? and here is the command to delete profile ! I recommend the same approach for other key words such as , start , stop , status , ... to make it less confusing for new users . we could add this to the Unit tests of profile , so if someone tries to set the profile name to a list of reserved words , it should return an error .	2	0
2 2 1.4 2.0 1.3 1.5 1.15 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 3 3 27	Windows Installer : the version should not be quoted . After installing Minikube , in the Windows Control Panel the Version appears with double quotes : With double quotes , some tools get confused . For instance , winget wrongly detects an available upgrade : Probably , removing the quotes would allow a little better user experience . Also , the quotes could be removed from further fields such as the ' Publisher ' one .	2	0
2 2 1.0 1.0 1.0 1.0 1.2 2.0 1.3333333333333333 2.0 0.8209606986899564 229 1.0 0 0 3 20	flake chart : allow selecting only one Test in the Chart . For example : when you visit KVM tests I want to click on One Single Test and all other tests be grayed out < URL >  	1	1
1 1 1.0 1.0 1.2 1.0 1.1 1.0 0.6666666666666666 1.0 1.5789473684210527 19 2.0 0 1 5 32	  create documentation for vmware driver on Windows . our site currently only has vmware docs for mac os : < URL > we need to add documents to the site for vmware on windows . < URL >	2	2
2 2 1.2 1.0 0.9 1.0 1.05 1.0 1.0 1.0 1.4285714285714286 7 2.0 4 8 11 44	Ability to see that the node is a minikube node . It would be great if minikube could add a standard annotation to the node , e.g. : k8s.minikube.io/version=1.7.3 . so that tools like skaffold can rely on that .	0	1
0 0 0.6 0.0 0.9 0.5 0.9 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 1 2 9	SSH driver : -- ssh-port ignored ( it always use port 22 ) ! . SSH driver : -- ssh-port ignored ( it always uses port 22 ) !	2	0
0 0 0.8 0.0 0.9 0.5 0.9 1.0 0.6666666666666666 0.0 0.989247311827957 93 1.0 2 12 27 58	restart does not respect -- wait flag . I did some playing around and noticed that restart no longer respects the : -- wait . flag . It always waits for system components ( hardcoded into kubeadm ) . Setting : -- wait = false . or : -- wait = true . has no change . I also noticed that : starter . Cfg . VerifyComponents . gets reset to false on restart :(  	1	0
1 1 0.8 1.0 0.8 1.0 1.05 1.0 1.0 1.0 0.0 0 0.0 0 0 1 2	Implement a built-in binary update mechanism . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Feature Request What you expected to happen : I would like to update my minikube version to its latest by running : minikube update . command . Anything else do we need to know : Did we had any discussion before @aaron -prindle @r2d4 ?	2	1
1 1 0.2 0.0 0.6 0.0 0.8 0.5 0.3333333333333333 0.0 2.0 1 2.0 3 5 17 34	hyperv : remote command exited without exit status or exit signal . PS C : /Users/Administrator > minikube start -- registry-mirror='<URL>' -- vm-driver='hyperv ' -- memory = 2048 -- hyperv-virtual-switch='nat ' -- mount -- mount-string='/f::/f ' o minikube v 0.35.0 on windows ( amd64 ) Creating hyperv VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... - ' minikube ' IP address is fe80 :: 215:5 dff : fe 01:8 acd - Configuring Docker as the container runtime ... - Preparing Kubernetes environment ... @ Downloading kubelet v 1.13.4 @ Downloading kubeadm v 1.13.4 ! Failed to update cluster : downloading binaries : copy : wait : remote command exited without exit status or exit signal * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL >	2	0
1 1 1.0 1.0 1.0 1.0 1.0 1.0 1.3333333333333333 1.0 1.1014492753623188 69 1.0 4 5 11 34	Deprecate the old varlink for podman-env . Podman v2 doesn't support the varlink protocol any longer , so have to move to using the unix socket over ssh : : export CONTAINER_HOST='ssh://docker@192 . 168.99.100 : 22/run/podman/podman . sock ' export CONTAINER_SSHKEY = /home/anders/ . minikube/machines/minikube/id_rsa . This also means that we need to set up the : podman . socket . unit , in order to start the : podman . service . unit . As well as making the : /run/podman/podman . sock . available to the connecting user , in this case called ' docker ' . See < URL > for configuration Requirements : Podman 2.0.6 or later Currently we use PODMAN_VARLINK_BRIDGE and : sudo varlink -A ' podman varlink /$VARLINK_ADDRESS ' bridge . This is only available with podman 1.9.3 , which is no longer supported upstream ( we have to build our own podman-remote ) See #8596 We can time this podman-env : ssh :// . feature with docker-env : #9229 ~ ~ Could also take a look at using : podman system connection . perhaps . ~ ~ ~~< URL >~~ Note : Podman 3.0 will remove varlink support It is usually compiled out , even with the 2 . x builds	0	1
0 0 1.4 2.0 1.3 1.5 1.0 1.0 1.3333333333333333 2.0 1.0 10 1.0 2 5 11 40	breaking changes in minikube docker-env -- shell none . : minikube docker-env -- shell none . had previously no comments . In 1.12.0 -beta . 0 : : DOCKER_TLS_VERIFY = 1 DOCKER_HOST = tcp :/ / 127.0.0.1 : 32770 DOCKER_CERT_PATH = /Users/balintp/ . minikube/certs MINIKUBE_ACTIVE_DOCKERD = minikube # To point your shell to minikube's docker-daemon , run : # eval $(minikube -p minikube docker-env ) . expected ( as it works in 1.11.0 ) : DOCKER_TLS_VERIFY = 1 DOCKER_HOST = tcp :/ / 127.0.0.1 : 32770 DOCKER_CERT_PATH = /Users/balintp/ . minikube/certs MINIKUBE_ACTIVE_DOCKERD = minikube . This is breaking skaffold currently and hence Cloud Code when current context is pointing at minikube	0	0
1 1 0.8 1.0 0.9 1.0 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 1 1 16 45	powershell core : Get-WindowsOptionalFeature -FeatureName Microsoft-Hyper-V-All -Online fails . The exact command to reproduce the issue : : minikube start -- vm-driver = hyperv . The full output of the command that failed : : * minikube v 1.7.2 on Microsoft Windows 10 Pro 10.0.18362 Build 18362 * Using the hyperv driver based on user configuration ! ' hyperv ' driver reported an issue : C : /Windows/System32/WindowsPowerShell/v 1.0 /powershell . exe Get-WindowsOptionalFeature -FeatureName Microsoft-Hyper-V-All -Online failed : * Suggestion : Start PowerShell as Administrator , and run : ' Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All ' * Documentation : < URL > X hyperv does not appear to be installed . The operating system version : Microsoft Windows 10 Pro 10.0.18362 Build 18362 I tried this on 2 seperate machines that I use PowerShell Core primarily from and both failed with the same error . If I run the suspected ' failed ' command : : Get-WindowsOptionalFeature -FeatureName Microsoft-Hyper-V-All -Online . It seems to work just fine from Windows PowerShell so the error is at the very least misleading . I would suggest at least putting notes in the documentation stating that it is incompatible with PS Core until the issue is fixed or update the error message to allow users to be more aware of the actual issue . Thanks !	2	0
1 1 0.8 1.0 1.1 1.0 1.2 1.5 0.6666666666666666 1.0 0.0 0 0.0 0 0 6 16	Improve ' Minikube tunnel ' when port is taken . Steps to reproduce the issue : Kill minikube with a tunnel open minikube start kubectl create deployment hello-node -- image = k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-node -- type = LoadBalancer -- port = 8080 Run : minikube logs -- file = logs . txt . and drag and drop the log file into this issue Full output of failed command if not : minikube start . : [ logs . txt ]( < URL > So I was testing the envoy-x509 example from SPIRE < URL > in this testing , I deleted my minikube cluster ( to tear down and set back up with different configuration ) and I did this with `minikube tunnel` running in a different window in my terminal ( I'm on MacOS ) . I don't know if that caused the issue , but after doing this a few times , ports stopped being opened . The tunnel spat out ```E0913 10:59:35 . 061018 9261 ssh_tunnel . go : 72 ] error listing services : Get ' < URL>': dial tcp 127.0.0.1 : 55548 : connect : connection refused``` When I killed the cluster while it was open , which makes sense . The issue resolves itself by restarting my computer , but it is still frustrating to have to do so	0	0
1 1 1.4 1.0 1.2 1.0 1.15 1.0 1.3333333333333333 1.0 0.9754098360655737 122 1.0 0 1 5 26	cluster launched from WSL2 conflicts with PowerShell launched cluster . Some quirky behavior I noticed : In the default configuration for Docker , WSL2 and PowerShell share a container space minikube sees the correctly named container , but does not have access to SSH certificates between environments .  	1	0
2 2 1.2 2.0 1.2 2.0 1.15 2.0 1.3333333333333333 2.0 1.5 2 1.5 0 1 7 28	minikube always modify virtualbox hostonly adapter dhcp configuration . problem minikube code modify existing dhcp server configuration lowerip and upperip . With some circumstance we need set same lowerip and upperip to get a predicted single VM ip . But the minikube code logic : src/ k8s.io/minikube/vendor/github.com/docker/machine/drivers/virtualbox/virtualbox.go . is always changing it to 192.168.99.100 ~ 192.168.99.254 . Could team expose args to indicate whether changing dhcp configuration to provide more flexibility ?	2	1
0 0 0.4 0.0 0.7 0.5 0.9 1.0 0.6666666666666666 1.0 2.0 1 2.0 0 5 8 26	image-repository not respected for cni = calico . Steps to reproduce the issue : minikube start -- driver = docker -- image-repository = private-registry.net/infrastructure -- cni = calico All manifests , including addons , include the specified registry , except calico deployment and daemonset . They still point to their default images ( from docker.io )  	1	0
0 0 0.8 0.0 1.1 1.5 0.9 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 0 0 2	none : Add support for non-systemd Linux distributions . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT - With latest v 0.26.0 , : minikube start -- vm-driver = none -- kubernetes-version = v 1.8.0 -- logtostderr . failed to start on travis ci . For more info : < URL >  	1	1
1 1 1.2 1.0 1.3 1.0 1.4 2.0 1.3333333333333333 1.0 0.0 0 0.0 2 6 21 77	add documentation for WSL2 . : minikube start -- driver = docker . : minikube ip . = : 172.17.0.2 . How are you supposed to access the web app from a browser when the IP is : 172.17.0.2 . ? Browser just spins for a while before saying it can't connect . Only occurs when : -- driver = docker . . Just about every other driver gives you an ip that is accessible from browser .    	1	2
0 0 1.4 2.0 1.6 2.0 1.4 2.0 1.0 1.0 0.0 0 0.0 0 1 6 50	Support for Linux Kernel 5.1 + . Is it possible to support Linux kernel 5.1 + in the built Minikube iso ? The reason for this is that I'm intending to run Ceph in Minikube ( for which the : ceph-csi . plugin requires kernel 5.1 + or higher for ' deep flatten ' support ) . It looks like this was done in < URL > but then reverted later in < URL > I've tried to manually rebuild the Minikube iso myself by effectively reinstating the changes in #8187 , but I've ended up with a Virtualbox VM that doesn't start . 棣冩Д	0	1
1 1 1.0 1.0 1.1 1.0 1.15 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 2 7 28	[ 1.13 ] Add wait = false as a minikube config . Great to see : minikube start -- wait = false . added which I think will help with #4588 It would also be extremely helpful if this could be set using : minikube config . so false can be the default value .	2	1
1 1 1.0 1.0 0.9 1.0 0.95 1.0 1.0 1.0 0.9375 176 1.0 1 1 8 23	  site : add calendar invite link to Triage Meeting . on our site , there should be a link to add a calendar invite for triage meeting , or we could direct the users to join a the google group so they receive the invitation on their calendar automatically < URL >	0	2
0 0 0.8 1.0 0.9 1.0 1.35 2.0 0.3333333333333333 0.0 1.4193548387096775 31 2.0 6 7 11 25	kic : fix minikube tunnel for mac . for kic on mac the tunnel doesnt seem to work . it might be we need to do bridge like hyperkit < URL > on linux it works fine : : medya @medya : ~ $ kubectl create deployment hello-minikube1 -- image = k8s.gcr.io/echoserver:1.4 deployment . apps/hello-minikube1 created medya @medya : ~ $ kubectl expose deployment hello-minikube1 -- type = LoadBalancer -- port = 8080 service/hello-minikube1 exposed medya @medya : ~ $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S ) AGE hello-minikube1 LoadBalancer 10.96.29.87 10.96.29.87 8080:30223 /TCP 7s kubernetes ClusterIP 10.96.0.1 < none > 443/TCP 3m3s medya @medya : ~ $ curl 10.96.29.87 : 8080 CLIENT VALUES : client_address = 10.244.0.1 command = GET real path =/ query = nil request_version = 1.1 request_uri = < URL > SERVER VALUES : server_version = nginx : 1.10.0 - lua : 10001 HEADERS RECEIVED : accept =*/ * host = 10.96.29.87 : 8080 user-agent = curl/ 7.66.0 BODY : -no body in request-medya @medya : ~ $ .	0	0
1 1 1.0 1.0 1.2 1.5 1.05 1.0 1.0 1.0 0.0 0 0.0 1 3 8 18	CHANGE_MINIKUBE_NONE_USER env variable not setting config . json ownership . Description Even after setting the CHANGE_MINIKUBE_NONE_USER env variable the config . json is still owned by root . : export CHANGE_MINIKUBE_NONE_USER = true sudo -E minikube start -- vm-driver = none sudo ls -lrt /home/ubuntu/ . minikube/machines/minikube/config . json . Output : -rw ------- 1 root root 2123 Feb 25 15:33 /home/ubuntu/ . minikube/machines/minikube/config . json . minikube version : v 0.34.1 OS : ubuntu 16.04 on EC2(AWS )	2	0
2 2 1.0 1.0 1.0 1.0 0.95 1.0 1.6666666666666667 2.0 1.108910891089109 101 1.0 2 10 22 75	CI : add download-only for containerd in tests . once this is merged < URL >	0	1
2 2 1.6 2.0 1.3 1.5 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 1 2 4 29	long-running tunnel breaks cluster connectivity : ssh : handshake failed : connection reset by peer . The exact command to reproduce the issue : minikube ip But the truth is , I kept minikube tunnel running for a while . I can access everything using kubectl , but when I want to do any command that communicates with the cluster in any way , it just hangs . I have done nothing special , just minikube start , in an other window minikube tunnel . I also tried minikube tunnel -- cleanup , did not fail , but did nothing either . Then i tried minikube ip -v = 7 : Using SSH client type : native &{{{ 0 [] [] []} docker [ 0x83b580 ] 0x83b550 [] 0s } 127.0.0.1 37871 } About to run SSH command : ip addr show and after a while : Error dialing TCP : ssh : handshake failed : read tcp 127.0.0.1 : 35568 -> 127.0.0.1 : 37871 : read : connection reset by peer The full output of the command that failed : It just hangs no output . The output of the : minikube logs . command : That just hangs too . The operating system version : Kubuntu 17.10	2	0
0 0 0.4 0.0 0.9 0.5 0.8 1.0 0.0 0.0 0.0 0 0.0 1 4 6 22	-- insecure-registry does not work on an existing VM . minikube version : v 1.1.1 operating system version : macOS 10.14 : minikube start / -- docker-env = HTTP_PROXY = $HTTP_PROXY / -- docker-env = HTTPS_PROXY = $HTTPS_PROXY / -- docker-env = NO_PROXY = 	2	0
2 2 1.2 1.0 1.2 1.0 1.1 1.0 1.3333333333333333 1.0 2.0 1 2.0 1 2 8 37	Kernel with CONFIG_FTRACE_SYSCALLS kubectl-trace . I would like the Minikube kernel to be compiled with < URL > ( basic tracer to catch the syscall entry and exit events ) . This is to be able to use kubectl-trace on Minikube . Related issue in kubectl-trace : < URL > Symptoms : : $ kubectl trace run minikube -e ' tracepoint : syscalls : sys_enter_open { printf('%s %s/n ' , comm , str(args -> filename )); }' trace 09ecf3b8-bc53-11ea-ac93-c85b763781a4 created $ kubectl logs kubectl-trace-09ecf3b8-bc53-11ea-ac93-c85b763781a4-4m667 if your program has maps to print , send a SIGINT using Ctrl-C , if you want to interrupt the execution send SIGINT two times ERROR : tracepoint not found : syscalls : sys_enter_open exit status 1 . This is a follow-up to previous efforts to make Minikube work with BPF tools in < URL > . See issue #8556 that added CONFIG_IKHEADERS . cc @mauriciovasquezbernal @marga -kinvolk	2	1
2 2 1.0 1.0 0.8 1.0 0.95 1.0 1.3333333333333333 1.0 0.8465116279069768 215 1.0 0 1 7 26	`minikube version -- short` should only return the plain text . no need to say minikube version for the -- short : $ minikube version -- short minikube version : v 1.19.0 . instead it should say : $ minikube version -- short v 1.19.0 .	0	0
0 0 1.0 1.0 1.2 1.0 1.05 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 0 4 32	Add documentation on how to delete images/containers/network/volumes created by minikube . I think you should explain further how to delete data when a minikube test failed ... e.g. just after ' If minikube fails to start , ... ' I found later at the end of < URL > : minikube delete -- all . But before I did that : Run as root !!! : podman system prune podman ps podman images pdoman rmi ... podman volume list pdoman volume prune . and look also $HOME/ . minikube	2	2
2 2 2.0 2.0 1.7 2.0 1.45 2.0 2.0 2.0 0.0 0 0.0 1 1 5 20	Support in VM and pods for VRF ( kernel module ) . I'm porting a virtualized network testbed from a system container . A correct migration requires moving the network pod's interface and routes into a non-default VRF . A large number of network namespaces compose the virtual net . bradmwalker/wanmap #48 VRF is enabled in Ubuntu 18.04 + and Fedora 29+ . The exact command to reproduce the issue : minikube ssh $ zcat /proc/config . gz | grep CONFIG_NET_VRF $ sudo ip link add vrf-cluster type vrf RTNETLINK answers : Operation not supported [ brad @x220 ~ ]$ minikube version minikube version : v 1.3.1 commit : ca60a424ce69a4d79f502650199ca2b52f29e631 Minikube VM is freshly created with kvm2 driver .	2	1
0 0 1.2 2.0 1.6 2.0 1.35 2.0 0.6666666666666666 0.0 1.5909090909090908 22 2.0 0 1 4 21	minikube cache add , doesn't add to other profiles unless restart . Here is how to reproduce : start two minikube profiles : minikube start minikube start -p p1 . tag an image and add it to the cache : medya@ ~ $ docker tag alpine justforfun medya@ ~ $ minikube cache add justforfun . ssh into minikube and see the images : medya@ ~ $ minikube ssh -p p1 docker images | grep justforfun medya@ ~ $ minikube ssh -p minikube docker images | grep justforfun justforfun latest 965ea09ff2eb 5 weeks ago 5.55 MB . as we see the cached image was loaded to minikube profile but not p1 however if we restart p1 , it will pick up the cached image	2	0
1 1 1.0 1.0 1.1 1.0 1.05 1.0 1.0 1.0 1.032 125 1.0 2 3 4 35	docker driver : check docker-desktop has enough cpu/ram on Mac/Windows .  	1	1
1 1 1.2 1.0 1.0 1.0 1.2 1.0 0.6666666666666666 1.0 0.6666666666666666 3 0.0 1 9 11 52	  website/handbook : don't advise users to create a symbolic link to minikube閳ユ獨 binary named 閳?kubectl 閳?. this confuses users and doesn't work ( eg , minikube calling kubectl would loopback to itself ) ref : < URL > ( < URL > : Alternatively , you can create a symbolic link to minikube閳ユ獨 binary named 閳?kubectl 閳?. ln -s $(which minikube ) /usr/local/bin/kubectl . example : < URL > previously , there were discussions in issue #8857 and pr #8872 alternatively , we can make it work by ' recognising ' that minikube was called as kubectl and assign the flags as flags for minikube's own downloaded kubectl binary ...	0	2
0 0 0.2 0.0 0.6 0.0 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 0 0 4	CentOS 7 : minikube tunnel failed with ' netstat : invalid option -- ' f '' . minikube tunnel fails with : : netstat : invalid option -- ' f ' . OS ver : : cat /etc/centos-release CentOS Linux release 7.6.1810 ( Core ) . netstat ver : : netstat -- version ne t-t ools 2.10 -alpha . deployed from the default : ne t-t ools : : ne t-t ools . x86_64 0:2 . 0-0 . 24.20131004 git . el7 .	0	0
2 2 1.8 2.0 1.8 2.0 1.35 2.0 1.6666666666666667 2.0 1.027027027027027 111 1.0 1 4 5 81	Warn about default Docker Desktop resources . on a fresh windows machine with 32 GB ram and 8 CPus I installed docker-desktop with all default settings , I noticed a painfully slower than normal minikube significantly ( I dont know how to time commands in powershell ) then I noticed docker desktop by default only has 2 GB ram , no matter what your system is . We need to 1- Detect the docker desktop resources 2- Warn the user to Increase if possible  	2	2
2 2 1.4 2.0 1.4 2.0 1.25 2.0 1.6666666666666667 2.0 0.0 0 0.0 4 7 17 66	registry aliases hosts update fails , `diff : command not found` . The hosts file update routine in registry aliases uses a recent Fedora image ( : registry.fedoraproject.org/fedora . ) that doesn't include a : diff . binary . This causes the hosts file update to fail and in turn causes the host environment to resolve registry names via DNS instead of finding aliases in the hosts file . I've worked around this for now by modifying the < URL > daemon set to use ubuntu instead of fedora . Steps to reproduce the issue : : minikube start . : minikube addons enable registry . : minikube addons enable registry-aliases . : kubectl -n kube-system logs registry-aliases-hosts-update-XXXXX -c update . Full output of failed command : The relevant output is the log from the update container in the host update pod : : 10.111.151.247 example.org 10.111.151.247 example.com 10.111.151.247 test.com 10.111.151.247 test.org bash : line 7 : diff : command not found Done . .	0	0
2 2 1.4 2.0 1.5 2.0 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 2 6 18	Support image secret and docker config . json customization for minikube . The exact command to reproduce the issue : minikube start , and when I want to create k8s deployment resource with thirdparty image registry , I need to create image secret inside k8s and create . docker/config . json inside minikube machine . Is there anyway to customize this , and avoid to manual operation for creating image secret and config . json . Thanks . The full output of the command that failed : None The output of the : minikube logs . command : None The operating system version : MacOS Mojave 10.14.6	2	1
2 2 1.0 1.0 0.7 0.5 1.0 1.0 1.0 1.0 0.0 0 0.0 0 0 4 28	Customize Hyper-V machine name . When Hyper-V driver is used , the virtual machine will be called : minikube . . Note that the VM is configured with bridge switch , so the domain name will be broadcasted in LAN . However , if two machines in a LAN both starts a Minikube , they will have same DNS name . It would be useful if user can specify the machine name , like : minikube start -- hyper v-v m-name minikube-frank . I think this is Hyper-V specific because it閳ユ獨 the only Type-1 Hypervisor officially supported .	2	1
1 1 1.2 1.0 1.4 1.5 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 2 4 14 50	Add documentation for VMware on Linux . Hello , there's no documentation available for vmware driver on Linux . I think this is very important to add in a near future :)  	2	2
2 2 1.2 1.0 1.3 1.0 1.2 1.0 1.6666666666666667 2.0 1.1485148514851484 101 1.0 0 1 5 48	When docker driver fails , we try to use an ( unspecified ) external VM instead . As seen in < URL > : 閴?Startup with docker driver failed , trying with alternate driver ssh 棣冦亞 StartHost failed , but will try again : config : please provide an IP address . It's not possible to use a remote VM (' ssh ' driver ) , without providing the address to it ... We should also not silently fallback to running locally (' none ' driver ) , because of no isolation . : 閴?Automatically selected the docker driver . Other choices : kvm2 , virtualbox , none , ssh , podman ( experimental ) . Currently the priority is ' Discouraged ' , which doesn't really the ' Unconfigured ' state properly . The goal should be to make them both opt-in , and not have them on the regular Alternatives list . : pkg/minikube/registry/drvs/none/none . go : Priority : registry . Discouraged , // requires root pkg/minikube/registry/drvs/ssh/ssh . go : Priority : registry . Discouraged , // requires external VM .	0	0
0 0 0.4 0.0 1.1 1.0 1.15 1.0 0.3333333333333333 0.0 2.0 1 2.0 0 1 4 28	Add CSI Hostpath driver as addon to minikube . I am currently working on enabling Volume Snapshots in minikube : #8343 . Enabling the snapshots is however useless without a CSI based storage provider , which the user would have to configure manually . Adding the CSI hostpath driver as an addon would not only make the volume snapshots ready for use , but provide a complete CSI based storage solution to minikube . This change would bring all the required yaml files from the driver to minikube . These files are dependent on kubernetes version , thus an upgrade of kubernetes in minikube could break it . However with a proper integration test this should be detected immediately and the yaml files upgraded accordingly . If approved , I will rework my PR ( #8461 ) and once done it would close both this issue as well as #8343 . kind/feature	2	1
2 2 1.8 2.0 1.3 2.0 1.15 1.5 1.6666666666666667 2.0 0.0 0 0.0 2 5 7 14	minikube alpine 13 . It would be nice to present the information about how to install minikube on alpine images , since many VMs work with that system . As a requirement to install it in an Alpine Linux image there is a requirement . : apk add libc6-compat . Just that . Otherwise minikube will return : : -ash : minikube : not found .  	2	2
0 0 0.4 0.0 0.8 1.0 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 1 3 5 25	update golang 1.12.9 to 1.12.10 . There is a CVE for Go and this vulnerability is fixed in 1.12.10 . There is a CVE for Go and this vulnerability is fixed in 1.12.10 . We should update it . See : < URL > See issue golang/go #34540 Seems this is fixed in 1.12.10 . I suggest we should update to 1.12.10 or later I recommend 1.12.12 /kind bug /priority important-soon	0	0
2 2 1.2 1.0 1.5 2.0 1.35 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 1 2 25	Feature Request - unset current-context after minikube stop -- only if minikube is current context . 4154 added unsetting the current context upon : minikube stop . . This is handy , but unexpected if current context is not minikube . Perhaps unsetting the context should only occur if the current context is still minikube ?  	1	1
2 2 1.8 2.0 1.6 2.0 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 0 0 3 43	`minikube docker-env -- unset` requires minikube to be running . Stopping minikube prevents the helper : minikube docker-env -- unset . from working . Since the output of this command doesn't rely on minikube to be running , it doesn't seem like it should require minikube to be running ( in fact , it of course makes a lot of sense to run it , if minikube is not running ) Steps to reproduce the issue : : $ minikube docker-env -- unset 棣冦仐 The control plane node must be running for this command 棣冩啝 To fix this , run : ' minikube start ' . Full output of : minikube start . command used , if not already included : NA : $ minikube version minikube version : v 1.11.0 commit : 57e2f55f47effe9ce396cea42a1e0eb4f611ebbd .  	1	0
0 0 0.8 1.0 1.0 1.0 1.0 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 1 3 19	Create the custom hosts file , there is no minikube with IP added . . minikube v 1.20.0 on Centos 7.7.1908 MINIKUBE_HOME = /opt/minikube . home Using the docker driver based on user configuration Starting control plane node minikube in cluster minikube Pulling base image ... Creating docker container ( CPUs = 16 , Memory = 8000MB ) ... Preparing Kubernetes v 1.20.2 on Docker 20.10.6 ... opt bip = 172.18.0.1 /16 Generating certificates and keys ... Booting up control plane ... Configuring RBAC rules ... Verifying Kubernetes components ... Using image gcr.io/k8s-minikube/storage-provisioner:v5 Enabled addons : storage-provisioner , default-storageclass Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default Steps to reproduce the issue : Create the : $MINIKUBE_HOME/ . minikube/files/etc/hosts . with the following content : - : 127.0.0.1 localhost :: 1 localhost ip6-localhost ip6-loopback fe00 :: 0 ip6-localnet ff00 :: 0 ip6-mcastprefix ff02 :: 1 ip6-allnodes ff02 :: 2 ip6-allrouters 172.17 . x.x my . some . host1 172.17 . x . y my . some . host2 . Start the : minikube . . Access the shell via : minikube ssh . . Inside the shell execute the : cat /etc/hosts . . ``` 127.0.0.1 localhost :: 1 localhost ip6-localhost ip6-loopback fe00 :: 0 ip6-localnet ff00 :: 0 ip6-mcastprefix ff02 :: 1 ip6-allnodes ff02 :: 2 ip6-allrouters 172.17 . x.x my . some . host1 172.17 . x . y my . some . host2 <--- There is no : 192.168.49.2 minikube . here . 192.168.49.1 host . minikube . internal 192.168.49.2 control-plane . minikube . internal ``` Full output of : minikube logs . command : N/A Full output of failed command : N/A  	1	0
2 2 1.0 1.0 1.3 1.5 1.25 1.5 1.3333333333333333 1.0 0.0 0 0.0 0 1 2 21	  Update minikube tutorial on kubernetes website on addons . < URL > minikube addons enable heapster X enable failed : property name ' heapster ' not found	0	2
2 2 1.2 1.0 1.4 1.5 1.45 2.0 1.3333333333333333 1.0 1.176991150442478 113 1.0 0 3 6 25	Implement `minikube image save` . Currently there is an option ( -- push ) to upload the image to a registry : : minikube image build -- tag image -- push -- file Dockerfile . . But there is no way to save the image into the cache , for later ' load ' . So we could add a new matching command : : minikube image save . It would save an existing image as an archive , possibly load/push it : : minikube image save image image . tar . : # this would save the image in cache - and either load the image into the docker daemon , or push it to the remote registry minikube image save image -- daemon = true : Cache image to docker daemon ( load ) -- remote = true : Cache image to remote registry ( push ) . : minikube image load image . tar . : # this will load the image from cache - by either save the image from the docker daemon , or pull it from the remote registry minikube image load image -- daemon = false : Cache image from docker daemon ( save ) -- remote = false : Cache image from remote registry ( pull ) . Could also add a short-hand option : -- cache . directly to : build . command . This would make the image available under : ~ / . minikube/cache/images . The default is to always save images in the cache when loading them . And never save images in the cache ( on the host ) when building them . Needs PR #10742 for the : minikube image build . command .  	1	1
2 2 1.2 1.0 1.3 1.5 1.35 2.0 1.0 1.0 0.0 0 0.0 2 2 8 15	macos/vbox : waiting for kube-proxy to be up for configmap update : timed out . Is this a BUG REPORT or FEATURE REQUEST ? ( Bug Report ): Please provide the following details : Environment : MacOS Mojave Version 10.14.2 Minikube version ( use : minikube v 0.31.0 . ): - OS ( e.g. from /etc/os-release ): - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): virtualbox - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): minikube-v 0.33.1 . iso v 0.33.1 Install tools : - Others * : The above can be generated in one go with the following commands ( can be copied and pasted directly into your terminal ): : minikube version echo ' ; echo ' OS:'; cat /etc/os-release echo ' ; echo ' VM driver:'; grep DriverName ~ / . minikube/machines/minikube/config . json echo ' ; echo ' ISO version ' ; grep -i ISO ~ / . minikube/machines/minikube/config . json . What happened : E0129 13:48:41 . 133127 90823 start . go : 210 ] Error parsing version semver : Version string empty E0129 14:10:43 . 188119 92531 start . go : 348 ] Error restarting cluster : restarting kube-proxy : waiting for kube-proxy to be up for configmap update : timed out waiting for the condition What you expected to happen ** : I expected the cluster to start up as normal How to reproduce it ( as minimally and precisely as possible ): Output of : minikube logs . ( if applicable ) : Anything else do we need to know : No	2	0
1 1 1.0 1.0 1.3 1.0 1.15 1.0 1.3333333333333333 1.0 0.5 2 0.5 1 1 3 21	auto-pause not working - kubectl hangs/fails to connect . Start minikube with auto-pause , kubectl commands all hang indefinitely or fails to connect .	0	0
2 2 1.6 2.0 1.0 1.0 1.1 1.0 1.3333333333333333 1.0 0.6842105263157895 19 0.0 1 2 9 26	  Automatically export all integration test information to documentation .	0	2
2 2 1.6 2.0 1.6 2.0 1.35 2.0 1.3333333333333333 2.0 1.0 1 1.0 0 1 4 18	Proposal for using mirror image for Chinese developer with Alibaba Cloud Registry Service . Steps to reproduce the issue : There are some fixes for making Chinese developer to use Minikube with the mirrored images from Alibaba Cloud Registry Service . And I hope we can fix it more systematically . and I create one prototype for that . Firstly , define the mirror mapping from the original repo to the mirrored repo , e.g. < URL > When enable one specific addon , it will check if the mirror will be used . If yes , it will generate the custom image and custom repo for override . < URL > Any comment or suggestion for that ? BTW , I built some simple tool to sync the minikube images to Alibaba Cloud Registry Service ' registry.cn-hangzhou.aliyuncs.com/google_containers ' everyday . In case anything missed , pls open one issue for me , and I will fix it soon . Thanks	0	1
2 2 1.2 1.0 1.1 1.0 0.85 1.0 1.0 1.0 1.0416666666666667 72 1.0 0 5 6 20	-- download-only does not download VM drivers . Something I noticed while testing v 1.6.1 on macOS .  	1	0
0 0 1.0 1.0 1.4 2.0 1.45 2.0 1.0 1.0 0.0 0 0.0 2 3 6 39	Minikube does not update the memory allocation for a stopped VirtualBox instance . The exact command to reproduce the issue : 1 . : minikube start -- vm-driver virtualbox . 2 . : minikube stop . 3 . : minikube start -- vm-driver = virtualbox -- memory = 4g . To improve the UX , we could 1 ) update the VM configuration if it's different than what already exists , or 2 ) Print a warning or error out if the configuration does not match the user's request .  	1	1
2 2 1.4 1.0 1.1 1.0 1.0 1.0 1.3333333333333333 1.0 1.8 5 2.0 3 6 9 27	Add documentation for virtualbox DNS settings . We need to document the options added by PR < URL > by @cvila84 for virtualbox DNS settings in the docs -- dns-proxy and -- host-dns-resolve    	1	2
2 2 1.2 1.0 1.4 2.0 1.15 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 0 0 15	`Property name kube-dns not found` . Whenever I try to enable kube-dns with : minikune addons enable kube-dns . I get : Property name kube-dns not found . eventhough this addon is listed : : 閴?~ minikube addons list - addon-manager : enabled - dashboard : enabled - default-storageclass : enabled - efk : disabled - freshpod : disabled - gvisor : disabled - heapster : enabled - ingress : disabled - kube-dns : disabled - metrics-server : disabled - nvidia-driver-installer : disabled - nvidia-gpu-device-plugin : disabled - registry : disabled - registry-creds : disabled - storage-provisioner : enabled - storage-provisioner-gluster : disabled . Environment : Minikube version ( use : minikube version . ): v 0.33.1 - OS MacOS 14.4.3 - VM Driver : virtualbox - ISO version : ` ' Boot2DockerURL ' : ' file :// /Users/kamilgregorczyk/ . minikube/cache/iso/minikube-v 0.33.1 . iso ' ,  	1	0
2 2 1.2 2.0 1.3 2.0 1.25 1.5 1.3333333333333333 2.0 1.4827586206896552 29 2.0 3 5 8 22	verify kic work offline . currently we pull one cni image on the fly , we should cache that as well so minikube can run offline .	0	1
2 2 2.0 2.0 1.3 2.0 1.35 2.0 2.0 2.0 0.9727272727272728 110 1.0 4 5 10 46	 kubectl top pods ' requires heapster - reinstate addon ? . The heapster addon was removed some time ago due to it being deprecated . I guess it isn't deprecated enough ! : kubectl top pods Error from server ( NotFound ): the server could not find the requested resource ( get services http : heapster :) $ minikube addons enable heapster 棣冩寴 enable failed : run callbacks : heapster is not a valid addon 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > $ kubectl version Client Version : version . Info{Major:' 1 ' , Minor:' 18 ' , GitVersion:' v 1.18.0 ' , GitCommit:' 9e991415386e4cf155a24b1da15becaa390438d8 ' , GitTreeState:' clean ' , BuildDate:' 202 0-0 3-26T 06:16:15 Z ' , GoVersion:' go 1.14 ' , Compiler:' gc ' , Platform:' darwin/amd64 ' } Server Version : version . Info{Major:' 1 ' , Minor:' 18 ' , GitVersion:' v 1.18.3 ' , GitCommit:' 2e7996e3e2712684bc73f0dec .  	1	0
0 0 1.0 1.0 1.4 2.0 1.4 2.0 0.6666666666666666 0.0 1.3333333333333333 33 2.0 8 9 13 27	kic : support multiple profiles . currently the container is bind to apiport on the host , for a second profile we will need to 1- store the port in config 2- choose random free open port if taken by other profile .	0	1
0 0 0.6 0.0 0.9 1.0 0.85 1.0 0.3333333333333333 0.0 2.0 1 2.0 1 8 24 79	bump kubernetesui/dashboard : v 2.0.0 . kubernetesui/dashboard v 2.0.0 is officially released . < URL > I'll send PR to bump minikube addon later .	0	1
1 1 0.4 0.0 0.6 0.0 0.7 0.0 0.6666666666666666 1.0 0.968944099378882 161 1.0 0 2 7 32	  Site : auto detect user's machine in browser for installation steps . < URL >	0	2
2 2 1.2 1.0 1.5 2.0 1.3 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 5 6 30	How to run apiserver in verbose mode ? . I am running minikube v 1.0.1 on Mac OS High Sierra using VirtualBox 6.0.6 . Is it possible to run the kubernetes api-server in minikube with maximum log verbosity ? : $ minikube start -- v 4 . didn't work for me . When I exec into the api-server container and do ps , the api-server commandline didn't have -- v = 4 in it . So , minikube is not passing the -- v = 4 down to the api-server . Thanks .  	2	2
0 0 0.6 1.0 1.0 1.0 1.05 1.0 0.6666666666666666 1.0 1.0317460317460319 126 1.0 1 3 11 38	flag delete-on-failure should delete if there is a cluster with different driver . if you have a cluster running with different driver and you wanna change the driver u get this error : : $ minikube start -- driver = hyperkit $ minikube start -- delete-on-failure -- driver = docker 棣冩 minikube v 1.12.0 -beta . 0 on Darwin 10.15.5 棣冩寽 The existing ' minikube ' VM was created using the ' hyperkit ' driver , and is incompatible with the ' docker ' driver . 棣冩啝 To proceed , either : 1 ) Delete the existing ' minikube ' cluster using : ' minikube delete ' * or * 2 ) Start the existing ' minikube ' cluster using : ' minikube start -- driver = hyperkit ' . but we should delete it for them since they tell us to delete ! this would be important because when we make the docker driver default many users would have a pre-existing hyperkit hyperv that wanna delete for them if they specified the flag	0	1
2 2 1.6 2.0 1.4 2.0 1.35 2.0 2.0 2.0 0.4444444444444444 27 0.0 2 5 11 52	Add more complex integration tests for `minikube image load` . In #10366 I added a simple test for the : minikube image load . command : 1 . Pull busybox 2 . Try to load it into minikube 3 . Make sure busybox exists in minikube It would be nice to beef up the tests a little bit and also verify the following cases : 1 . Try to load an image that already is loaded in , and make sure nothing breaks 2 . Load in an image with a new tag and make sure the new tag exists in the daemon If anyone would be interested in picking this up , please feel free to assign this issue to yourself by commenting /assign .   	1	1
0 0 1.4 2.0 1.0 1.0 1.1 1.0 1.0 1.0 0.8266666666666667 225 1.0 0 0 1 30	new containerd config causes KVM Containerd test to time out and Docker Containerd test addons to fail . After New Containerd Config in this PR < URL > the KVM test times out like this in 90 minutes ( used to be Green in 63 minutes ) here is full KVM logs that timed out < URL > Also the Docker Driver Test Addons Fail < URL > The integraiton tests run on Debian 9 : jenkins @debian -jenkins-agent-9 : ~ $ uname -r 4.9.0 -15-amd64 . the New Containerd Config improves the perofmrance on cloud shell interestingly after that PR the performance chart on cloud shell shows , containerd has same performance as Docker after that PR the cloud shell linux uses kernel 5 : medya @cloudshell : ~ $ uname -r 5.4.104 + .	0	0
0 0 1.2 1.0 0.9 1.0 1.1 1.0 1.0 1.0 0.890625 192 1.0 0 5 7 50	investigate running minikube inside a pod . pre requisite for us to add minikube to prow so basically run minikube inside minikube as a pod	0	1
2 2 0.6 0.0 0.3 0.0 0.75 0.0 0.6666666666666666 0.0 0.9467455621301775 169 1.0 1 5 10 41	fix docker network inspect to handle networks without MTU . while this is rate but it might happen ( if user uses a bad docker verison once and then later upgrade docker but still have a stuck old minikube network ) to replicate , create a network without mtu : $ docker network create m1 c681cfeab80f7cce0e16948aa6c31b2c451d03974e2ec3675ddbba54c7f3c8c9 . Currrent behaviour ' using this code < URL > it will run this : : $ docker network inspect m1 -- format ' {' Name ' : ' {{ . Name }}' , ' Driver ' : ' {{ . Driver }}' , ' Subnet ' : ' {{ range . IPAM . Config }}{{ . Subnet}}{{end }}' , ' Gateway ' : ' {{ range . IPAM . Config }}{{ . Gateway}}{{end }}' , ' MTU ' : {{( index . Options ' com . docker . network . driver . mtu ' )}} , {{$first : = true }} ' ContainerIPs ' : [{{ range $k , $v : = . Containers }}{{ if $first}}{{$first = false}}{{else }} , {{ end}}'{{$v . IPv4Address}}'{{end }}]}' . which outputs this ( and it is missing MTU ) : {' Name ' : ' m1 ' , ' Driver ' : ' bridge ' , ' Subnet ' : ' 172.19.0.0 /16 ' , ' Gateway ' : ' 172.19.0.1 ' , ' MTU ' : , ' ContainerIPs ' : []} . we want it to fix the go template to instead return 0 for the MTU so it does NOT have a parsing error . so it should return desired behaviour : {' Name ' : ' m1 ' , ' Driver ' : ' bridge ' , ' Subnet ' : ' 172.19.0.0 /16 ' , ' Gateway ' : ' 172.19.0.1 ' , ' MTU ' : 0 , ' ContainerIPs ' : []} . Originally posted by @baiwfg2 in < URL >	0	0
1 1 0.8 1.0 1.2 1.0 1.05 1.0 0.6666666666666666 1.0 0.0 0 0.0 1 3 7 30	Update documentation to show docker as a preferred driver . Minikube switched to Docker driver by default on macOS in 1.12 . But documentation still says that Hyperkit driver is preferred : < URL >  	2	2
0 0 1.0 1.0 1.0 1.0 1.1 1.0 0.6666666666666666 1.0 1.25 28 1.0 0 1 1 18	The istio-provisioner addon image is only valid for the amd64 platform . So we should probably not enable it by default , for any other architectures ( e.g. arm64 ) ? : image : docker.io/istio/operator:1.4.0 . < URL > It only fails to run , anyway ,.  	1	1
0 0 0.8 0.0 0.8 0.5 0.8 1.0 0.0 0.0 1.6666666666666667 15 2.0 1 2 7 29	Site : add mailing list and groups details to community page . we need to add the community related docs on the README.md to < URL > for example links to minikube-users mailing list minikube-dev mailing list which is only on the README.md should be there    	1	2
0 0 1.4 2.0 1.1 1.0 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 1 4 32	mount : Error finding IPV4 address for VirtualBox Host-Only Ethernet Adapter . minikube . exe mount E : /docker/testk8s : /testk8s X Error getting the host IP address to use from within the VM : Error getting VM/ Host IP address : Error finding IPV4 address for VirtualBox Host-Only Ethernet Ad apter #2	2	0
1 1 0.8 1.0 1.1 1.0 1.15 1.5 1.3333333333333333 1.0 1.2040816326530612 49 1.0 0 9 11 56	Make the persistent storage actually survive reboots . For the KIC ( and for ' none ' ) , we need to make sure that the persistant directories are preserved : < URL > These are persisted today : : /var/lib/minikube /var/lib/docker /var/lib/containers . ( the third one is for crio , and for podman ) These also need to be kept : : /data /tmp/hostpath_pv /tmp/hostpath-provisioner . ( I don't think the third one is used anymore ) We can put them on the docker/podman volume , and at least mention them in the ' none ' docs ... They are normally bind mounts , to some place that is persistent ( like disk image or docker volume ) : TARGET SOURCE FSTYPE OPTIONS /tmp/hostpath_pv /dev/sda1[/hostpath_pv ] ext4 rw , relatime /tmp/hostpath-provisioner /dev/sda1[/hostpath-provisioner ] ext4 rw , relatime /mnt/sda1 /dev/sda1 ext4 rw , relatime /var/lib/docker /dev/sda1[/var/lib/docker ] ext4 rw , relatime /var/lib/containers /dev/sda1[/var/lib/containers ] ext4 rw , relatime /data /dev/sda1[/data ] ext4 rw , relatime /var/lib/minikube /dev/sda1[/var/lib/minikube ] ext4 rw , relatime . A nice change to go with this one , would be to stop volume mounting all of : /var . ( at the top level ) . Because it conflicts with existing paths , in the ubuntu image . Like : /var/lib/dpkg/alternatives . . See #8056 and #8100 The kicbase doesn't boot without it .	0	0
2 2 1.2 1.0 1.1 1.0 1.2 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 1 1 3	` . /minikube logs -f` do not work . Is this a BUG REPORT or FEATURE REQUEST ? : BUG REPORT Please provide the following details : Environment : Minikube version : v 0.24.1 - OS : CentOS-7 - VM Driver : none - Install tools : : curl -Lo minikube < URL > && chmod +x minikube . What happened : Run command : . /minikube start -- vm-driver = none . Command end successfully , and a local k8s cluster is running as expected : : kubectl . commands works . Run command : . /minikube logs . , gets the logs of the running localkube instance as expected Run command : . /minikube logs -f . , nothing shows up . What you expected to happen : The help message for minikube logs says : -f , -- follow Show only the most recent journal entries , and continuously print new entries as they are appended to the journal . I hope this command can work as it says . How to reproduce it : - Run command : . /minikube logs -f . - Run command : . /minikube logs . and check the latest message timestamps , confirms new logs do exist during the time we run : . /minikube logs -f . . Anything else do we need to know : cat /tmp/minikube . INFO : Log file created at : 2018/01/23 11:38:11 Running on machine : GLB-Shanghai-ChenLi-003 Binary : Built with gc go 1.9.1 for linux/amd64 Log line format : [ IWEF]mmdd hh : mm :ss . uuuuuu threadid file : line ] msg I0123 11:38:11 . 666772 7165 notify . go : 109 ] Checking for updates ... I0123 11:38:11 . 718853 7165 exec_runner . go : 49 ] Run with output : if [[ `systemctl` = ~ -/ . mount ]] & > /dev/null ; then sudo journalctl -f -u localkube else tail -n +1 -f /var/lib/localkube/localkube . err /var/lib/localkube/localkube . out fi .  	1	0
2 2 1.8 2.0 1.3 1.5 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 3 9 47	Minikube's Hyper-V Driver ignores Hyper-V Settings . I was looking at : < URL > and I wonder why this hasn't been fixed . What is the point of using Hyper-V if Minikube it isn't using those resources ? I have several testing profiles and they each take 5G+ in the C : drive . I have set Hyper-V to use other drives but Minikube ignores those settings . Why ?	0	1
2 2 1.0 1.0 1.2 1.5 1.1 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 0 2	Update build bot to use ' minikube ' AUR instead of ' minikube-bin ' . I'm writing to inform you that I renamed the AUR package to minikube-bin . I've given you a heads-up in the package comments but there was no response . It is common to give users the information in the package name if they are installing from source or whether they are installing a binary straight from upstream . This issue exists so you can update your build scripts to point at this new name . Furthermore it would be nice if you could provide a package built from source at the now-vacant minikube name in the AUR .	2	0
0 0 1.0 1.0 1.0 1.0 1.05 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 1 10 25	Exposing ingress to a remote host . Hi , I am running a vagrant box using virtual box ( running headless ubuntu 18.04 ) on windows 10 host machine . Inside the virtual box , I have minikube set up using docker as the vm-driver : minikube start -- memory = 6144 -- cpus = 2 -- disk-size = 40g -- vm-driver = docker -- bootstrapper kubeadm -- kubernetes-version = 1.17.4 . My application is exposed via an ingress to the ubuntu machine running inside virtual box and I am able to access the application via wget/cURL On doing minikube IP it gave me the IP of the docker container in which minikube runs Some additional configuration info - Vagrant file - I would like to access the application from my windows machine's browser , any idea how to achieve that ? vagrant port forwarding doesn't seem to help . Thanks	2	1
1 1 1.2 1.0 1.1 1.0 1.1 1.0 1.0 1.0 0.0 0 0.0 0 0 4 9	Allow users to specify a VM disk location for HyperV . Feature Request There should be an extra flag something like : -- hyper v-v m-disklocation . such that you can specify where you want the VMdisk to be located . This is because the C :/ might not always have enough space . Or at the very least have an option to let hyperv create the vmdisk similar to what docker for windows does , this way one could configure the VMDisk location from Hyper-V Please provide the following details : Environment : Minikube version : - v 0.28.2 - hyperv	2	1
0 0 1.4 2.0 1.1 1.5 1.0 1.0 1.0 1.0 1.0598290598290598 117 1.0 2 8 14 57	hyper driver health : nil pointer . this nil pointer breaks minikube , so minikube wont be able to do checks for other drivers . : PS C : /Users/jenkins/Downloads > . /minikube-windows-amd64 . exe start * minikube v 1.10.1 on Microsoft Windows Server 2019 Datacenter 10.0.17763 Build 17763 * Using the hyperv driver based on existing profile * Starting control plane node minikube in cluster minikube * Creating hyperv VM ( CPUs = 2 , Memory = 6000MB , Disk = 20000MB ) ... panic : runtime error : index out of range [ 0 ] with length 0 goroutine 132 [ running ]: github.com/docker/machine/drivers/hyperv.hypervAvailable(0xc000398cc0 , 0xc000258910 ) /go/pkg/mod/ github.com/machine-drivers/machine@v0.7.1-0.20200323212942-41eb826190d8/drivers/hyperv/powershell.go:64 +0x105 github.com/docker/machine/drivers/hyperv.(* Driver ) . PreCreateCheck(0xc0003bf680 , 0x0 , 0x0 ) /go/pkg/mod/ github.com/machine-drivers/machine@v0.7.1-0.20200323212942-41eb826190d8/drivers/hyperv/hyperv.go:171 +0x3b k8s.io/minikube/pkg/minikube/machine.(* LocalClient ) . Create(0xc000351e00 , 0xc0003bf6e0 , 0x0 , 0x0 ) /app/pkg/minikube/machine/client . go : 221 +0x3f5 k8s.io/minikube/pkg/minikube/machine.timedCreateHost.func2(0x1e65f40 , 0xc000351e00 , 0xc0003bf6e0 , 0xc000413720 , 0xc00006e540 ) /app/pkg/minikube/machine/start . go : 167 +0x42 created by k8s.io/minikube/pkg/minikube/machine.timedCreateHost /app/pkg/minikube/machine/start . go : 166 +0x10e PS C : /Users/jenkins/Downloads > .	0	0
2 2 1.0 1.0 1.3 2.0 1.15 1.0 1.0 1.0 0.0 0 0.0 0 0 1 6	  Document how to build and use minikube for offline development . I want to add a feature to minikube , but I am having trouble finding documentation on how to build minikube from source . Are there docs ? I am seeing the following error when I try to build without making any code changes but I have a question of what goals do I need to actually make minikube ? Help ! (:	0	2
1 1 1.2 1.0 1.1 1.0 1.15 1.0 0.6666666666666666 1.0 1.0 4 1.0 3 11 13 45	Copy in vmpath . GuestPersistentDir/minikube/binaries into preloaded images . This should save a couple seconds on : minikube start . , ref < URL >	0	1
0 0 0.8 0.0 1.0 1.0 1.35 2.0 0.6666666666666666 0.0 0.0 0 0.0 1 1 3 31	minikube requires docker upgrade to v 19.03 + to address issues with chown on COPY . The exact command to reproduce the issue : Have a docker file with COPY -- chown =${ USER }: ${GROUP } FILENAME . docker build fails within minikube context . The full output of the command that failed : unable to convert uid/gid chown string to host mapping : can't find uid for user ${USER }: no such user : ${USER } The output of the : minikube logs . command : docker info | grep ' Server Version ' Server Version : 18.09.9 which does not have the docker fix The operating system version : macOS 10.14.6 Relates < URL >	0	1
2 2 1.4 2.0 1.5 2.0 1.3 2.0 1.6666666666666667 2.0 1.0 65 1.0 0 2 5 33	hyperv : Check if user has access to start Hyper-V VM's . The user needs elevated privileges , and needs to be either in the admin group or the hyperv admin group .	1	1
2 2 1.2 1.0 1.2 1.0 1.5 2.0 1.6666666666666667 2.0 1.0793650793650793 63 1.0 3 3 7 20	Document minikube docker driver version requirements . Kubernetes itself supports ancient versions , but KIC doesn't really ... : kubeadm . says : On each of your machines , install Docker . Version 19.03.11 is recommended , but 1.13.1 , 17.03 , 17.06 , 17.09 , 18.06 and 18.09 are known to work as well . Keep track of the latest verified Docker version in the Kubernetes release notes . Should verify the Docker version , like we do with the Podman version ? And have a similar recommendation for minikube , on the driver page . EDIT : I don't think we want to keep supporting 17.03 - 18.09 - or even earlier . < URL > 2017-03-01 This release includes bugfixes for 1.13.1 but there are no major feature additions and the API version stays the same . Upgrading from Docker 1.13.1 to 17.03.0 is expected to be simple and low-risk . < URL > 2019-07-25 Security Fixed loading of nsswitch based config inside chroot under Glibc . CVE-2019-14271 So the easiest would be to require Docker Engine 19.03 + or Docker Desktop 2.1 + ? < URL > 2019-08-08 Upgrades Docker 19.03.1 Kubernetes 1.14.3 Otherwise we need to look at version and API , and implement workarounds .  	2	2
2 2 1.4 2.0 1.1 1.5 0.95 1.0 1.6666666666666667 2.0 1.0330578512396693 121 1.0 0 0 12 41	docker : mount on windows doesn't work . I believe this is same as the hyperv issue that we had closed without fixing . : PS C : /Users/jenkins/actions-runner/_work/minikube/minikube/minikube_binaries > . /minikube-windows-amd64 . exe mount -p minikube C : /Users/jenkins/actions-runner/_work/minikube/minikube/minikube_binaries : /mount-9p * Mounting host path C : /Users/jenkins/actions-runner/_work/minikube/minikube/minikube_binaries into VM as /mount-9p ... - Mount type : < no value > - User ID : docker - Group ID : docker - Version : 9p2000 . L - Message Size : 262144 - Permissions : 755 (-rwxr-xr-x ) - Options : map [] - Bind Address : 127.0.0.1 : 50211 * Userspace file server : ufs starting * X mount failed : mount with cmd /bin/bash -c ' sudo mount -t 9p -o dfltgid =$( grep ^docker : /etc/group | cut -d : -f3 ) , dfltuid =$( id -u docker ) , msize = 262144 , port = 50211 , trans = tcp , version = 9p2000 . L 192.168.65.2 /mount-9p ' : /bin/bash -c ' sudo mount -t 9p -o dfltgid =$( grep ^docker : /etc/group | cut -d : -f3 ) , dfltuid =$( id -u docker ) , msize = 262144 , port = 50211 , trans = tcp , version = 9p2000 . L 192.168.65.2 /mount-9p ' : Process exited with status 32 stdout : stderr : mount : /mount-9p : unknown filesystem type ' 9p ' . * * minikube is exiting due to an error . If the above message is not useful , open an issue : - < URL > . the root cause is not clear yet . but It could we need to add a powershell-exec runner	0	0
2 2 1.4 2.0 1.3 1.5 1.15 1.0 1.0 1.0 0.0 0 0.0 4 6 8 38	update kubernetes blog post to use docker driver instead of none . ~ $ sudo minikube start 棣冩 minikube v 1.11.0 on Ubuntu 18.04 閴?Using the none driver based on existing profile 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩敡 Restarting existing none bare metal machine for ' minikube ' ... 閳╃櫢绗?OS release is Ubuntu 18.04.4 LTS 閴?[ NONE_DOCKER_EXIT_5 ] Failed to enable container runtime sudo systemctl start docker : exit status 5 stdout : stderr : Failed to start docker . service : Unit docker . service not found . 棣冩寱 Suggestion : Ensure that Docker is installed and healthy : Run ' sudo systemctl start docker ' and ' journalctl -u docker ' . Alternatively , select another value for -- driver 棣冩憣 Documentation : < URL > 閳﹀绗?Related issue : < URL >    	1	2
0 0 1.2 1.0 0.9 1.0 0.95 1.0 1.0 1.0 0.5263157894736842 19 0.0 0 0 1 9	VM drivers : ` -- mount` & ` -- mount-string` doesn't persist between stops . Docker retains mount settings after a stop , but VM drivers do not . : $ minikube start -- driver = hyperkit -- mount ... $ minikube ssh -- ls /minikube-host Users ... $ minikube stop ... $ minikube start ... $ minikube ssh -- ls /minikube-host ls : cannot access ' /minikube-host ' : No such file or directory ssh : Process exited with status 2 .	0	1
1 1 1.4 2.0 1.3 1.5 1.35 2.0 1.6666666666666667 2.0 0.8775510204081632 196 1.0 3 5 11 49	auto-pause : add support for internal api-serfver requests . minikube should un-pause the api-server when the requests are coming from internal for example the ' operators ' or corn jobs ...	0	1
2 2 1.2 1.0 1.3 1.5 1.45 2.0 1.3333333333333333 1.0 0.0 0 0.0 1 3 13 49	SSH tunnels left open after sending SIGINT to `minikube tunnel` . Steps to reproduce the issue : Minikube running with docker driver on macOS Catalina : minikube tunnel . On another terminal : : kill -INT < PID_OF_THE_PREVIOUS_COMMAND > . : ps aux | grep minikube . Full output of failed command : The : ps . command will show multiple ssh tunnels still open , outliving the parent process . Details In pkg/minikube/tunnel/kic/ssh_tunnel . go the interrupt signal is caught and it stops the LoadBalancerEmulator . However , it assumes that the interrupt will also propagate to the children ssh tunnels , which are running in a goroutine inside : startConnections . . The interrupt , however , kills the goroutine and the blocking call which waits for the child process to finish . This leaves the ssh tunnels dangling . On the other hand , if : CTRL + C . is sent on the same terminal as : minikube tunnel . and the clean-up routine included killing all ssh tunnels , the OS would error with ' process already finished ' . Related issues < URL > < URL >  	1	0
1 1 0.8 1.0 0.9 1.0 1.25 1.5 0.6666666666666666 1.0 0.5 20 0.0 0 0 1 12	Installing cri-o from deb packages causing errors . When running a kic image build we get the following error : : Failed to fetch < URL > ~ 0_amd64 . deb File has unexpected size ( 20107660 ! = 20107360 ) . Mirror sync in progress ? . We should install a new way that doesn't use deb packages so we can continue to build our kic images . Related < URL >	0	0
2 2 0.8 1.0 0.7 0.5 0.9 1.0 1.3333333333333333 1.0 0.8114754098360656 244 1.0 0 1 1 11	minikube start -- kubernetes-version should also take ' newest ' . currently we support : minikube start -- kubernetes-version = latest . but newest doesnt work : $ make ; mk start -- kubernetes-version = newest make : `out/minikube ' is up to date . 棣冩 minikube v 1.24.0 on Darwin 12.0.1 ( arm64 ) 閴?Exiting due to MK_USAGE : Unable to parse ' newest ' : No Major . Minor . Patch elements found .	0	1
2 2 1.8 2.0 1.5 2.0 1.35 2.0 2.0 2.0 0.75 4 0.5 2 3 7 37	Add JoinCluster function to bootstrapper . This is the actual : kubeadm join . portion of the multinode plan . This is will be hooking up all the nodes together . Remember to change minikube status . ref #94	0	1
1 1 1.4 2.0 1.4 2.0 1.3 1.5 1.6666666666666667 2.0 1.1411764705882352 85 1.0 3 3 7 35	Build machine drivers for different architectures . Currently these are hardcoded to amd64 : docker-machine-driver-kvm2 < URL > docker-machine-driver-hyperkit < URL > We need to start building them also for arm64 . Ultimately we will also soon have to start looking at building and hosting all machine drivers , since upstream is dying . That would in that case also include docker-machine-driver-vmware and docker-machine-driver-parallels , from ' registry ' It would also be nice to get them untangled from the minikube source code and git repository , but that is a stretch target . Currently we allow some drivers in-tree , like virtualbox and hyperv , but not others . So the situation is a bit confusing . virtualbox hyperv ~ ~ vmwarefusion ~ ~ none generic < URL >	0	1
2 2 1.2 1.0 0.9 1.0 0.95 1.0 1.0 1.0 0.0 0 0.0 1 2 5 26	MinikubeRunner Copy function doesn't work and is unused . I was trying to use the MinikubeRunner < URL > function in an integration test , and nothing is copied over ( because the : cat . command doesn't run anything ! ) : cmd : = exec . Command('/bin/bash ' , ' -c ' , path , ' ssh ' , ' --' , fmt . Sprintf('cat >> %s ' , filepath . Join(f . GetTargetDir () , f . GetTargetName ()))) . It also doesn't seem like this function is used anywhere , so maybe removing it would be the best solution ?	2	0
2 2 1.4 1.0 1.1 1.0 1.25 1.0 1.3333333333333333 1.0 0.0 0 0.0 2 2 5 53	virtualbox on windows : Error VirtualBox Host-Only Ethernet Adapter . Steps to reproduce the issue : minikube start minikube v 1.10.0 on Microsoft Windows 10 Home Single Language 10.0.17134 Build 17134 Using the virtualbox driver based on existing profile Starting control plane node minikube in cluster minikube Restarting existing virtualbox VM for ' minikube ' ... Found network options : NO_PROXY = 192.168.99.100 no_proxy = 192.168.99.100 Preparing Kubernetes v 1.18.1 on Docker 19.03.8 ... env NO_PROXY = 192.168.99.100 env NO_PROXY = 192.168.99.100 E0513 02:22:00 . 961660 9096 start . go : 95 ] Unable to get host IP : Error getting VM/Host IP address : Error finding IPV4 address for VirtualBox Host-Only Ethernet Adapter #4 * X failed to start node : startup failed : Failed to setup kubeconfig : Error getting VM/Host IP address : Error finding IPV4 address for VirtualBox Host-Only Ethernet Adapter #4 * minikube is exiting due to an error . If the above message is not useful , open an issue : < URL >	0	0
0 0 1.0 1.0 1.0 1.0 1.2 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 2 25	Enable collecting cri-o metrics . I can currently run : : minikube start -- container-runtime = cri-o . This will start minikube with CRI-O as the container runtime . CRI-O exposes metrics with this flag enabled : -- enable-metrics : Enable metrics endpoint . Default is localhost : 9090 . . I would like to access the metrics exposed here for minikube but this currently isn't possible without additional changes in the VM itself . Would it be possible to allow passing in runtime flags as a feature , at least for all currently supported alternative container runtimes ?  	1	1
2 2 1.4 2.0 1.2 1.5 1.35 2.0 1.6666666666666667 2.0 0.9032258064516129 31 1.0 3 3 6 31	Document all ' start ' options on the minikube website . So that our options are more discoverable .  	2	2
1 1 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0 0.0 1 2 3 12	[ DOCS]Minkube in a container . Do we have any doc related to installing Minkube in a container ? See < URL >  	2	2
1 1 1.4 1.0 1.2 1.0 0.95 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 2 10 28	Minikube fails to start with docker driver on macOS 11.1 . Error : /var/lib/dpkg : no such file or directory . Steps to reproduce the issue : docker system prune -- volumes minikube delete docker start -- driver = docker Full output of failed command : 棣冩 minikube v 1.17.0 on Darwin 11.1 閴?Using the docker driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 1990MB ) ... 閴?Stopping node ' minikube ' ... 棣冩暉 Deleting ' minikube ' in docker ... 棣冦亞 StartHost failed , but will try again : creating host : create : creating : create kic node : container name ' minikube ' : log : 2021-01-28T 21:19:13 . 799746495Z + mount 2021-01-28T 21:19:13 . 806546956Z + df -h /var 2021-01-28T 21:19:13 . 809660555Z + find /var/lib/dpkg 2021-01-28T 21:19:13 . 822425814Z find : ' /var/lib/dpkg ' : No such file or directory : container exited unexpectedly 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 1990MB ) ... 棣冩▼ Failed to start docker container . Running ' minikube delete ' may fix it : creating host : create : creating : create kic node : container name ' minikube ' : log : 2021-01-28T 21:20:01 . 961530779Z + mount 2021-01-28T 21:20:01 . 964084926Z + df -h /var 2021-01-28T 21:20:01 . 969789288Z + find /var/lib/dpkg 2021-01-28T 21:20:01 . 971640267Z find : ' /var/lib/dpkg ' : No such file or directory : container exited unexpectedly Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command :	2	0
2 2 1.4 2.0 1.1 1.5 1.2 1.0 1.3333333333333333 2.0 1.2075471698113207 53 1.0 3 6 9 42	try save docker volumes in minikube home . even though we create volumes with label and we clean up by label , it would still be nice to explicity spcify the file system and also the path on the host machine to save the volume . for next release we could try to make docker volumes be saved in minikube home folder maybe using : $ docker volume create -- driver local / -- opt type = ext4 / -- opt device = : /path/to/dir / .  	1	1
0 0 1.2 2.0 1.0 1.0 1.1 1.5 0.6666666666666666 0.0 0.0 0 0.0 0 0 1 6	Make it easier to use PSP ( PodSecurityPolicy ) clusters . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Creating a PSP enabled cluster involved multiple steps . Will be make sense to add some : -- enable-psp . flag to : minikube start . command to automatically perform those steps ?	2	1
0 0 1.4 2.0 1.3 2.0 1.1 1.0 1.0 1.0 0.935251798561151 139 1.0 4 4 8 56	docker : Confirm `minikube start` provides actionable feedback if `docker system prune` is run . Test : minikube stop docker system prune minikube start minikube start -- delete-on-failure 3 and #4 should probably succeed without the user having to perform additional steps . It may be acceptable in #3 if we just provided advice .    	1	2
1 1 0.8 1.0 1.1 1.0 0.9 1.0 1.0 1.0 0.9476744186046512 172 1.0 3 7 12 43	implement operation auditing for minkube ( -- user ) flag . there should be a way to know what user did what operition on a minikube cluster ( to be used by multiple tools that use minikube at the same time skaffold , cloud code , cloud run , cloud shel .... ) : ---------------------------------------- command , option , user , star t_t ime , end_time ---------------------------------------- start , -- driver = docker , driver , cloud-code , 223232 , 2 23232 start , -- driver = hyperkit , driver , code-shell , 223232 , 2 23232 stop , steven , 223232 , 2 23232 . if no user was provided , it should default to the OS user name	0	1
2 2 0.8 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.7142857142857142 7 2.0 0 1 7 22	profile's kube-context should have a ' minikube-' prefix . I have been thinking , if we should make minikube kube contexts have a minikube- prefix to them , so the user knows it was created by minikube ? let say if I create a minikube profile named dev , or test , I might have a GKE cluster or another cluster named test or dev , I wouldn't be sure which kube context belongs to minikube . and would be harder to clean up ! I would love to know others opinions on this .	2	1
2 2 1.4 2.0 1.0 1.0 0.65 0.0 1.6666666666666667 2.0 0.9619047619047619 105 1.0 1 2 17 49	-- kubernetes-version = latest upgrade : Error parsing old version ' latest ' : No Major . Minor . Patch elements found . It looks like : -- kubernetes-version = latest . is broken for the upgrade case . To replicate , start a v 1.17 . x cluster : : minik	1	0
2 2 1.2 1.0 1.3 1.5 1.4 2.0 1.3333333333333333 1.0 0.0 0 0.0 1 1 8 33	`minikube config set memory` with given unit results in error . Steps to reproduce the issue : Try to configure the memory with a given unit : : minikube config set memory 8g . Full output of failed command : : 閴?minikube version minikube version : v 1.15.1 commit : 23f40a012abb52eff365ff99a709501a61ac5876 閴?minikube config set memory 8g -- alsologtostderr I1124 13:08:06 . 532895 91753 out . go : 185 ] Setting OutFile to fd 1 ... I1124 13:08:06 . 534342 91753 out . go : 237 ] isatty . IsTerminal(1 ) = true I1124 13:08:06 . 534423 91753 out . go : 198 ] Setting ErrFile to fd 2 ... I1124 13:08:06 . 534434 91753 out . go : 237 ] isatty . IsTerminal(2 ) = true I1124 13:08:06 . 535094 91753 root . go : 279 ] Updating PATH : /Users/hag/ . minikube/bin I1124 13:08:06 . 547629 91753 out . go : 110 ] W1124 13:08:06 . 548405 91753 out . go : 146 ] 閴?Exiting due to MK_CONFIG_SET : run validations for ' memory ' with value of ' 8g ' : [ memory : strconv . Atoi : parsing ' 8g ' : invalid syntax ] 閴?Exiting due to MK_CONFIG_SET : run validations for ' memory ' with value of ' 8g ' : [ memory : strconv . Atoi : parsing ' 8g ' : invalid syntax ] W1124 13:08:06 . 549032 91753 out . go : 146 ] W1124 13:08:06 . 549059 91753 out . go : 146 ] 棣冩▼ If the above advice does not help , please let us know : 棣冩▼ If the above advice does not help , please let us know : W1124 13:08:06 . 549096 91753 out . go : 146 ] 棣冩啝 < URL > 棣冩啝 < URL > I1124 13:08:06 . 570154 91753 out . go : 110 ] . Full output of : minikube start . command used , if not already included : not applicable here IMHO the unit should be respected , see < URL >	0	0
2 2 1.6 2.0 1.4 2.0 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 2 4 25	Bug in project ID Automated Google Cloud Platform Authentication ( gcp-auth addon ) when using end user credentials . There seems to be an issue with the : gcp-auth . addon when using end user credentials . I have logged in using : gcloud auth login . . Then I've started minikube using : minikube start -- adons = gcp-auth . . Then I've deployed a container that does the following : import firebase_admin from firebase_admin . auth import verify_id_token app = firebase_admin . initialize_app () verify_id_token('some-token ' ) . which leads to the following error : firebase_admin . _auth_utils . InvalidIdTokenError : Firebase ID token has incorrect ' aud ' ( audience ) claim . Expected ' dev-plx ' but got ' dev-plx ' . Make sure the ID token comes from the same Firebase project as the service account used to authenticate this SDK . See < URL > for details on how to retrieve ID token . . A bit of investigating shows that the project ID from the authentication is wrong : > print(f'Project ID : { repr(app . project_id )}') Project ID : ' dev-plx/n ' . This was originally posted in < URL > Workaround : export GOOGLE_APPLICATION_CREDENTIALS = /path/to/service-account . json . : minikube start -- adons = gcp-auth . : from firebase_admin import credentials , initialize_app from firebase_admin . auth import verify_id_token cert = credentials . Certificate('/google-app-creds . json ' ) app = initialize_app(cert ) verify_id_token('some-token ' ) .	0	0
2 2 1.6 2.0 1.3 1.5 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 2 4	none : Allow host IP to be settable . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.26.1 - OS ( e.g. from /etc/os-release ): Debian GNU/Linux 9 ( stretch ) - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): none - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): - Install tools : - Others : What happened : I'm getting a broken cluster everytime my host IP changes . It looks like the Pods keep trying to reach the host using the old IP . I tried to set the host IP by starting minikube with the following command , but it did not work : : minikube start -- vm-driver = none -- apiserver-name = localhost -- apiserver-ips = 127.0.0.1 -- extra-config = kubelet . node-ip = 127.0.0.1 . What you expected to happen : I would like to be able to set the host IP to make my minikube installation survive network changes . How to reproduce it ( as minimally and precisely as possible ): Start a minikube cluster using : vm-driver = none . Deploy an application to the cluster ( It can be the dashboard ) Stop minikube Change the host IP address Restart minikube Try to access the application . Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	1
1 1 0.8 1.0 0.9 1.0 1.0 1.0 1.0 1.0 1.0 1 1.0 2 3 8 38	Provide more descriptive error codes for PROVIDER_DOCKER_ERROR . PROVIDER_DOCKER_ERROR serves as a catch-all error for docker . Can minikube provide more descriptive error codes for docker related failures ( besides docker not installed and docker not running which already have corresponding error codes ) ?	2	1
0 0 0.8 1.0 0.7 1.0 0.85 1.0 1.0 1.0 0.9523809523809523 168 1.0 0 4 9 40	docker-env do not exit if can't restart docker . this line should be changed to klog . Warnning < URL > this would be helpful for the workflows that run in a loop , and would hit the burst liimit of 5 times per miniute	0	0
1 1 1.0 1.0 1.2 1.5 0.95 1.0 0.3333333333333333 0.0 0.5652173913043478 23 0.0 0 0 2 13	Add UEFI bootloader for x86 ISO . < URL >	0	1
2 2 1.2 2.0 1.0 1.0 1.1 1.0 2.0 2.0 0.86 200 1.0 7 9 15 53	auto-pause : intiliaze the pause state from the current state . curently when we start the addon we assume it is not pause . which could be wrong ( though not the standard use-case but might be possible ) so instead of : var runtimePaused = false . we should initialize it as getting the current state	0	1
0 0 0.4 0.0 0.8 0.5 0.85 1.0 0.6666666666666666 0.0 1.32 25 1.0 1 2 4 26	Problem with ' exotic arch ' for addons . It seems like the dash went missing : : Failed to pull image ' k8s.gcr.io/kube-addon-managerarm64:v9.0.2 ' : rpc error : code = Unknown desc = Error response from daemon : manifest for k8s.gcr.io/kube-addon-managerarm64:v9.0.2 not found . It is supposed to be ' ( for amd64 ) or ' -arm64 ' : a : = runtime . GOARCH // Some legacy docker images still need the -arch suffix // for less common architectures blank suffix for amd64 ea : = ' if runtime . GOARCH ! = ' amd64 ' { ea = runtime . GOARCH } . Workaround is re-tagging the images locally . : docker pull k8s.gcr.io/kube-addon-manager-arm64:v9.0.2 docker tag k8s.gcr.io/kube-addon-manager-arm64:v9.0.2 k8s.gcr.io/kube-addon-managerarm64:v9.0.2 .  	1	0
0 0 0.8 0.0 0.8 0.5 0.9 1.0 0.0 0.0 0.0 0 0.0 1 1 5 31	registry-creds addon : Temporary Error : secrets ' registry-creds-ecr ' not found . starting with a fresh cluster on Ubuntu 19.10 using VirtualBox minikube start ( all is starting fine ) 棣冩 minikube v 1.5.2 on Ubuntu 19.10 閴?Automatically selected the ' virtualbox ' driver ( alternates : [ none ]) 棣冩崚 Downloading VM boot image ... > minikube-v 1.5.1 . iso . sha256 : 65 B / 65 B [ -------------- ] 100.00% ? p/s 0s > minikube-v 1.5.1 . iso : 143.76 MiB / 143.76 MiB [-] 100.00% 3.09 MiB p/s 47s 棣冩暉 Creating virtualbox VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... 棣冩儞 Preparing Kubernetes v 1.16.2 on Docker ' 18.09.9 ' ... 棣冩崙 Downloading kubeadm v 1.16.2 棣冩崙 Downloading kubelet v 1.16.2 棣冩 Pulling images ... 棣冩畬 Launching Kubernetes ... 閳?Waiting for : apiserver 棣冨及 Done ! kubectl is now configured to use ' minikube ' Then running ecr credentials fails : minikube addons configure registry-creds ( entering all data for ECR ) 閴?ERROR creating : registry-creds-ecr . secret : Temporary Error : secrets ' registry-creds-ecr ' not found 閴?ERROR creating : registry-creds-gcr . secret : Temporary Error : secrets ' registry-creds-gcr ' not found 閳跨媴绗?ERROR creating : registry-creds-dpr . secret 閴?registry-creds was successfully configured After this procedure no secret has been setup and therefore image Pull fails . Any Idea ? Thanks , detlef	0	0
0 0 0.8 0.0 0.8 1.0 0.9 1.0 0.6666666666666666 0.0 1.5 12 2.0 0 2 4 22	Add details about what can be done to fix an unhealthy driver . Steps to reproduce the issue : On a Windows machine , open a PowerShell prompt without administrator privileges and turn off Docker . : minikube start . Full output of failed command : : PS D: /dev/minikube > minikube-windows-amd64 . exe start * minikube v 1.14.2 on Microsoft Windows 10 Enterprise 10.0.18363 Build 18363 * Unable to pick a default driver . Here is what was considered , in preference order : - vmware : Not installed : exec : ' docker-machine-driver-vmware ' : executable file not found in %PATH% - docker : Not healthy : ' docker version -- format {{ . Server . Os}}-{{ . Server . Version }}' exit status 1 : Error response from daemon : open // . /pipe/docker_engine_linux : The system cannot find the file specified . - hyperv : Not healthy : Hyper-v commands have to be run as an Administrator - podman : Not installed : exec : ' podman ' : executable file not found in %PATH% - virtualbox : Not installed : unable to find VBoxManage in $PATH X Exiting due to DRV_NOT_DETECTED : No possible driver was detected . Try specifying -- driver , or see < URL > . As we can see from the output , it is difficult to understand what can be done to fix an unhealthy driver . The drivers are there and available but in this case , Docker & Hyper-V are unhealthy because Docker is turned off and for Hyper-V , permissions are required . Also , since the drivers are detected but unhealthy , : DRV_NOT_DETECTED . should be something like : DRV_FOUND_BUT_UNHEALTHY . . I suggest that we tabularize the information along with the possible fix for each of the drivers unhealthy or not like we do for the : minikube profile list .    	1	2
0 0 1.0 1.0 1.0 1.0 1.0 1.0 0.3333333333333333 0.0 2.0 1 2.0 1 1 4 41	CRI-O : Default podman CNI config prevents use of custom network plugins . Steps to reproduce the issue : Download minikube 1.11.0 ; Install Virtualbox . Run : minikube start -- network-plugin = cni -- container-runtime = cri-o . without : -- enable-default-cni . Install a custom network plugin in order to make use of Kubernetes NetworkPolicies . E . g . following < URL > ( except regarding the invocation of : minikube start . ) . Test NetworkPolicies according to the < URL > . Notice that applied NetworkPolicies are not enforced . Apparent cause of the issue : It turns out that there is a default podman bridge network CNI configuration at : /etc/cni/net . d/87-podman-bridge . conflist . in the minikube VM which prevents the custom network plugin to be used . If using docker as container runtime , this default configuration does not seem to cause any problems . Workaround : Deleting this CNI configuration , restarting CRI-O and recreating the concerned pods causes the NetworkPolicies to be enforced as expected . : minikube ssh -- sudo rm /etc/cni/net . d/87-podman-bridge . conflist minikube ssh -- sudo systemctl restart crio kubectl delete pods -- all . The next time the minikube VM is started , : /etc/cni/net . d/87-podman-bridge . conflist . will be present again . Thoughts : Actually , am not sure if that is an issue with minikube or if cri-o just handles prioritization differently than docker . Either way , when using minikube , this behavior is unexpected , IMHO .   	1	0
0 0 1.0 1.0 1.0 1.0 1.05 1.0 0.3333333333333333 0.0 1.0363636363636364 110 1.0 3 3 8 80	  site : add to FAQ page how to ignore system verification . . the answer is here < URL >	0	2
2 2 1.2 1.0 1.4 2.0 1.3 1.5 1.6666666666666667 2.0 0.9021739130434783 184 1.0 9 9 14 38	Spinner , avoid emitting spinniner characters if there is not PTY . currently if minikube is running in integraiton test or in other tools the spinner will spit out characters like W K WKWKWKWKWKWKWKWK ...	0	0
1 1 1.2 1.0 1.3 1.5 1.25 1.0 1.0 1.0 0.8295964125560538 223 1.0 1 3 6 32	add maintainer column to addons list .  	1	1
0 0 1.2 1.0 1.2 1.0 1.1 1.0 1.0 1.0 0.6666666666666666 6 0.0 1 7 13 46	Distinguish preloaded tarballs by container runtime . So that we can have preloaded tars for crio/containerd/docker	0	1
0 0 1.2 2.0 1.4 2.0 1.2 1.5 0.6666666666666666 0.0 0.0 0 0.0 0 3 5 21	Deleted hostpath PVs stuck in released state after minikube restart . Dynamically provisioned PVs are stuck in the released state if minikube is restarted before deleting the pvc . It seems that the Provisioner Identity changes on reboot which causes the hostpath provisioner to ignore the PV . The exact command to reproduce the issue : : cat << EOF | kubectl apply -f - kind : PersistentVolumeClaim apiVersion : v1 metadata : name : mypvc spec : resources : requests : storage : 10Mi accessModes : - ReadWriteOnce EOF minikube stop minikube start kubectl delete pvc mypvc kubectl get pv . The full output of the command that failed : : $ minikube ssh tail /tmp/storage-provisioner . INFO ... I0620 17:17:11 . 727823 1 controller . go : 1040 ] deletion of volume ' pvc-8674aa0d-937a-11e9-b2aa-000c29bcb67d ' ignored : ignored because identity annotation on PV does not match ours . The output of the : minikube logs . command : N/A The operating system version : MacOS 10.14.5 minikube v 1.1.1 kubernetes 1.14.3	2	0
1 1 1.4 2.0 1.5 2.0 1.5 2.0 1.0 1.0 1.25 4 1.0 0 5 11 24	Test image registry connectivity should also ignore scheme prefixed proxy . < URL > Could extract a util function from #8885 By the way , I am wondering if we can convert localhost to host machine IP ? Or add a flag to replace localhost ? something like : : minikube start -- host-ip =$( nmcli -g ip4 . address dev show eth0 ) .	2	1
1 1 1.2 1.0 1.0 1.0 0.9 1.0 1.0 1.0 1.1764705882352942 17 1.0 0 3 7 27	Add some Docker Machine documentation . Since Minikube uses Docker Machine to create and provision the VM , it could be nice to have some more ' advanced ' docs on ( lib)machine ? We used to have a reference to < URL > but it's not posted anymore on our new website ( for some reason ) . Also , some of the user docs are available on < URL > And some non-official drivers live on < URL > I think these are quite nice , and could serve as ' inspiration ' for minikube docs : < URL > * < URL > * < URL > < URL > * < URL > * < URL > It also needs some technical documentation , but most of that is yet to be written ... The focus will be on these drivers : : virtualbox . ( default ) - all : generic . ( remote ) - all : none . ( local ) - linux And on these future-proof drivers : : kvm2 . ( libvirt ) - linux : hyperkit . - darwin : hyperv . - windows  	2	2
1 1 1.4 1.0 0.9 1.0 1.15 1.0 1.3333333333333333 1.0 0.3333333333333333 3 0.0 1 1 5 25	All commands should print cluster settings ( driver , container runtime ) to logs . Currently , in issues or when debugging , it can be hard to figure out what driver and container runtime is being used for any given command . Having these in the logs for all commands ( not just minikube start ) would be very helpful .	2	1
1 1 1.0 1.0 0.9 1.0 0.8 1.0 1.3333333333333333 1.0 1.2456140350877194 57 1.0 3 7 16 48	minikube should not change the profile to a not existing profile . Try to change profile to something that doens't exist : medya@ ~ /workspace/minikube ( crio_kic ) $ . /out/minikube profile lit 閴?profile ' lit ' not found 閴?if you want to create a profile you can by this command : minikube start -p lit . try doing normal minikube things ( it is trying to apply the command on the profile that didn't exist ) : medya@ ~ /workspace/minikube ( crio_kic ) $ . /out/minikube docker-env 棣冩寴 Error getting config : cluster ' lit ' does not exist 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > medya@ ~ /workspace/minikube ( crio_kic ) $ . /out/minikube -p minikube docker-env .	0	0
2 2 1.6 2.0 1.5 1.5 1.2 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 1 2 7	Bad file ownership in minikube deb packages . Please provide the following details : Environment : Minikube version : 0.32.0 , 0.33.0 , probably others - OS : Debian - VM Driver : N/A - ISO version : N/A - Install tools : N/A - Others : N/A What happened : The package has the binary owned by non-root user/group . On my system this ended up as : usbmix . / : geoclue . , but that is a somewhat random coincidence . The package shows that it ' thinks ' it's trying to install the binary owned by : jenkins . / : jenkins . , which is also wrong , but points to the source of the error . What you expected to happen : The minikube binary to be installed with standard permissions , specifically owned by root/root How to reproduce it ( as minimally and precisely as possible ): Install the package . : ls -l /usr/bin/minikube . Output of : minikube logs . ( if applicable ) : N/A Anything else do we need to know : N/A	0	0
0 0 0.8 1.0 1.1 1.0 1.15 1.0 0.3333333333333333 0.0 0.8232758620689655 232 1.0 0 4 5 22	site : Add command-line download instructions for windows binary . how about as a follow up PR fix the windows ' Download ' so we have a command to download binaries for windows too ? instead of asking peoople to go to the release page < URL > something like this might work ? ( that we use to download gopogh ) : ( New-Object Net . WebClient ) . DownloadFile('<URL>' , ' C : /ProgramData/chocolatey/bin/gopogh . exe ' ) . Originally posted by @medyagh in < URL >    	1	2
0 0 1.2 1.0 1.3 1.0 1.35 1.0 1.0 1.0 0.0 0 0.0 0 1 7 40	lvm volumes are not activated after reboot . I'm running minikube with kvm2 driver . I've attached a virtual sata device and used this to create lvm volumes . However , after a reboot these volumes are not activated automatically . Prior to a reboot after creating the lv we can see the active/open flags : ao . under the attributes column : : $ sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert osd-data-91656739-d2a4-4406-8d6b-362daff0faf4 ceph-c1042f4b-8ba0-4d32-b9e5-ac47d6eea3c0 -wi-ao ---- < 10.00 g . After reboot the same logical volume is not active or open . Running : lvchange -ay . will reactivate the volumes . I suppose this is likely an issue with the udev rules in the minikube host . : $ minikube version minikube version : v 0.35.0 .	2	0
1 1 1.4 1.0 1.1 1.0 1.0 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 5 7 25	[ FEATURE ] feature-gates via minikube config set ... . I would love to be able to set defaults for : -- feature-gates . just like for : -- vm-driver . etc . : $ minikube config set feature-gates foo 棣冩寴 Set failed : Property name feature-gates not found .	2	1
2 2 0.8 0.0 1.1 1.0 0.8 0.5 0.6666666666666666 0.0 0.0 0 0.0 5 7 20 60	Error message prints a dead link . The error on < URL > links to a dead resource : < URL >	0	0
2 2 1.6 2.0 1.5 2.0 1.5 2.0 1.3333333333333333 2.0 0.9925373134328358 134 1.0 0 0 4 20	Figure out KVM failures in jenkins on Debian 10 .	0	0
0 0 1.0 1.0 1.4 2.0 1.35 2.0 1.0 1.0 1.1147540983606556 61 1.0 0 1 5 19	Build multi-arch manifest for the storage-provisioner-image . We should move to using a multi-arch , just like kubernetes does ... Same arch images as before , but rename the ' top ' one to -amd64 And then use : docker manifest . to stitch all the architectures together . See < URL > There is some code in kubernetes to do this , that we could borrow : : ALL_ARCH = amd64 arm arm64 ppc64le s390x push-manifest : docker manifest create -- amend $(IMAGE ): $(TAG ) $(shell echo $(ALL_ARCH ) | sed -e ' s ~ [^ ]* ~ $(IMAGE)/-& :$( TAG ) ~ g ' ) set -x ; for arch in $(ALL_ARCH ); do docker manifest annotate -- arch $${arch } ${IMAGE }: ${TAG } ${IMAGE}-$${arch }: ${TAG }; done docker manifest push -- purge ${IMAGE }: ${TAG } .	0	1
2 2 1.4 2.0 1.5 2.0 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 2 4 8 79	allow minikube be accessible from outside network ( add -- listen-address flag ) . ' sudo /usr/local/bin/minikube start -- driver = docker ' Problem : : Not able to access POD web application after deployment . Proposed Solution : : Need to find a way to change minikube IP manually v 1.18.0 . I have a deployed minikube on VM ( REDHAT 7.8 ) running over VMware and I have deployed minikube with -- driver = docker . I have delpoyed minikube with command ' sudo /usr/local/bin/minikube start -- driver = docker ' . All went fine . Below are the logs : [ osadmin @dockerce simplek8s]$ kubectl cluster-info Kubernetes master is running at < URL > KubeDNS is running at < URL > [ osadmin @dockerce simplek8s]$ minikube status m01 host : Running kubelet : Running apiserver : Running kubeconfig : [ osadmin @dockerce simplek8s]$ minikube ip 127.0.0.1 Now the issue is that minikube IP is 127.0.0.1 . And my machine IP is 10.239 . X.X . So after I deployed a POD(web application ) , I am not able to access it from outside my browser from where 10.239 . X.X is reachable . I have defined the port fine and POD got created as well .  	1	1
2 2 1.2 1.0 1.2 1.0 1.3 1.0 1.3333333333333333 1.0 0.0 0 0.0 4 8 9 29	Incompatible hyperkit driver causes ' Waiting for SSH access ... ' hang after macOS restart . After rebooting my computer , any attempts to restart my Minikube cluster do not work . I get the following output : : 棣冩 minikube v 0.35.0 on darwin ( amd64 ) 棣冩寱 Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Restarting existing hyperkit VM for ' minikube ' ... 閳?Waiting for SSH access ... . And it just sits there , never even timing out no matter how long I wait . How to replicate : : minikube start -- cpus = 4 -- memory = 4096 -- disk-size = 30g -- vm-driver = hyperkit -- kubernetes-version = v 1.10.12 -- extra-config = apiserver . authorization-mode = RBAC . I am using macOS Mojave Version 10.14.3 . Here's my output from : minikube logs . : : 棣冩寴 command runner : getting ssh client for bootstrapper : Error dialing tcp via ssh client : dial tcp 192.168.64.11 : 22 : connect : operation timed out 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > .  	1	0
0 0 1.0 1.0 0.9 1.0 1.05 1.0 0.6666666666666666 0.0 1.164179104477612 134 1.0 3 4 6 19	Need some more regression tests for image build . At least two major features are currently broken , environment variables and build arguments : Issue #12431 Issue #12384  	1	1
0 0 1.2 1.0 1.2 1.0 1.15 1.0 0.6666666666666666 1.0 1.3333333333333333 12 2.0 2 2 3 7	Add apiserver logs to ' minikube logs ' command . This way users can see flag misconfiguration errors .	0	1
0 0 0.6 0.0 0.9 1.0 0.95 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 2 3 10	Add ' kubeflow ' add-on . Hi forks , I have a question if you guys have any plan to support kubeflow as an add-on . Microk8s supports kubeflow as an add-on like this : < URL > If you guys are interested , I'd like to take this issue . Thanks in advance .  	1	1
0 0 1.0 1.0 0.7 0.5 0.8 1.0 1.0 1.0 0.0 0 0.0 2 7 12 51	minikube kubectl appends -- cluster flag breaking exec commands . Looks like < URL > may have introduced a bug : [ minikube ] ~ $ minikube kubectl -- -- context minikube exec ambassador-84ffc9fdc4-nxp4h -- echo ' hello ' hello -- cluster minikube [ minikube ] ~ $ kubectl -- context minikube exec ambassador-84ffc9fdc4-nxp4h -- echo ' hello ' hello . This causes issues when trying to run exec commands because the : -- cluster . flag is always appended to the end leading to issues such as : [ minikube ] ~ $ minikube kubectl -- -- context minikube exec mysql-db-79b945965d-gtzf7 -- mysqladmin ping mysqladmin : [ ERROR ] unknown option ' -- cluster ' command terminated with exit code 2 . : [ minikube ] ~ $ minikube version minikube version : v 1.18.1 commit : 09ee84d530de4a92f00f1c5dbc34cead092b95bc .	0	0
0 0 0.6 0.0 0.9 0.5 0.85 0.5 0.0 0.0 0.5714285714285714 7 0.0 2 3 14 57	Build preloaded images tar from commit SHA . Ref < URL >  	1	1
2 2 1.0 1.0 1.0 1.0 1.25 1.0 1.3333333333333333 1.0 1.1764705882352942 17 1.0 1 1 7 14	  Publish conformance results @ cncf/k8s-conformance . We should be posting our test results at < URL >	0	2
2 2 1.0 1.0 1.1 1.0 1.05 1.0 1.0 1.0 0.0 0 0.0 0 1 1 3	Add a KubeVirt add-on . KubeVirt provides adds a virtualization API to Kubernetes in order to launch regular virtual machines . This request is about adding KubeVirt as an add-on to minikube , based on the work in < URL > Once this PR is merged , we can import manifests from the official releases in order to deploy KubeVirt in minikube .	2	1
1 1 1.0 1.0 1.3 2.0 0.9 0.5 1.0 1.0 0.9477611940298507 134 1.0 0 2 7 29	delete -- purge -- all : open . minikube/logs/audit . json : no such file or directory . Steps to reproduce the issue : Install minikube v 1.17.1 Run : minikube delete -- purge -- all . If you do not have a new profile created , you will see an error such as : : 棣冩暉 Successfully deleted all profiles 棣冩媰 Successfully purged minikube directory located at - [/usr/local/google/home/tstromberg/ . minikube ] E0201 09:02:33 . 689733 567894 audit . go : 59 ] unable to open /usr/local/google/home/tstromberg/ . minikube/logs/audit . json : open /usr/local/google/home/tstromberg/ . minikube/logs/audit . json : no such file or directory . Note : You may need to run the delete command twice , to have an empty directory , in order to duplicate this .	0	0
1 1 1.2 1.0 1.4 1.0 1.3 1.5 1.0 1.0 0.0 0 0.0 2 4 5 19	Enables support for connection tracking zones ( CONFIG_NF_CONNTRACK_ZONES ) in the guest VM . The mini-kube VM currently has ' # CONFIG_NF_CONNTRACK_ZONES is not set ' I have a need to use this feature and would like to update the VM image . Is there a recommended way to do this ? Is there a way to include this feature by default ? Any guidance is appreciated .	2	1
2 2 1.4 2.0 1.6 2.0 1.45 2.0 2.0 2.0 2.0 1 2.0 0 3 9 49	Unable to -- cleanup profile tunnel other than default profile . I cannot delete the tunnel created for a minikube profile other than the default profile . Steps to reproduce the issue : Start a minikube profile , like : minikube start -p foo . Check that context is : foo . . Start tunnel , using : minikube tunnel -p foo . Stop tunnel , using Ctrl+C Try to delete tunnel , using : minikube tunnel -c -p foo . , or : minikube -p foo -c . , or : minikube -c . Full output of failed command : The first 2 start the tunnel . The latter gives : 棣冦仐 Profile ' minikube ' not found . Run ' minikube profile list ' to view all profiles . 棣冩啝 To start a cluster , run : ' minikube start ' . Environment specs : - minikube version : v 1.16.0 ( did not see tunnel related changes in newer versions ) - kernel 5.10 on Manjaro Linux  	1	0
0 0 1.0 1.0 1.1 1.5 1.15 1.5 0.6666666666666666 0.0 0.9371428571428572 175 1.0 3 5 7 26	add -- light option to invoke `profile list` without checking status ( for cheaper and faster ) . : $ minikube profile list | ----------|----------- | ---------|-------------- | ------|--------- | ---------|------- | | Profile | VM Driver | Runtime | IP | Port | Version | Status | Nodes | | ----------|----------- | ---------|-------------- | ------|--------- | ---------|------- | | minikube | docker | docker | 192.168.49.2 | 8443 | v 1.20.0 | Running | 1 | | ----------|----------- | ---------|-------------- | ------|--------- | ---------|------- | . to this : $ minikube profile list -- light | ----------|----------- | ---------|-------------- | ------|--------- | --------- | | Profile | VM Driver | Runtime | IP | Port | Version | Nodes | | ----------|----------- | ---------|-------------- | ------|--------- | --------- | | minikube | docker | docker | 192.168.49.2 | 8443 | v 1.20.0 | 1 | | ----------|----------- | ---------|-------------- | ------|--------- | --------- | .  	1	1
1 1 0.8 1.0 0.7 1.0 0.8 1.0 1.0 1.0 1.6666666666666667 3 2.0 2 3 8 25	update contributing guide docs . the adding a dependency part is not relevant anymore < URL >  	2	2
1 1 1.0 1.0 1.2 1.0 1.35 1.0 0.6666666666666666 1.0 0.9659863945578231 147 1.0 2 3 6 36	add support for setting driver's network cidr for KVM and Docker driver . . currently user can set the network CIDR for only virtualbox driver , for KVM we have hard coded this , we should be able to allow user to configure this for all drivers : -- host-only-cidr='192 . 168.99.1 /24 ' : The CIDR to be used for the minikube VM ( virtualbox driver only .   	1	1
1 1 1.0 1.0 1.1 1.0 1.05 1.0 1.0 1.0 0.46153846153846156 26 0.0 1 4 11 51	Look into improving `minikube start` time with containerd by 10% . This would be about a 4 second improvement .	0	1
2 2 1.4 2.0 1.4 2.0 1.4 2.0 1.0 1.0 1.3333333333333333 3 1.0 0 1 2 22	Improve Hyper-V performance by reduce duplicate Hyper-V operations . See #10135 : docker-env . and : status . need 10s in average . Seems like each Hyper-V operation ( get running state , get ip addr ) takes 2s+ . Comparison : Linux OS < 1s /kind feature   	1	1
2 2 1.4 2.0 1.2 1.5 0.95 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 5 27	container running as root , not as user . I am developing a Kubernetes application for < URL > in < URL > . To test and reproduce , you can take a look at the docs of the project : * < URL > * < URL > Started developing with Minikube 0.35.0 and it worked flawless . Recently updated to Minikube 1.1.0 . Now bootstrapping job is failing : see also issue IQSS/dataverse-kubernetes #60 . I believe this is a regression bug in Minikube , recently introduced . When creating a : batch/v1/Job . , specified commands are run as : root . in the container , not as the < URL > . When using the k3s/k3d demo , the user is correct ( : glassfish . ) . The exact command to reproduce the issue : Deploy the demo up to bootstrap job ( will take some time , big application ... ) . Replace < URL > with : whoami . , and deploy it . When complete , look at the jobs log . The full output of the command that failed : Does not apply . Deployment is successful . The output of the : minikube logs . command : : root . ( Should have been : glassfish . ) The operating system version : Fedora 29 and CentOS 7	0	0
2 2 1.0 1.0 1.3 1.5 1.1 1.0 1.0 1.0 1.2857142857142858 21 1.0 0 2 3 27	Upgrade Toolbox fedora version . Currently we are using : fedora : 29 . , should upgrade to : fedora : 30 . ( or even : fedora : latest . ) : TOOLBOX_DOCKER_IMAGE = fedora TOOLBOX_DOCKER_TAG = 29 . We could also merge rkt/podman/systemd-nspawn with upstream , or at least get the bug fixes : < URL > < URL > Can do this with #5237 Similar issue to ~ #3058 ~ I think we will keep with Fedora 30 and using systemd-nspawn , rather than latest and podman < URL > ( this has the same life-span as the other LTS )  	1	1
1 1 1.6 2.0 1.5 1.5 1.45 2.0 1.3333333333333333 1.0 0.0 0 0.0 0 2 5 14	minikube - missing the Publisher Name on Windows . The minikube installed via the Windows installer from the official source page found at < URL > is missing the Publisher Name which is causing Windows Defender to display a message that the app is not trusted and that it may put the PC at risk . This may be a blocker for new users who may want to try out minikube . Screenshot : minikube : v 1.21.0 OS : Windows 10 Version 10.0.19042 Build 19042  	1	0
2 2 1.0 1.0 0.7 0.0 0.95 1.0 1.3333333333333333 2.0 0.0 1 0.0 2 4 6 26	upgrade go version 1.12.9 -> 1.12.12 . There is a CVE for Go and this vulnerability is fixed in 1.12.12 . We should update it . See : < URL > See issue golang/go #34540 Seems this is fixed in 1.12.12 . I suggest we should update to 1.12.12 first /kind bug /priority important-soon Fixes #5675	0	0
2 2 1.4 2.0 1.4 2.0 1.25 1.5 1.0 1.0 0.0 0 0.0 0 0 1 10	HTTP_PROXY doesn't update when minikube start with different proxy . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.30.0 - OS ( e.g. from /etc/os-release ): Win10 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): virtualbox - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): minikube-v 0.30.0 . iso - Install tools : choco - Others : What happened : : 1 . minikube start -- docker-env = HTTP_PROXY = < URL > -- docker-env = HTTPS_PROXY = < URL > 2 . minikube stop 3 . minikube start -- docker-env = HTTP_PROXY = < URL > -- docker-env = HTTPS_PROXY = < URL > . After finish those command , docker http proxy still remain ip1 : port1 , not ip2 : port2 What you expected to happen : docker http proxy should reset to ip2 : port2 How to reproduce it ( as minimally and precisely as possible ): : 1 . minikube start -- docker-env = HTTP_PROXY = < URL > -- docker-env = HTTPS_PROXY = < URL > 2 . minikube stop 3 . minikube start -- docker-env = HTTP_PROXY = < URL > -- docker-env = HTTPS_PROXY = < URL > . Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	0
2 2 1.6 2.0 1.1 1.0 0.8 1.0 1.6666666666666667 2.0 2.0 1 2.0 1 1 12 38	minikube start : kubectl info : exec : exit status 2 . Steps to reproduce the issue : : minikube start -- vm = true -- kubernetes-version='v1 . 20.0 ' . Full output of failed command : : $ minikube start -- vm = true -- kubernetes-version='v1 . 20.0 ' 棣冩 minikube v 1.15.1 on Darwin 11.1 閴?Automatically selected the hyperkit driver . Other choices : vmware , vmwarefusion 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating hyperkit VM ( CPUs = 2 , Memory = 6000MB , Disk = 20000MB ) ... 棣冩儞 Preparing Kubernetes v 1.20.0 on Docker 19.03.13 ... 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : storage-provisioner , default-storageclass 棣冨及 Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default E1216 11:24:31 . 467901 25710 start . go : 266 ] kubectl info : exec : exit status 2 $ . : -- alsologtostderr . didn't help clarify this error . Last few lines of run with : -- alsologtostderr . : : I1216 11:28:23 . 304254 25968 start . go : 203 ] waiting for startup goroutines ... I1216 11:28:23 . 408164 25968 out . go : 110 ] 棣冨及 Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default 棣冨及 Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default E1216 11:28:23 . 408190 25968 start . go : 266 ] kubectl info : exec : exit status 2 $ . If neccessary I can attach the full log with : -- alsologtostderr . . Optional : Full output of : minikube logs . command : < URL >	2	0
0 0 1.2 2.0 1.2 1.5 1.2 1.5 0.6666666666666666 0.0 0.0 0 0.0 0 1 3 19	Support for Windows 10 WSL2 . Is there is a plan to support WSL2 on Windows 10 ? Docker is already experimenting with it : < URL > Do You think supporting WSL2 could save us from using virtual machines ?	0	1
2 2 1.8 2.0 1.8 2.0 1.55 2.0 2.0 2.0 1.5 10 2.0 1 6 15 46	Improve search engine ranking for minikube.sigs.k8s.io . I was searching for the documentation which is hosted as part of this repo at this link - < URL > & surprisingly , I couldn't find it in any of the following search engines on the 1st page when just searching for : minikube . - 1 . Google 2 . DuckDuckGo 3 . Bing I think that the docs website should come up . What do you guys think ?  	1	2
1 1 0.6 1.0 1.1 1.0 1.35 1.5 0.6666666666666666 1.0 0.0 0 0.0 1 3 9 31	hyperv : Unable to start VM : create : creating : exit status 1 ( reason unknown ) . Ask Question 0 I installed Minikube on Windows 10 but can't get it to run . I tried to start it with : minikube start -- vm-driver = hyperv The first error was : [ HYPERV_NO_VSWITCH ] create : precreate : no External vswitch found . A valid vswitch must be available for this command to run . I then searched on Google and found the solution to this error with this page : < URL > I then fixed the problem by defining a vswitch but I got this error : minikube start -- vm-driver hyperv -- hyper v-v irtual-switch ' Minikube ' o minikube v 1.0.1 on windows ( amd64 ) $ Downloading Kubernetes v 1.14.1 images in the background ... Creating hyperv VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... ! Unable to start VM : create : creating : exit status 1 Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL > This is a pretty generic error . What do I do to get this working ? Thanks !	2	0
1 1 1.4 2.0 1.5 2.0 1.25 1.5 1.0 1.0 0.5294117647058824 17 0.0 1 2 4 20	crio . conf empty with Docker driver . Since updating cri-o from 1.20 to 1.22 ( #12425 ) the : /etc/crio/crio . conf . file is empty when using the Docker driver .  	1	0
1 1 1.2 1.0 1.2 1.0 1.0 1.0 1.3333333333333333 1.0 0.9518072289156626 166 1.0 6 8 10 41	upload jenkins raw logs Per commit . our txt file is uploaded at < URL > but out html is uploaded per commmit < URL > I like our Text to be uploaded per commit that is in the Jenkins Plugin  	1	1
1 1 0.6 1.0 0.8 1.0 0.95 1.0 0.6666666666666666 1.0 1.0077519379844961 129 1.0 0 0 3 26	-- extra-config with double quotes breaks kubeadm init . we should validate options like this : . /out/minikube start -- driver = docker -- extra-config = etcd . client-cert-auth = false 閳?. currently running that will make minikube kubeadm hang . [ backl	1	0
2 2 1.2 2.0 1.5 2.0 1.3 2.0 0.6666666666666666 0.0 1.0603448275862069 116 1.0 2 8 13 57	windows : update suggestion when SSL certs need to be installed . this issue can be reproduced when docker driver has a left-over volume . : [ certs ] Using existing apiserver certificate and key on disk stderr : W0516 00:22:25 . 414604 1303 configset . go : 202 ] WARNING : kubeadm cannot validate component configs for API groups [ kubelet.config.k8s.io kubeproxy.config.k8s.io ] [ WARNING IsDockerSystemdCheck ]: detected ' cgroupfs ' as the Docker cgroup driver . The recommended driver is ' systemd ' . Please follow the guide at < URL > [ WARNING FileContent -- proc-sys-net-bridge-bridge-nf-call-iptables ]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist [ WARNING Swap ]: running with swap on is not supported . Please disable swap [ WARNING Service-Kubelet ]: kubelet service is not enabled , please run ' systemctl enable kubelet . service ' [ WARNING DirAvailable -- var-lib-minikube-etcd ]: /var/lib/minikube/etcd is not empty error execution phase certs/apiserver-kubelet-client : [ certs ] certificate apiserver-kubelet-client not signed by CA certificate ca : crypto/rsa : verification error To see the stack trace of this error execute with -- v = 5 or higher * Suggestion : Try ' minikube delete ' to force new SSL certificates to be installed * Related issue : < URL > PS C : /Users/jenkins/Downloads > . problem 1 : in case of non-kic , we need to purge minkube certs ( minikube dete wont help ) problem 2 : in case of docker driver needs deleting the left over volume .  	1	0
0 0 0.8 1.0 1.0 1.0 1.15 1.0 1.0 1.0 0.92 25 1.0 0 1 4 24	Automatically recommend users use -- image-repository if k8s.gcr.io is unavailable .  	1	1
0 0 0.8 1.0 0.9 1.0 1.2 1.0 0.3333333333333333 0.0 1.4615384615384615 13 2.0 1 3 5 23	Update SuggestFix for Docker Driver on Windows . The following line needs to be changed - < URL > The error which pops up with latest version of Docker Desktop on Windows is - : Error response from daemon : open // . /pipe/docker_engine_linux : The system cannot find the file specified . .	2	0
1 1 1.2 1.0 1.4 1.5 1.25 1.5 1.0 1.0 0.8288288288288288 222 1.0 0 1 5 31	Auto generate Base Image Tools versions . in our kic base automation < URL > we could generate the base image version of things each time we build a new Kic Base or ISO for example docker version crictl verison or things that users care about , so when we install latest version of everything on each build we communicate to the user what docker verison is included in the base    	1	2
0 0 1.2 2.0 1.0 1.0 1.0 1.0 0.6666666666666666 0.0 0.4782608695652174 23 0.0 5 6 11 34	Explore solutions for running skaffold with minikube+containerd . Right now skaffold errors out if it detects that minikube is running with containerd To solve this issue , we need to : Find a suitable docker-env alternative for continerd runtime  	1	1
2 2 1.2 1.0 1.0 1.0 0.8 1.0 1.0 1.0 0.0 0 0.0 3 3 7 33	macOS : `brew install` should be `brew cask install` . When following the installation guide for macOS , it says to install using : brew . with : brew install minikube . . However , : minikube . is cask now so the correct command is : brew cask install minikube . . Not a big issue since the failed command tells you what to try but seems like it might be worth updating . : brew install minikube . : Error : No available formula with the name ' minikube ' Found a cask named ' minikube ' instead . Try brew cask install minikube .  	2	2
0 0 1.0 1.0 1.2 1.5 1.15 1.5 1.3333333333333333 2.0 1.0 3 1.0 3 4 6 50	[ go . mod ] build error : package go.opentelemetry.io/otel . /kind bug When I built minikube binary , the errors of : go mod . happend . : $ make go build -tags ' -ldflags='-X k8s.io/minikube/pkg/version.version=v1.17.1 -X k8s.io/minikube/pkg/version.isoVersion=v1.17.0 -X k8s.io/minikube/pkg/version.gitCommitID='051caa1d68b2812095efadc536edb76ec4c6e010-dirty ' -X k8s.io/minikube/pkg/version.storageProvisionerVersion=v4 ' -o out/minikube k8s.io/minikube/cmd/minikube go : finding module for package go.opentelemetry.io/otel/api/trace go : finding module for package go.opentelemetry.io/otel/api/global .. / .. / .. /pkg/mod/ github.com/!google!cloud!platform/opentelemetry-operations-go/exporter/trace@v0.13.0/cloudtrace.go:28:2 : module go.opentelemetry.io/otel@latest found ( v 0.17.0 ) , but does not contain package go.opentelemetry.io/otel/api/global .. / .. / .. /pkg/mod/ github.com/!google!cloud!platform/opentelemetry-operations-go/exporter/trace@v0.13.0/cloudtrace.go:29:2 : module go.opentelemetry.io/otel@latest found ( v 0.17.0 ) , but does not contain package go.opentelemetry.io/otel/api/trace make : *** [ out/minikube ] Error 1 . This is caused by the missmatch of module version . < URL > : go.opentelemetry.io/otel/api/trace . and : go.opentelemetry.io/otel/api/global . are old packages at v 0.13.0 . In v 0.17.0 , these package don't exist . And these absent causes the errors of : go mod . . We need to update other modules to take consistency .	0	0
2 2 1.0 1.0 1.1 1.0 1.2 1.0 0.6666666666666666 0.0 1.18 50 1.0 4 10 15 59	Add popularity contest for different drivers and runtimes . It would be great to have something similar to < URL > The popularity-contest package sets up a cron job that will periodically anonymously submit to the Debian developers statistics about the most used Debian packages on this system . This information helps Debian make decisions such as which packages should go on the first CD . It also lets Debian improve future versions of the distribution so that the most popular packages are the ones which are installed automatically for new users . We have the survey , but it's more about user stories and general feedback ... We have a fast 5-question survey to learn how & why you are using minikube , and what improvements we should make . We would love to hear from you ! 棣冩 This ' form ' could probably be 99% auto-generated from e.g. : minikube info . ( Where ' info ' is a fictitious command that outputs all the nice stuff from ' start ' . ) It would include such things as os/arch and driver/runtime , maybe resources . Maybe output to something like JSON , and then user opts-in to contribute it ?	2	1
1 1 0.6 0.0 0.6 0.0 0.75 0.0 1.0 1.0 0.0 0 0.0 0 5 8 20	Minikube addons should be able to be used from air-gapped setups . Minikube addons may reference images living in a variety of different registries . It would be ideal if a user could resolve these addon images to a single location , such as a private registry , so as to enable air-gapped setups .  	1	1
0 0 0.4 0.0 0.9 1.0 0.85 1.0 0.3333333333333333 0.0 0.5882352941176471 17 0.0 1 5 16 42	Enable scheduled stop . Top level issue tracking the following : [ x ] Implement schedule stop for unix < URL > [ x ] implement scheduled stop on windows ( < URL > [ x ] Add -- cancel-scheduled-stop flag to cancel all existing scheduled stops < URL > [ x ] : minikube status . should show existing scheduled stops and time left < URL >	0	1
2 2 1.6 2.0 1.4 2.0 1.35 2.0 2.0 2.0 0.4 5 0.0 1 3 9 36	Support `cp` between minikube nodes . Related to < URL > It would be nice to have an ability to copy files not only between the host and a node , but also between minikube nodes : minikube cp node1 : /tmp/foo node2 : /tmp/bar .  	1	1
2 2 1.2 1.0 1.1 1.0 1.0 1.0 1.0 1.0 0.5263157894736842 19 0.0 2 5 10 34	`minikube status` should show existing scheduled stops and time left . Related to < URL >	0	1
0 0 1.0 1.0 0.8 0.5 1.15 1.5 0.6666666666666666 0.0 1.1333333333333333 90 1.0 0 2 7 38	Separate container runtimes from the image . Currently we include all three CRI runtimes , in the base image : * Docker ( docker ) * CRI-O ( crio + podman ) * Containerd ( containerd + buildkit ) This makes the image somewhat big , compared to having just one . ISO KIC We only start one of them , but the others still takes up space on the disk . ( actually we always start docker , as long as using the ' docker machine ' API ... but we stop it again quickly afterwards , if using the containerd or cri-o runtimes ) One alternative to doing three ' base ' images , would be to use the ' provisioner ' . This is a docker-machine feature , that is used to install docker on the machine . < URL > So the base image wouldn't have any of them ( maybe : runc . ) from the start . Instead they would be downloaded and installed from separate tarballs/packages . This is how we currently handle different kubernetes versions , for instance . linux/v 1.20.0 /kubeadm linux/v 1.20.0 /kubectl linux/v 1.20.0 /kubelet It's also how we handle the preload , instead of making three ' node ' images : preloaded-images-k8s-v8-v 1.20.0 -containerd-overlay2-amd64 . tar . lz4 preloaded-images-k8s-v8-v 1.20.0 -cri- o-o verlay-amd64 . tar . lz4 preloaded-images-k8s-v8-v 1.20.0 -docker-overlay2-amd64 . tar . lz4 As compared with kind : : docker.io/kindest/node:v1.20.0 . ( always uses containerd-overlay2 )	1	1
2 2 1.2 2.0 1.3 2.0 1.25 2.0 1.3333333333333333 2.0 1.5769230769230769 26 2.0 0 1 9 21	Create a minikube native default CNI for kic . I need help to make as mall CNI plugin for kic but native to minikube . currently using kindnet < URL >	2	1
2 2 1.4 2.0 1.1 1.0 1.15 1.0 1.3333333333333333 2.0 1.5 2 1.5 0 2 11 55	document work around for VirtualBox networking be slow . Hi , we are using minikube to deploy local test installations of our stack , in particular for developers . In general , we are using the virtual box hypervisor ( yes , even on win 10 machines , for reasons ;-)) . We note a general sluggishness of the minikube environment , even when running on very powerful host machines to which we assign lots of cores (<< #physical cores on the host , naturally ) and RAM to the minikube VM with the ' minikube start ' command . We are still in the early phase of pinpointing the bottlenecks , but one observation that we already made is that network performance seems to be a problem and it seems unrelated to our own software , because we can also observe it when doing something trivial like a docker build : When building one of our images where we download a number of larger artifacts from our artifact repo ( Nexus ) , the build takes ~ 8 secs on the Docker Toolbox ' default ' VM . The same build , on the same host machine , takes almost a minute on the minikube VM ( which BTW has 6 cores vs . 2 for the docker toolbox VM ) . The main difference comes from all operations in our RUN commands that involve the network . For example , the download of a ~ 90MB artifact with curl command reaches ~ 70MB/s on Docker Toolbox , while it barely reaches 2 , 5MB/s with the minikube VM . The issue is the same when putting a number of large files into the Docker build context and letting Docker upload them . Is this a known issue ? Any ideas where to start looking ? JG  	2	2
0 0 1.0 1.0 1.0 1.0 1.1 1.0 1.0 1.0 0.9545454545454546 22 1.0 0 1 3 39	status fails at head : executing ' status ' at < . ApiServer > : can't evaluate field ApiServer in type cmd . Status . : $ . /out/minikube delete $ . /out/minikube status host : kubelet : apiserver : 棣冩寴 Error executing status template : template : status :3 : 13 : executing ' status ' at < . ApiServer > : can't evaluate field ApiServer in type cmd . Status .	0	0
0 0 1.2 2.0 1.1 1.5 1.1 1.5 0.6666666666666666 0.0 0.0 0 0.0 1 12 17 56	`standard_init_linux . go : 219 : exec user process caused : exec format error` in arm64 . err in arm64 : : WARNING : The requested image's platform ( linux/amd64 ) does not match the detected host platform ( linux/arm64/v8 ) and no specific platform was requested standard_init_linux . go : 219 : exec user process caused : exec format error . It seems that default kicbase image cannot work well in arm64 platform sloved by add : : minikube start ... -- base-image kicbase/stable : v 0.0.18 .	2	0
2 2 0.8 0.0 0.9 0.5 1.05 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 2 9 28	Use configured DNS domain in bootstrapper kubeadm templates . The following two lines specify : cluster . local . , but if minikube was started with : -- dns-domain bla . , that domain should probably be used instead . < URL > < URL >  	1	0
1 1 1.0 1.0 0.9 1.0 1.15 1.0 1.0 1.0 0.0 0 0.0 2 6 10 39	minikube installation section is empty under Get Started . ' Installation ' section is empty . < URL >	0	0
2 2 0.6 0.0 0.9 0.5 1.05 1.0 1.0 1.0 0.0 0 0.0 1 2 14 54	feature request : minikube on ppc64le . Steps to reproduce the issue : : minikube start -- driver = kvm2 . Full output of failed command : Full output of : minikube start . command used , if not already included : : [ myuser @myhost ~ ]$ minikube start -- driver = kvm2 棣冩 minikube v 1.11.0 on Redhat 7.6 ( kvm/ppc64le ) 閴?Using the kvm2 driver based on user configuration 棣冩崙 Downloading driver docker-machine-driver-kvm2 : > docker-machine-driver-kvm2 . sha256 : 65 B / 65 B [ ------- ] 100.00% ? p/s 0s > docker-machine-driver-kvm2 : 13.88 MiB / 13.88 MiB [ ---- ] 100.00% ? p/s 0s 棣冩崚 Downloading VM boot image ... > minikube-v 1.11.0 . iso . sha256 : 65 B / 65 B [ ------------- ] 100.00% ? p/s 0s > minikube-v 1.11.0 . iso : 174.99 MiB / 174.99 MiB [] 100.00% 8.69 MiB p/s 20s 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 6000MB , Disk = 20000MB ) ... 棣冦亞 StartHost failed , but will try again : new host : Error attempting to get plugin server address for RPC : Failed to dial the plugin server in 10s 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 6000MB , Disk = 20000MB ) ... ^C [ myuser @myhost ~ ]$ . : [ myuser @myhost ~ ]$ rm -rf . minikube [ myuser @myhost ~ ]$ minikube version minikube version : v 1.11.0 commit : 57e2f55f47effe9ce396cea42a1e0eb4f611ebbd [ myuser @myhost ~ ]$ uname -m ppc64le [ myuser @myhost ~ ]$ file . minikube/bin/docker-machine-driver-kvm2 . minikube/bin/docker-machine-driver-kvm2 : ELF 64-bit LSB executable , x86-64 , version 1 ( SYSV ) , dynamically linked ( uses shared libs ) , for GNU/Linux 2.6.32 , BuildID[sha1 ] = 4f7a31492b3472664ca78b19e0894c4dddb3dc9b , not stripped [ myuser @myhost ~ ]$ . Obviously a minor oversight but I cannot see any other reference to this issue .	2	1
1 1 1.0 1.0 1.4 1.5 1.1 1.0 1.3333333333333333 1.0 1.0 2 1.0 0 0 5 40	Make addon yaml templates image name configurable . Some registry may have different image name rule , for example : aliyun ingress image name : : registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v0.40.2 . , but in the template : < URL > Notice the image name : ingress-nginx/controller . and : nginx-ingress-controller . . We may need more configurable YAML templates .	2	1
1 1 0.8 1.0 1.0 1.0 1.25 1.0 0.6666666666666666 1.0 1.0923076923076922 65 1.0 0 4 7 30	Mention brew on the linux install page too . From PR #9066 : Homebrew has supported Linux since a while ago and I've been using it on my Linux environment too :) Currently we only mention Homebrew ( < URL > for Mac , while it also supports Linux ( as ' linuxbrew ' ) : brew install minikube .  	2	2
0 0 0.6 1.0 0.8 1.0 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 1 2 4 17	Make virtualbox DNS settings configurable . 3453 change the default behavior and as a result , I believe I am seeing breakage as a result . If these items were configurable I believe it would allow me to more easily validate the issue is related , and bypass the current values that are problematic . As it it is currently , it seems even if I attempt to change the value of this setting , minikube will overwrite it .	2	1
1 1 1.4 2.0 1.4 2.0 1.4 2.0 1.0 1.0 0.0 0 0.0 0 1 4 28	docker-env script for tcsh is not correct . Steps to reproduce the issue : Login with default shell set to tcsh start minikube on docker : : minikube start -- driver = docker . execute command to print out environment : : minikube docker-env . or name shell explicity : : minikube docker-env -- shell = tcsh . Full output of : minikube logs . command : Not needed here ... Full output of failed command : : export DOCKER_TLS_VERIFY='1 ' export DOCKER_HOST='tcp://192 . 168.99.100 : 32771 ' export DOCKER_CERT_PATH='/home/andi/ . minikube/certs ' export MINIKUBE_ACTIVE_DOCKERD='minikube ' # To point your shell to minikube's docker-daemon , run : # eval $(minikube -p minikube docker-env ) . But the expected output should look like this : : setenv DOCKER_TLS_VERIFY ' 1 ' setenv DOCKER_HOST ' tcp :/ / 192.168.99.100 : 32771 ' setenv DOCKER_CERT_PATH ' /home/andi/ . minikube/certs ' setenv MINIKUBE_ACTIVE_DOCKERD ' minikube ' # To point your shell to minikube's docker-daemon , run : # eval `minikube -p minikube docker-env` . In no Go-expert , but it seems that the source-codefor handling a ( t)csh-shell differently than bash or similar shells is missing . The docker here is running on a vm , but at least by explicitly naming the shell I expect the correct script .	2	0
2 2 1.6 2.0 1.1 1.5 1.15 1.0 2.0 2.0 0.5 2 0.5 2 3 3 6	Network pre-flights : Detect network issues and present actionable feedback . Many incoming issues are due to local network access problems , mostly from proxies , but many also due to HyperV : < URL > We should have a basic connectivity check to the minikube VM , in a way that detects problems in a user-friendly way , such as : : * Unable to establish a network connection to the VM : please check < URL > for possible solutions . . And at least until we have good story around offline usage : : * Unable to establish a fully functional internet connection : please check < URL > for possible solutions . .	0	1
1 1 1.6 2.0 1.5 2.0 1.1 1.0 1.3333333333333333 1.0 0.0 0 0.0 3 4 4 12	minikube docker-env writes error to stdout if VM isn't running - confuses shell . Environment : Minikube version : 0.30 - OS ( e.g. from /etc/os-release ): macOS 10.13 - VM Driver : VirtualBox - ISO version : N/A - Install tools : - Others : What happened : : % eval $(minikube docker-env ) E1107 17:14:21 . 381360 3836 env . go : 330 ] Error setting machine env variable(s ): Error getting ip from host : Host is not running zsh : = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = not found . What you expected to happen : If a Minikube VM is not running , : eval $(minikube docker-env ) . should fail , but not in such a confusing way , with shell trying to execute a command : = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = . . How to reproduce it ( as minimally and precisely as possible ): Stop the Minikube VM . Run : eval $(minikube docker-env ) .  	1	0
1 1 1.0 1.0 0.8 0.5 0.75 0.0 1.0 1.0 0.9166666666666666 12 1.0 2 3 15 50	minikube delete does not delete volumes of worker nodes in multinode . : $ out/minikube start -- driver = docker 棣冩 minikube v 1.10.1 on Darwin 10.15.4 閴?Using the docker driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 1991MB ) ... 棣冩儞 Preparing Kubernetes v 1.18.2 on Docker 19.03.2 ... 閳?kubeadm . pod-network-cidr = 10.244.0.0 /16 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : default-storageclass , storage-provisioner 棣冨及 Done ! kubectl is now configured to use ' minikube ' $ docker volume ls DRIVER VOLUME NAME local minikube $ out/minikube node add 棣冩 Adding node m02 to cluster minikube 閴?Multi-node clusters are currently experimental and might exhibit unintended behavior . To track progress on multi-node clusters , see < URL > 棣冩啢 Starting node minikube-m02 in cluster minikube 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 2200MB ) ... 棣冩儞 Preparing Kubernetes v 1.18.2 on Docker 19.03.2 ... 棣冨及 Successfully added m02 to minikube ! $ docker volume ls DRIVER VOLUME NAME local minikube local minikube-m02 $ out/minikube delete 棣冩暉 Deleting ' minikube ' in docker ... 棣冩暉 Deleting container ' minikube ' ... 閴?Stopping ' minikube-m02 ' in docker ... 棣冩磧 Powering off ' minikube-m02 ' via SSH ... 棣冩暉 Deleting ' minikube-m02 ' in docker ... 棣冩暉 Removing /Users/selgamal/ . minikube/machines/minikube ... 棣冩媰 Removed all traces of the ' minikube ' cluster . $ docker volume ls DRIVER VOLUME NAME local minikube-m02 .	0	0
0 0 1.2 1.0 1.2 1.5 1.2 1.5 1.0 1.0 0.0 0 0.0 2 3 5 22	NotABug : unable to create log : open : permissions denied . Steps to reproduce the issue : I want to use Minikube on a linux machine with several users , so this is what I did so far : Setup user for minikube and switch user : bash adduser -- disabled-login minikube usermod -aG docker minikube su minikube . Start Minikube : bash minikube config set vm-driver docker minikube config set cpus 4 minikube config set memory 8192 minikube config set disk-size 10G minikube config set kubernetes-version v 1.18.17 mkdir ~ /logs minikube start -- log_dir =~/ logs . Prepare access to Minikube to other users : : bash chmod -R g+wrx ~ / . minikube chmod -R g+r ~ / . kube . Provide access to Minikube to user ' test-user ' : : bash su test-user ln -s /home/minikube/ . minikube ~ / . minikube cp -r /home/minikube/ . kube ~ / . kube . : ~ $ minikube version minikube version : v 1.19.0 commit : 15cede53bdc5fe242228853e737333b09d4336b5 . When then one user1 runs : minikube ssh . and afterwards user2 does the same i get the following error message : : I0426 19:45:31 . 875059 366698 out . go : 278 ] Setting OutFile to fd 1 ... log : exiting because of error : log : unable to create log : open /tmp/minikube_ssh_6bca5e284ff1b988432c36e406f313bc2482ae72_0 . log : permission denied . Even though I set the log_dir to another directory he is creating them in /tmp and why is the permission denied ? All users are in group ' minikube ' so they should be able to overwrite that file ...	0	0
2 2 1.4 2.0 1.4 2.0 1.4 2.0 2.0 2.0 0.0 0 0.0 1 3 4 27	Documentation : Add suggestion to improve productivity while using minikube kubectl . Steps to reproduce the issue : Visit < URL > Note that most of the time we're using the : kubectl . command directly . But the user might not have installed kubectl directly and hence he/she has to replace all the commands with : minikube kubectl -- . every time he's copying the instructions . I personally found using : minikube kubectl -- . to be very frustrating thing and came up with a simple solution . Suggestion for documentation : Simply suggest the user to add the following in his/her . bashrc/ . zshrc file : alias kubectl='minikube kubectl --' . if he/she has installed kubectl using minikube . If you think this is a valid suggestion then I would love to add this in the documentation as my first contribution to the project : smile :  	2	2
2 2 0.8 1.0 1.3 1.5 1.3 2.0 0.6666666666666666 0.0 0.9411764705882353 136 1.0 0 5 7 47	kubeadm stderr is occasionally truncated . As noted in #10508 - I suspect there may be a recent regression in our ability to preserve kubeadm output , but it may also be a much older issue . It might be worth checking against minikube v 1.15 for instance , to see if the behavior changed , with an identical -- kubernetes-version selected .  	1	0
2 2 1.4 2.0 1.3 1.5 1.1 1.0 1.6666666666666667 2.0 0.0 0 0.0 1 5 6 19	hyperv : minikube stuck at ' Waiting for SSH access ... ' . When i start my minikube cluster i have these logs , : $ minikube start o minikube v 0.34.1 on windows ( amd64 ) i Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . : Re-using the currently running hyperv VM for ' minikube ' ... : Waiting for SSH access ... . And it stuck at waiting for SSH access . I use hyper V and when i connect to the vm the minikube cluster start successfully but in my console don't show it . If i kill the command and i try to run minikube delete i have these logs : $ minikube delete - Powering off ' minikube ' via SSH ... . And the command stuck at ' Powering off ' minikube ' via SSH ... ' Im using : Windows v1809 minikube v 0.34.1 Thank you in advance for your help !	2	0
0 0 0.8 1.0 0.8 1.0 1.1 1.0 1.0 1.0 1.15 80 1.0 3 5 7 25	The debian ISO build machine is crashing when running buildroot . The node is running debian oldstable ( stretch ) , which is still running gcc-multilib 4:6 . 3.0 -4 It seems to have some issues , when trying to bootstrap and cross-compile our buildroot ? : g++ : internal compiler error : Killed ( program cc1plus ) Please submit a full bug report , with preprocessed source if appropriate . See < file :// /usr/share/doc/gcc-6/README . Bugs > for instructions . Makefile : 152 : recipe for target ' cmGeneratorTarget . o ' failed . See PR #9628 and PR #9645 , both crashed	1	0
0 0 1.0 1.0 1.1 1.0 0.95 1.0 0.3333333333333333 0.0 1.1794871794871795 39 1.0 6 10 13 49	  Getting Started for Linux suggests non-existing Docker Desktop . Currently the documentation for the ' docker ' driver suggests installing Docker Desktop : < URL > < URL > However , that Docker product is only available for Windows and Mac - and not for Linux . < URL > < URL > So it should instead link to Docker Engine , listed under Server ( rather than Desktop ): < URL > < URL > < URL > < URL > It might of course already be available as distribution supported package ... Typically called something like : docker.io . ( or : docker-ce . - or : docker . ) < URL > < URL > Newer versions of Fedora and CentOS prefer : podman . over : docker . . So on those distributions , you might want to use the ' podman ' driver .	0	2
2 2 1.6 2.0 1.7 2.0 1.5 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 0 3	hyperv : ' minikube stop ' fails ( Shutting down 0% until it eventually times out ) . BUG REPORT Please provide the following details : Environment : Windows 10 Pro 1803 Minikube version ( use : minikube version . ): v 0.28.0 - OS ( e.g. from /etc/os-release ): Windows 10 Pro 1803 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): Hyper-v - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.28.0 - Install tools : - Others : What happened : Minikube stop fails to stop minikube VM What you expected to happen : VM stops How to reproduce it ( as minimally and precisely as possible ): With running minikube , perform minikube stop Output of : minikube logs . ( if applicable ) : Anything else do we need to know : Clean install of Minikube 0.28.0 ( deleted . minikube folder ) Command sits with powershell overlay showing Shutting down 0% until it eventually times out Trying to shutdown via Hyper-V manager/Powershell doesn't work either Opening VM in hyper-v shows command prompt - may not be relevant ?	0	0
0 0 0.2 0.0 0.6 0.5 1.0 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 5 9 32	Support buildkit . I am using minikube 1.0.0 with kubernetes v 1.14.0 ( on Fedora Linux , xen2 driver ) . I am using a docker client connected to the minikube docker , and issueing docker build commands , but I really need to use buildkit . If I set ' DOCKER_BUILDKIT = 1 ' the docker client complains because the daemon does not support buildkit . I tried all the different cointainer engines supported by minikube , but the result is always the same . Then I tried running docker in experimental mode inside the minikube vm : - ssh into the vm - created the docker config file with the experimental = true option - restarted the docker service with systemctl Then the error I got on a docker build was ' pivot_root invalid argument ' . I tried setting DOCKER_RAMDISK to true or false , but nothing changed . It might be that docker 18.09 fixes this , or it might be a tiny core linux issue , but understanding that docker engine subtlety is beyond me . What I know is that the same docker build succeeds on my workstation ( Fedora 29 with docker 18.09.3 ) . Given clear instructions I could try building minikube with docker 18.09 and test it with that . IMHO supporting buildkit in local dev environments should be desirable ...  	1	1
2 2 1.6 2.0 1.3 1.5 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 3 7 18	profile < name > : error acquiring lock for x : Name ' -x ' not valid . Trying Minikube 1.4.0 and seeing this error when setting the profile name : : minikube profile operator-minikube 棣冩▼ Error while setting kubectl current context : writing kubeconfig : Error writing file operator-minikube : error acquiring lock for operator-minikube : Name ' -operator-minikube ' not valid . Seems like a bug ? I did prefix any : - . into the profile name .	0	0
2 2 1.0 1.0 1.0 1.0 1.15 1.5 1.3333333333333333 2.0 1.1875 32 1.0 0 4 5 25	Document how to install the docker client . Some people seem to be confused by the new docker.com site , and some static binaries are missing ... < URL > We might need to write a simple guide on how to install the docker client ( the CLI ) , using regular tools ? Note that the daemon is not needed , it is provided by minikube ! macOS : $ brew install docker . < URL > Windows : $ choco install docker-cli . < URL > Linux As usual the Linux packaging situation is a bit confused , but it might be available as ' < URL > ' . But for that platform , the < URL > are still available and < URL > still works ... Installing Docker as a CRI for kubeadm ( : none . driver ) is documented elsewhere : < URL > Support for Docker Desktop and other products is also documented elsewhere : < URL > For minikube , a simple : docker . CLI program is enough to be able to use : minikube docker-env . The download should be about 50M , and is a single go binary - without the daemons and such . And of course , if you use a different container runtime such as CRI-O even that is not needed . Then you use something like : podman . or other < URL > tool , in order to build your container images .  	2	2
1 1 1.6 2.0 1.4 1.5 1.2 1.5 1.6666666666666667 2.0 0.9622641509433962 159 1.0 2 6 12 32	Add dedicate network and static IP for podman driver . in minikube 1.14.0 we had docker driver with dedicate network , we should do same thing for podman for consistancy Similar to this PR < URL > This will make Podman driver to also have a Static IP similar to our Docker Driver after that PR You would need to install Podman on Linux to test this  	1	1
0 0 1.2 2.0 1.3 1.5 1.4 2.0 1.3333333333333333 2.0 0.7222222222222222 18 0.5 1 5 8 49	  The front page of our documentation has an old ' latest release ' . On < URL > it says our latest release is 1.16.0 , which is 3 months old and 2 releases old now . We should add automation to our release scripts to update this on release .	0	2
2 2 1.2 1.0 1.2 1.0 1.15 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 0 0	Add integration test to assert that HEAD can use state created by latest release . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Feature request The 0.12.0 to 0.12.1 release required a recreate of the minikube VM because of localkube DNS changes . This caused a lot of confusion for users upgrading . ( #781 ) Minikube should ideally be able to be upgraded without recreating the VM . We should add a test during our release job to enforce this behavior . In the rare case that we can't get around recreating the VM , we should make a clear note of it in the release docs .	0	1
0 0 0.8 0.0 1.1 1.5 0.9 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 1 4	hyper v-v irtual-switch ; startup hangs indefinitely trying to get ip address . Minikube Version : 0.26.1 OS : Windows 10 - 1709 VM Driver : hyperv ISO : minikube-v 0.26.0 . iso I am having a similar problem #2425 . I am searching through open issues to see if anyone has a work around for my problem . Attempting to use Windows 10/Hyper-v . I have setup a Virtual Switch ( external network with Ethernet network adapter ) . I am using release 0.25.2 to get around #2634 . ( Update : This morning I updated to 0.26.1 ) I run the following command in powershell : : minikube start -- vm-driver='hyperv ' -- memory = 4096 -- hyperv-virtual-switch='My Virtual Switch ' -- v = 7 -- alsologtostderr . The script goes along for while . It successfully creates the minikube VM . But then it gets to a point where it continually polls for the vm's state and ip address . It appears the command to get the network adapter IP address returns nothing . But command to retrieve the state returns ' Running ' . It does this several times until it just hangs indefinitely . If I then open another powershell instance and do kubectl cluster-info , it initially says Kubernetes master is running at < URL > but then receives an error stating it couldn't connect to target machine because connection has been actively refused . Update : Minikube ssh does not work . But if I connect to the VM via Hyper-V Manager , I can execute curl command and get a webpage , so it does have network connectivity with my Virtual Switch	2	0
2 2 1.2 1.0 1.1 1.0 1.1 1.0 1.3333333333333333 2.0 1.1262135922330097 103 1.0 0 2 12 49	The new load command still uses the cache directory . Instead of loading to the container runtime directly , it still copies to the cache ... : machine . CacheAndLoadImages . This is not needed , it should be able to transfer the file to the machine directly . With the current API we have to make a remote disk copy , but that is more minor * ... * Eventually it should be possible to stream the tarball directly into the runtime . But not every runtime supports this at the moment , so load from a temporary file .	0	0
2 2 1.4 2.0 1.5 2.0 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 1 2 3 8	Hung at ContainerCreating : Error response from daemon : Get < URL > net/http : TLS handshake timeout ' . Environment : Minikube version : v 0.25.2 - OS : Windows 10 - VM Driver : hyperv - ISO version : v 0.25.1 - Install tools : chocolate - Others : What happened : The kube-addon-manager-minikube hangs on ContainerCreating What you expected to happen : I exepct it to have the Running status and to have access to the Dashboard How to reproduce it ( as minimally and precisely as possible ): Execute the following command ( Minikube being the networking interface i defined in Hyperv ) : minikube start -- vm-driver hyperv -- hyper v-v irtual-switch ' Minikube ' Output of : minikube logs . ( if applicable ) : See below file : < URL > Anything else do we need to know : I'm working behind my firm's firewall .  	1	0
2 2 1.6 2.0 1.3 1.0 1.35 1.5 1.6666666666666667 2.0 1.155844155844156 77 1.0 0 2 2 20	Upgrade to Docker 20.10 . We are currently running Docker 19.03 ( CE ) Once a stable 20.10 is out , we could upgrade  	1	1
1 1 1.4 1.0 1.4 1.5 1.3 1.5 1.0 1.0 0.8952879581151832 191 1.0 2 3 6 49	bump oldest supported kuberntes from v 1.13.0 to v 1.14.0 . currently with latest build of kic image we have broken support for v 1.13.0 k8s I tested manually with the new kic image we need to have at least kubernetes v . 1.14.0 traditionally we have supported 5 recent versions of k8s now k8s is is on 1.120.0 , if we bump older supported version to 1.14.0 we still are on 6 most recent versions so it is reasonable for us to bump oldest supported version to 1.14.0 < URL >	0	0
1 1 1.0 1.0 1.1 1.0 1.4 2.0 0.6666666666666666 1.0 0.0 0 0.0 1 2 4 28	minikube + ssh + nat gate in front of the remote host . Hello I am trying to geht minikube together with the ssh driver and a remote host behind a nat gateway up and running . I faced some issues with kublet using the wrong Ip which I could fix hacky . But I still got the impression that the combination nat gate + ssh driver won't work . Is there anyone out there who got experience with this or is even running such a setup successfully ? Cheers J鐪塺gen  	2	2
2 2 1.4 1.0 1.7 2.0 1.4 2.0 1.6666666666666667 2.0 0.864321608040201 199 1.0 6 8 14 52	auto-pause : add VM driver support . currently for KIC we do this : . PHONY : deploy/kicbase/auto-pause # auto pause binary to be used for kic image work arround for not passing the whole repo as docker context deploy/kicbase/auto-pause : $(SOURCE_GENERATED ) $(SOURCE_FILES ) GOOS = linux GOARCH =$( GOARCH ) go build -o $@ cmd/auto-pause/auto-pause . go . < URL > and in Dockerfile we add this : COPY auto-pause /usr/local/bin/auto-pause . we need to do something similar for our ISO	0	1
2 2 0.8 1.0 1.0 1.0 1.2 1.0 1.0 1.0 1.0 2 1.0 3 9 15 58	Bump helm-tiller 2.16.7 . Minikube v 1.10.1 has helm-tiller addon v 2.16.3 . There is a newer helm version v 2.16.7 which includes bug fixes . Detail : < URL > I'll send PR to upgrade to this version .  	1	1
2 2 1.6 2.0 1.1 1.5 1.25 1.5 2.0 2.0 0.0 0 0.0 0 0 0 1	Add the option to set namespace in kubectl context ( or stop deleting it ! ) . Currently minikube saves its context without any namespace . So I must add : -n mynamespace . to each : kubectl . command every time . I would like to have an option to store my namespace in minikube config file which will be saved in kubectl context on : minikube start . . Currently if I modify the context by hands , these modifications are being lost on next : minikube start . .  	1	1
2 2 1.4 2.0 1.0 1.0 1.05 1.0 1.6666666666666667 2.0 1.0978260869565217 92 1.0 0 7 25 63	direct user to using minikube's inner kubectl when not installed . on a system without kubectl : : med @xmac : ~ $ minikube start 棣冩 minikube v 1.9.2 on Ubuntu 20.04 閴?Automatically selected the docker driver 棣冩啢 Starting control plane node m01 in cluster minikube 棣冩 Pulling base image ... 棣冩崙 Downloading Kubernetes v 1.18.0 preload ... > preloaded-images-k8s-v2-v 1.18.0 -docker-overlay2-amd64 . tar . lz4 : 542.91 MiB 棣冩暉 Creating Kubernetes in docker container with ( CPUs = 2 ) ( 8 available ) , Memory = 3900MB ( 15990MB available ) ... 棣冩儞 Preparing Kubernetes v 1.18.0 on Docker 19.03.2 ... 閳?kubeadm . pod-network-cidr = 10.244.0.0 /16 棣冨皞 Enabling addons : default-storageclass , storage-provisioner 棣冨及 Done ! kubectl is now configured to use ' minikube ' 棣冩寱 For best results , install kubectl : < URL > . instead of saying : 棣冩寱 For best results , install kubectl : < URL > . we could provide them a link to docs ( and add nice docs in website ) how to use kubectl inside minikube minikube kubectl -- get pods ... and then then in the end say , alternatively install kubectl  	1	1
1 1 1.4 2.0 1.1 1.0 1.5 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 1 5	Support extra volumes for Kubeadm . This is a feature request . Support for configuring these fields in the kubeadm . yaml configuration file ( implemented by < URL > : APIServerExtraVolumes ControllerManagerExtraVolumes SchedulerExtraVolumes . This will help support extra options like : -- extra-config = apiserver . token-auth-file . . Currently , this does not work because the : kube-apiserver . docker container could not find the file . The path of the token file must be mounted first . Environment : Minikube version : v 0.26.1 OS : Ubuntu 16.04 VM Driver : virtualbox ISO version : v 0.26.0	2	1
0 0 1.0 1.0 0.9 1.0 0.9 1.0 1.3333333333333333 2.0 0.9893617021276596 94 1.0 2 8 24 64	kic : ' delete ' hangs forever with a paused docker cluster . Something I found during benchmarking : : minikube start -- driver = docker . : minikube pause . : minikube delete -- all -- alsologtostderr . outputs : : I0413 16:33:07 . 799145 30021 oci . go : 250 ] executing with [ docker ps -a -- filter label = name.minikube.sigs.k8s.io -- format {{ . Names }}] timeout : 30s I0413 16:33:07 . 839564 30021 oci . go : 250 ] executing with [ docker ps -a -- filter label = created_by . minikube.sigs.k8s.io=true -- format {{ . Names }}] timeout : 30s I0413 16:33:07 . 877344 30021 oci . go : 250 ] executing with [ docker inspect minikube -- format ={{ . State . Status }}] timeout : 19s . ps shows : : 19267 30025 30021 0 4:33 PM ttys000 0:00 . 05 docker rm -f -v minikube . : minikube unpause . after this shows : : 棣冩寴 Pause : list paused : docker : docker ps -- filter status = paused -- filter = name = k8s_ . *_(kube-system|kubernetes-dashboard|storage-gluster|isti o-o perator)_ -- format ={{ . ID }}: exit status 126 stdout : OCI runtime exec failed : exec failed : container_linux . go : 349 : starting container process caused ' process_linux . go : 101 : executing setns process caused /'exit status 1/': unknown . It's unclear what's going on , as : minikube ssh . works just fine after a pause . Seemingly the only way to unwedge things is to restart docker .	0	0
2 2 1.4 2.0 1.3 2.0 1.1 1.0 1.0 1.0 0.0 0 0.0 1 1 3 37	`node list` command not print all nodes . . : minikube node list . command only print nodes in default profile . It should print all nodes regardless of their profile . Also , Implement additional argument to filter nodes by profile name will be good . : 閴?minikube profile list | --------------|----------- | ---------|-------------- | ------|--------- | ---------|------- | | Profile | VM Driver | Runtime | IP | Port | Version | Status | Nodes | | --------------|----------- | ---------|-------------- | ------|--------- | ---------|------- | | minikube | docker | docker | 192.168.49.2 | 8443 | v 1.20.0 | Stopped | 2 | | test-profile | docker | docker | 192.168.49.2 | 8443 | v 1.20.0 | Running | 2 | | --------------|----------- | ---------|-------------- | ------|--------- | ---------|------- | 閴?Found 2 invalid profile(s ) ! test-profile-m02 minikube-m02 棣冩寱 You can delete them using the following command(s ): $ minikube delete -p test-profile-m02 $ minikube delete -p minikube-m02 閴?minikube node list minikube 192.168.49.2 minikube-m02 192.168.49.3 .	2	1
2 2 1.2 1.0 1.0 1.0 1.2 1.5 1.6666666666666667 2.0 0.0 0 0.0 0 12 15 30	Incompatible kubectl version warning is misleading . Using minikube 1.6.2 , kubernetes-cli 1.17.0 and trying to start a cluster with Kubernetes version 1.15.3 it produces a misleading warning : : 閳跨媴绗?/usr/local/bin/kubectl is version 1.17.0 , and is incompatible with Kubernetes 1.15.3 . You will need to update /usr/local/bin/kubectl or use ' minikube kubectl ' to connect with this cluster . This is misleading because ' updating ' : /usr/local/bin/kubectl . will not fix the problem . It should say ' downgrade ' rather than ' update ' . The exact command to reproduce the issue : : minikube start -- kubernetes-version = 1.15.3 . The full output of the command that failed : : 棣冩 [ observability ] minikube v 1.6.2 on Darwin 10.14.6 閳?MINIKUBE_PROFILE = observability 閳?MINIKUBE_KUBERNETES_VERSION = 1.15.3 閴?Selecting ' hyperkit ' driver from existing profile ( alternates : [ virtualbox ]) 棣冩啢 Kubernetes 1.17.0 is now available . If you would like to upgrade , specify : -- kubernetes-version = 1.17.0 棣冩敡 Starting existing hyperkit VM for ' observability ' ... 閳?Waiting for the host to be provisioned ... 棣冩儞 Preparing Kubernetes v 1.15.3 on Docker ' 19.03.5 ' ... 閳?kubelet . authentication-token-webhook = true 閳?kubelet . authorization-mode = Webhook 閳?scheduler . address = 0.0.0.0 閳?controller-manager . address = 0.0.0.0 棣冩畬 Launching Kubernetes ... 棣冩寙 To connect to this cluster , use : kubectl -- context = observability 閳跨媴绗?/usr/local/bin/kubectl is version 1.17.0 , and is incompatible with Kubernetes 1.15.3 . You will need to update /usr/local/bin/kubectl or use ' minikube kubectl ' to connect with this cluster . The output of the : minikube logs . command : The operating system version : macOS 10.14.6 ( 18G103 )    	1	2
2 2 1.2 1.0 1.2 1.0 1.2 1.0 1.6666666666666667 2.0 0.9880952380952381 84 1.0 2 3 9 53	Factor in -- interactive flag value in driver autodetection . Some drivers may require an interactive installation . The driver detection should take this into account .  	1	1
1 1 1.8 2.0 1.5 2.0 1.3 2.0 1.6666666666666667 2.0 2.0 3 2.0 0 0 3 20	Output tables for commands should be the same as kubectl tables . The exact command to reproduce the issue : : minikube profile list . The full output of the command that failed : : | ----------|------------ | ----------------|----------- | -------------------- | | Profile | VM Driver | NodeIP | Node Port | Kubernetes Version | | ----------|------------ | ----------------|----------- | -------------------- | | minikube | virtualbox | 192.168.99.169 | 8443 | v 1.16.0 | | ----------|------------ | ----------------|----------- | -------------------- | . The operating system version : : minikube version minikube version : v 1.4.0 commit : 90bf0f8ec57f051ac014d3b0db6d9051475874d6 . Why don't we use a regular table like kubectl uses without pipe separators ? It makes it harder to wrap scripts around minikube when we are doing this . I would expect something like this instead : : PROFILE VM DRIVER NODEIP NODE PORT KUBERNETES VERSION minikube virtualbox 192.168.99.169 8443 v 1.16.0 .	2	1
2 2 1.4 2.0 1.0 1.0 0.85 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 8 12 45	sch_htb kernel module missing for network traffic shaping . The : net_htb . module is missing . Aside from general network traffic shaping , : net_htb . is used by Chaos Engineering tools such as < URL > to simulate poor network conditions , latency , etc . This is a similar use case to #6033 and #6738 . The full output of the command that failed : : $ modprobe -n net_htb modprobe : FATAL : Module net_htb not found in directory /lib/modules/ 4.19.81 . The operating system version : minikube version : v 1.7.2 commit : 50d543b5fcb0e1c0d7c27b1398a9a9790df09dfb	2	1
0 0 1.2 2.0 1.4 2.0 1.4 2.0 1.3333333333333333 2.0 1.0 1 1.0 0 2 5 25	gcp-auth registries is too restrictive . The gcp-auth addon configures : kubernetes.io/dockercfg . secrets but only for : gcr.io . and : us-docker . pkg . dev . . < URL > We should either make this configurable or add support for a broader set of endpoints . One approach would be to query GCR/AR for the list of registries visible to the account . cc : @sharifelgamal	0	1
1 1 0.4 0.0 0.7 1.0 0.75 1.0 0.6666666666666666 1.0 0.0 0 0.0 1 5 6 13	Option to skip kubeadm preflight checks . FEATURE REQUEST Since a new docker version has been released and is not validated , the preflight checks raise an error and prevent minikube to start . This would be nice if we could add a custom : -- ignore-preflight-errors . to kubeadm , currently hardcoded here : < URL > Also eventually override or remove the default ones depending on the environnement . Minikube version : v 0.28.2 OS : Ubuntu 18.04.1 VM Driver : none	2	1
1 1 0.4 0.0 0.4 0.0 0.8 1.0 0.3333333333333333 0.0 1.152542372881356 118 1.0 2 4 11 29	Use gotestsum to generate minikube integration test JSON . The current logs all claim to have been run the same second , with duration in microseconds : : {' Time ' : ' 2021-05-01T 17:13:07 . 828742425+ 02:00 ' , ' Action ' : ' run ' , ' Test ' : ' TestFunctional/parallel/BuildImage ' } {' Time ' : ' 2021-05-01T 17:13:07 . 8287592+ 02:00 ' , ' Action ' : ' pause ' , ' Test ' : ' TestFunctional/parallel/BuildImage ' } {' Time ' : ' 2021-05-01T 17:13:07 . 828763322+ 02:00 ' , ' Action ' : ' cont ' , ' Test ' : ' TestFunctional/parallel/BuildImage ' } {' Time ' : ' 2021-05-01T 17:13:07 . 828858375+ 02:00 ' , ' Action ' : ' pass ' , ' Test ' : ' TestFunctional/parallel/BuildImage ' , ' Elapsed ' : 0.7 } . In reality , the tests had rather different timings which can be seen when reading the live output : : {' Time ' : ' 2021-05-01T 17:13:53 . 877285629+ 02:00 ' , ' Action ' : ' run ' , ' Test ' : ' TestFunctional/parallel/BuildImage ' } {' Time ' : ' 2021-05-01T 17:13:53 . 877327628+ 02:00 ' , ' Action ' : ' pause ' , ' Test ' : ' TestFunctional/parallel/BuildImage ' } {' Time ' : ' 2021-05-01T 17:13:53 . 877331911+ 02:00 ' , ' Action ' : ' cont ' , ' Test ' : ' TestFunctional/parallel/BuildImage ' } {' Time ' : ' 2021-05-01T 17:13:54 . 571565772+ 02:00 ' , ' Action ' : ' pass ' , ' Test ' : ' TestFunctional/parallel/BuildImage ' , ' Elapsed ' : 0.69 } . The root cause is misusing the : test2json . tool . Inserting timestamps assumes a live test stream .	0	0
1 1 1.8 2.0 1.5 2.0 1.55 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 0 3	mount : Error finding IPV4 address for VirtualBox Host-Only Ethernet Adapter . Bug Report Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.24.1 - OS ( e.g. from /etc/os-release ): Windows 10 Home x64 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): virtualbox ( v 5.2.6 ) - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.23.6 - Install tools : Installed via choco What happened : I tried mounting a folder from my host machine into minikube via : minikube mount C : /Users/Benedikt/projects/ : /projects . This failed with the following error message : : E0121 16:05:59 . 868890 6556 mount . go : 113 ] Error getting the host IP address to use from within the VM : Error getting VM/Host IP address : Error finding IPV4 address for VirtualBox Host-Only Ethernet Adapter #4 . I then followed the workaround mentioned in #1473 and renamed the Network Interface according to what the error output showed . After this , i could correctly mount the folder and access the files from within minikube . What you expected to happen : The mount correctly executes and my host files are available in Minikube . How to reproduce it ( as minimally and precisely as possible ): Try mounting a volume into minikube with virtualbox provider .	2	0
0 0 1.4 2.0 1.0 1.0 0.95 1.0 1.0 1.0 0.8270042194092827 237 1.0 0 2 4 18	Separate Preload for Kuberentes Images and anything else . This will make user not have to download another preload for only bumping Dashboard image or stuff like this < URL >  	1	1
0 0 1.0 1.0 1.1 1.5 1.2 1.5 1.0 1.0 0.0 0 0.0 0 1 2 11	sudo -E minikube start -- vm-driver = none does not change directory owner when CHANGE_MINIKUBE_NONE_USER = true . BUG REPORT Environment : - Minikube version ( use : minikube version . ): v 0.30.0 - OS ( e.g. from /etc/os-release ): 18.04.1 LTS ( Bionic Beaver ) - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): none - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): - Install tools : - Others : What happened : sudo -E minikube start -- vm-driver = none ends with an error . : ubuntu @u1804 : ~ $ sudo -E minikube start -- vm-driver = none Starting local Kubernetes v 1.10.0 cluster ... Starting VM ... Getting VM IP address ... Moving files into cluster ... Downloading kubeadm v 1.10.0 Downloading kubelet v 1.10.0 Finished Downloading kubeadm v 1.10.0 Finished Downloading kubelet v 1.10.0 Setting up certs ... Connecting to cluster ... Setting up kubeconfig ... Starting cluster components ... E1026 05:34:36 . 554940 25621 start . go : 297 ] Error starting cluster : timed out waiting to elevate kube-system RBAC privileges : creating clusterrolebinding : Post < URL > Gateway Timeout . $HOME/ . kube and $HOME/ . minikube owner are not changed even if CHANGE_MINIKUBE_NONE_USER = true . What you expected to happen : When CHANGE_MINIKUBE_NONE_USER = true , $HOME/ . kube and $HOME/ . minikube owner are changed from root to $HOME_USER How to reproduce it ( as minimally and precisely as possible ): Follow the URL : < URL > Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	0
2 2 1.0 1.0 1.0 1.0 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 2 5 6 37	2.0.0 -dev is not getting accepted as valid version . : $ minikube start -- driver = podman -- container-runtime = cri-o 棣冩 minikube v 1.12.0 -beta . 0 on Fedora rawhide 閴?Using the podman ( experimental ) driver based on user configuration 閴?' podman ' driver reported an issue : Invalid character(s ) found in prerelease ' dev/n ' 棣冩寱 Suggestion : Cant verify minimum required version for podman . See podman website for installation guide . 棣冩憣 Documentation : < URL > 棣冩寴 Failed to validate ' podman ' driver . : $ podman version Version : 2.0.0 -dev API Version : 1 Go Version : go 1.14.3 Git Commit : 9980991f9be8f5fbb4e87caf5ac34ce2575a232c Built : Thu Jun 18 03:00:00 2020 OS/Arch : linux/amd64 . < URL >	2	0
2 2 1.4 2.0 1.6 2.0 1.3 2.0 1.3333333333333333 2.0 0.0 0 0.0 2 5 22 65	Add list option for ' minikube node ' command . with #94 adding support for multi-node kubernetes cluster with minikube , we need and option to list the nodes . Currently there is no ' minikube node list ' command . minikube version : v 1.9.2 current output : minikube node list 棣冩寱 Usage : minikube node [ add|start|stop|delete ] expected output : ' minikube node list ' should list the node name , status and ip addresses  	1	1
1 1 1.2 1.0 1.2 1.5 1.35 2.0 1.0 1.0 0.8425925925925926 216 1.0 0 3 7 27	minikube start should wailt till ' download-only ' is finishing . we could use our lock package < URL > to put a lock when : minikube start -- download-only . happens and then when : minikube start . runs when this lock exists will Wait for it to finish up to a reasonable time	0	1
2 2 1.0 1.0 1.2 1.0 1.25 1.5 0.6666666666666666 0.0 0.0 0 0.0 1 2 3 7	Allow minikube . iso download to be resumeable . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Environment : Minikube version : v 0.28.0 OS : Debian GNU/Linux 9 ( stretch ) Hello everybody , I installed Minikube yesterday and executed : minikube start . , after that it starts downloading Minikube . ISO . I had an internet connection problem , than I was obliged to re-execute : minikube start . . I excepted it to continue the downloading from where it stopped , but it restarted from the beginning . The same happened when downloading kubelet . I would like to suggest restarting the download from the point of interruption . I would like to work on this issue , and I dig a little in the code , but I don't know if it is related to Minikube itself , or Docker , or the server that provides the ISO . So i'll be glad if someone more experienced want to work on this , or guide me to work on it . Thank you .	2	1
0 0 0.8 1.0 1.0 1.0 0.95 1.0 0.6666666666666666 1.0 0.5 6 0.5 0 2 5 31	Use containerd runtime by default . It looks like in the next years Kubernetes development will be based on containerd as CRI implementation . Let's follow it and switch default container runtime from docker to containerd .  	1	1
2 2 1.2 1.0 1.2 1.0 1.1 1.0 1.6666666666666667 2.0 0.3076923076923077 13 0.0 0 2 7 29	Fix `minikube start && stop && start` in Cloud Shell : ubepods . slice already exists . ' . Error Message : Aug 16 21:40:19 minikube kubelet[8019 ]: E0816 21:40:19 . 352054 8019 kubelet . go : 1384 ] ' Failed to start ContainerManager ' err='Unit kubepods . slice already exists . ' . Running : : minikube start minikube stop minikube start . In Cloud Shell , results in the following failure on the second start . File : < URL >	0	0
0 0 0.8 0.0 1.2 2.0 1.1 1.0 0.6666666666666666 0.0 1.054054054054054 74 1.0 1 2 2 18	master : isti o-o perator should not be enabled by default . On a fresh start : : NAMESPACE NAME READY STATUS RESTARTS AGE isti o-o perator isti o-o perator-5488b4489d-nqqm9 1/1 Running 0 15s kube-system coredns-6955765f44-nnprq 0/1 Running 0 15s kube-system coredns-6955765f44-zpltx 0/1 Running 0 15s kube-system etcd-minikube 1/1 Running 0 24s kube-system kube-addon-manager-minikube 1/1 Running 0 24s kube-system kube-apiserver-minikube 1/1 Running 0 24s kube-system kube-controller-manager-minikube 1/1 Running 0 23s kube-system kube-proxy-k5qmt 1/1 Running 0 15s kube-system kube-scheduler-minikube 1/1 Running 0 23s kube-system storage-provisioner 1/1 Running 1 15s .	0	0
1 1 1.2 1.0 1.3 1.0 1.25 1.0 1.0 1.0 0.8303571428571429 224 1.0 2 4 7 33	  pullsheet leaderborad page in contributors website section .	0	2
0 0 0.6 1.0 0.8 1.0 1.05 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 3 6 26	Make svc from one minikube cluster talk to another svc in another minikube cluster .	2	1
1 1 0.8 1.0 1.3 1.5 1.2 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 6 7 24	start/delete fails : open C : /Users/ . minikube/machines/minikube/config . json : The system cannot find the file specified . . I downloaded minikube in my windows 7 laptop . I installed it successfully . but when I try to run below command , it fails with below message . I tried minikube delete command also but it also fails . Not sure how to proceed . Please help , $ minikube start * minikube v 1.4.0 on Microsoft Windows 7 Enterprise Service Pack 1 6.1.7601 Build 7601 * Retriable failure : Error loading existing host . Please try running [ minikube delete ] , then run [ minikube start ] again . : filestore ' minikube ' : open C : /Users/ . minikube/machines/minikube/config . jso n : The system cannot find the file specified . * Retriable failure : Error loading existing host . Please try running [ minikube delete ] , then run [ minikube start ] again . : filestore ' minikube ' : open C : /Users/ . minikube/machines/minikube/config . jso n : The system cannot find the file specified . * Retriable failure : Error loading existing host . Please try running [ minikube delete ] , then run [ minikube start ] again . : filestore ' minikube ' : open C : /Users/ . minikube/machines/minikube/config . jso n : The system cannot find the file specified . * Retriable failure : Error loading existing host . Please try running [ minikube delete ] , then run [ minikube start ] again . : filestore ' minikube ' : open C : /Users/ . minikube/machines/minikube/config . jso n : The system cannot find the file specified . * X Unable to start VM : Error loading existing host . Please try running [ minikube delete ] , then run [ minikube start ] again . : filestore ' minikube ' : open C : /Users/ . minikube/machines/minikube/config . js on : The system cannot find the file specified .	2	0
2 2 1.4 2.0 0.9 1.0 1.25 1.5 1.0 1.0 0.0 0 0.0 1 1 4 18	  vmware driver install instructions for Mac . These instructions for installation on a Mac are wrong . r = < URL > curl -LO $(curl -s $r/releases/latest | grep -o ' http . * darwin_amd64 ' | head -n1 ) / && install docker-machine-driver-vmware_darwin_amd64 / /usr/local/bin/docker-machine-driver-vmware You need an uppercase ' D ' ( for darwin ) in ' http . * darwin_amd64 ' and also ' docker-machine-driver_darwin_amd64 ' or change the name of the distribution file in Github	0	2
0 0 0.8 0.0 1.4 2.0 1.2 1.5 0.6666666666666666 0.0 0.8242677824267782 239 1.0 3 5 7 13	addon list should list addons even if minikube cluster is not running . for not running it shoudl list only names witihout status	0	1
0 0 1.6 2.0 1.4 2.0 1.2 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 1 3 13	Minikube ' start ' fails when using the ' -- cache-images ' option . BUG REPORT Environment Windows 10 Minikube version 0.30.0 OS NA VM Driver VirtualBox ISO version 0.30.0 What happened : Running ' minikube start ' with the ' -- cache-images ' option hangs as it appears that image caching fails to complete . Waiting for image caching to complete ... Moving files into cluster ... .......... hangs at this point How to reproduce it ( as minimally and precisely as possible ): ' minikube start -- cache-images ' Output of : minikube logs . ( if applicable ) : Adding the ' -v 10 ' option to the start command results in the following error being displayed : Waiting for image caching to complete ... E1013 14:36:00 . 778091 18324 start . go : 247 ] Error caching images : Caching images for kubeadm : caching images : caching image C : /Users/Mark . minikube/cache/images/ k8s.gcr.io/pause-amd64_3.1 : getting destination path : parsing docker archive dst ref : replace a Win drive letter to a volume name : CreateFile / ? /Volume{806aa746-d64c-11e5-acef-806e6f6e6963}/Users/Mark . minikube/cache/images/ k8s.gcr.io: The system cannot find the path specified . Moving files into cluster ... Anything else do we need to know :   	1	0
2 2 1.4 1.0 1.3 1.0 1.1 1.0 1.3333333333333333 1.0 0.4782608695652174 23 0.0 1 3 4 18	Go through all documentation and ensure commands/tutorials exist for both Unix & Windows . Our documentation is generally written for Unix , this can be annoying if users have to find the Windows versions of the commands online themselves . To help out our Windows users we should ensure all documentations contains both Unix & Windows commands , this will greatly help out new Windows users who are trying to get started with Windows .    	1	2
2 2 0.6 0.0 1.1 1.5 1.0 1.0 0.6666666666666666 0.0 1.1333333333333333 15 1.0 0 0 2 23	pkg/util : TestTeePrefix has race conditition . There is something unsafe going on , with the use of : Scanner . in TeePrefix : < URL > failed : : --- FAIL : TestTeePrefix ( 0.00 s ) utils_test . go : 197 : output=' , want : ' goo/ng/r/n/r/nle ' utils_test . go : 203 : log=' , want : ' ( : goo )(: g )(: le )' . Reproducer : : GO111MODULE = on go test -race . /pkg/util/utils . go . /pkg/util/utils_test . go . WARNING : DATA RACE  	1	0
1 1 0.8 1.0 0.8 1.0 1.15 1.0 1.0 1.0 1.0909090909090908 88 1.0 4 11 29 60	Warn user on start if node is under disk pressure .	0	1
0 0 1.2 1.0 1.1 1.0 1.2 1.0 1.3333333333333333 2.0 0.625 16 0.0 3 5 7 29	Make sure we fall back to cache and load if preload fails . If preload fails for any reason , make sure we fall back to caching and loading images as usual .	0	1
2 2 0.6 0.0 1.1 1.5 1.2 1.5 1.0 1.0 1.3333333333333333 6 1.5 0 1 5 28	uninstaller should be able to delete VM's . As of now , running the uninstaller of : minikube . just removes the folder in which : minikube . was installed . This should have the capability to do a clean uninstall of the minikube . Probably have an option to do a : Clean Uninstall . in which it removes the following - Stops & deletes the Virtual machines created by Minikube ( Possibly for all the profiles available ) Removes the : . minikube . folder in ' C : /Users/USER_NAME ' EDIT -- I believe this should apply for the chocolatey installer as well .	2	1
0 0 1.2 1.0 1.2 1.0 1.2 1.0 1.0 1.0 1.6666666666666667 6 2.0 0 0 6 21	Add option to delete all profiles . minikube should have an option to delete all profiles with one command . : minikube delete -- all .	2	1
1 1 1.2 1.0 0.6 0.0 1.05 1.0 1.3333333333333333 1.0 1.0526315789473684 19 1.0 0 1 3 16	none driver : minikube shouldn't write binaries to /usr/bin . With the : none . driver , minikube will overwrite the following system paths : /usr/bin/kubeadm - Updated to match the exact version of Kubernetes selected /usr/bin/kubectl - Updated to match the exact version of Kubernetes selected Basically , anywhere else is better , but to write directly to /usr/bin feels yucky . It'd be nicer to write the binaries somewhere else , preferably but not necessarily versioned . For instance : : $XDG_CACHE_HOME/minikube/binaries/kubeadm-v 1.13.0 . Or : : /usr/local/minikube/kubeadm-v 1.13.0 .	0	0
2 2 0.8 0.0 0.9 0.5 0.9 1.0 0.6666666666666666 0.0 2.0 1 2.0 6 10 13 33	tiller link is broken . This link is broken : < URL > in this page : < URL >  	2	2
0 0 1.0 1.0 0.8 0.5 1.0 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 4 6 32	dashboard : Add Node condition check ( DiskPressure and pod status checks ) before opening . when i run it always hangs forever and sometimes throws 503 garretsidzaka@$$$$$ : ~ $ sudo minikube dashboard : Verifying dashboard health ... Launching proxy ... Verifying proxy health ... . ^C garretsidzaka@$$$$$ : ~ $ garretsidzaka@$$$$$$ : /usr/bin$ minikube version minikube version : v 1.4.0 commit : 7969c25 ubuntu vm 1804 vm-driver = none  	1	1
0 0 1.2 2.0 1.3 2.0 1.2 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 2 7 37	Provide more descriptive error codes for GUEST_PROVISIONG . GUEST_PROVISION comprises 8.5% of minikube start errors in our environment . These could be because minikube failed to obtain driver options/ generate cluster config/ cache minikube ISO . Can minikube provide more descriptive error codes for their scenarios ? It would also help to make the error messages actionable wherever possible .  	1	1
1 1 1.0 1.0 0.9 1.0 0.95 1.0 0.6666666666666666 1.0 1.5 8 2.0 1 1 5 26	Can we get Sonarcloud setup for our main repository ? . Can we get Sonarcloud setup for our main repository ? This is free for open source projects and provides valuable metrics like code quality issues and what not . It also supports Pull Request validations . I have set it up on my fork and you can see the results over here - < URL > -Pranav	2	1
2 2 1.4 1.0 1.4 1.5 1.2 1.0 1.6666666666666667 2.0 0.9743589743589743 156 1.0 2 4 6 24	  Update docs on office hours and ensure everyone gets correct calendar invite . lets add minikube-dev on calendar .	0	2
0 0 1.6 2.0 1.2 1.5 1.1 1.0 1.3333333333333333 2.0 1.1363636363636365 44 1.0 4 9 20 81	  Add important requirements to none driver page . We now link to the < URL > directly from < URL > which is good . But we should list some of the more important requirements from : kubeadm . explicitly : 2 CPU 2200M RAM available iptables ( in legacy mode ) conntrack ( and others ? ) SELinux permissive Downgrade to cgroups v1 Maybe we should rephrase ' VM ' as something virtual or physical : A Linux VM with the following : Docker systemd ( OpenRC based systems are also supported in v 1.10 +) This should probably say ' container runtime ' instead of just Docker . Also need to install : crictl . At least when we fix the bug that all runtimes currently require a : docker . binary in order to start Most of these should probably be validated , and give helpful messages . This to cut down on the number of identical support issues being opened .   	1	2
0 0 1.2 1.0 1.2 1.0 1.2 1.0 1.3333333333333333 2.0 1.144578313253012 83 1.0 1 1 5 33	Make the available drivers section a bit clearer . Currently it is a bit unclear what you actually need : Container or virtual machine manager , such as : Docker , Hyperkit , Hyper-V , KVM , Parallels , Podman , VirtualBox , or VMWare This mixes containers and virtual machines etc . We need to make some kind of distinction between local drivers Docker and Podman , and remote drivers Docker and Podman . And try to explain how it is related to the hypervisors , Hyperkit , Hyper-V , KVM ( Libvirt ) , Parallels , VMware and the : -- vm = true . Beyond separating Docker Engine and Docker Desktop .  	2	2
0 0 0.8 0.0 1.1 1.5 1.35 2.0 0.6666666666666666 0.0 0.0 0 0.0 0 2 8 31	xhyve : json : cannot unmarshal bool into Go struct field Driver . Virtio9p of type [] string . The exact command to reproduce the issue : minikube start -- vm-driver = xhyve The full output of the command that failed : minikube start -- vm-driver = xhyve 棣冩 minikube v 1.0.1 on darwin ( amd64 ) 棣冦仚 Downloading Kubernetes v 1.14.1 images in the background ... 閳跨媴绗?The xhyve driver is deprecated and support for it will be removed in a future release . Please consider switching to the hyperkit driver , which is intended to replace the xhyve driver . See < URL > for more information . To disable this message , run [ minikube config set WantShowDriverDeprecationNotification false ] 棣冩暉 Creating xhyve VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... 棣冩寴 Unable to start VM : new host : json : cannot unmarshal bool into Go struct field Driver . Virtio9p of type [] string 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The output of the : minikube logs . command : minikube logs 棣冩寴 api load : filestore : Docker machine ' minikube ' does not exist . Use ' docker-machine ls ' to list machines . Use ' docker-machine create ' to add a new one . 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The operating system version : Mac OS mojave 10.14.3	2	0
1 1 1.4 1.0 1.4 1.5 1.2 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 0 0 0	Proxy for easier access to NodePort services . One of the major hurdles people have using k8s as a development platform is having easy access to DNS and uncomplicated access to ' localhost ' ports . This might be something we can tackle in this bug , I discussed the idea here : < URL > Option 1 - Fancy Proxy This is an idea to make working with the single-node cluster easier . The basic idea would be to have something like : kubectl port-forward . that forwards every nodePort to localhost based on the original targetPort . So , for example : : # User does something like this $ kubectl run -- image quay.io/philips/golang-outyet outyet $ kubectl expose deployment outyet -- target-port = 8080 -- port = 8080 -- type = NodePort # This is the part that needs automating : $ socat tcp-listen : 8080 , reuseaddr , fork tcp : 172.17.4.99 :$( kubectl get service outyet -o template -- template='{{range . spec . ports }}{{ . nodePort}}{{end }}') . This would be a huge boon to people trying to use kubernetes as a development workflow for running caches , services , etc and developing against those APIs . Psuedo code event loop : : for { for _ , e : = range kubernetesServiceEvent () { if e = = newNodePort { go proxy(context , e . NodePort , e . NodeIP , ' localhost ' , e . TargetIP ) } if e = = dyingNodePort { contexts[e . NodePort ] . Done () } } } . Option 2 - VPN Having a simple VPN setup would allow a user to get access to cluster DNS and cluster networking . The downside here is that stuff like OpenVPN , etc is a major hurdle . Anyone know of a simple VPN in Go that works cross platform ?	2	1
2 2 0.8 1.0 1.0 1.0 0.85 1.0 0.6666666666666666 0.0 0.0 1 0.0 0 3 5 28	Add transient states (' stopping ' , ' starting ' ) . Minikube status lists Running , Paused or Stopped for each profile . It does not account for the state between minikube start and Running . It would be helpful to indicate when minikube is starting , pausing ( maybe , this state seems like it might be super short ) , stopping and possibly deleting as well as valid states .	0	1
0 0 0.8 0.0 0.9 0.5 1.05 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 3 4 30	Document how to use the registry addon - covering usage on none & Windows . The operating system version : Microsoft Windows 10 PRO - Version 10.0.17134 Build 17134 Can we add to the documentation a tutorial of some sort on how to ' Use local images by re-using the Docker daemon ' or how to have a minikube registry up and running on Windows ? I am really struggling with this one and in the end , after spending 6 hours trying to set up with no success I ended up using Amazon ECR for pushing images and then pulling down to Minikube with skaffold . Some sort of step by step tutorial for local development with local registries or other mechanisms on Windows with Minikube would be highly appreciated . Thank you ! Let me know your thoughts guys .  	2	2
2 2 1.4 2.0 1.4 2.0 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 3 5 19 47	Cannot start podman driver with doas instead of sudo . Steps to reproduce the issue : : $ minikube start -- driver podman 棣冩 minikube v 1.17.1 on Arch rolling 閴?Using the podman ( experimental ) driver based on user configuration 棣冩寴 Exiting due to PROVIDER_PODMAN_NOT_RUNNING : exec : ' sudo ' : executable file not found in $PATH 棣冩憣 Documentation : < URL > . I'm using : doas . on my system instead of : sudo . ( because it's much more light weight and secure ) . It would be great if minikube could fall back to : doas . in such a case or if I could at least specify the ' sudo ' command and set it to ' doas ' .	2	1
2 2 1.4 2.0 1.5 2.0 1.45 2.0 1.0 1.0 0.0 0 0.0 0 3 4 18	Run ingress addon on without the annotaion on the Ingress . Steps to reproduce the issue : Execute minishift with ingress enable Apply an ingress object without annotation or IngressClass field Ingress controller fails with ' removing ingress because of unknown ingressclass ' and Ingress doesn't work at expected . I would like to enable ingress add-on but I don't want to annotate all my ingress objects . Is there a way to enable : -- watch-ingress-without-class . flag on the nginx ingress controller ? I'm using minikube 1.23.1 < URL >  	1	1
1 1 1.6 2.0 1.3 1.5 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 1 3 11 28	Translation error in every minikube command since 1.15.0 . When I am trying to execute any command in minikube I have error message : : ~ $ minikube version I1114 13:59:47 . 843637 11252 translate . go : 89 ] Failed to load translation file for en : Asset translations/en . json not found minikube version : v 1.15.0 commit : 3e098ff146b8502f597849dfda420a2fa4fa43f0 .	0	0
0 0 0.6 0.0 0.9 1.0 1.0 1.0 0.0 0.0 1.0 20 1.0 4 9 10 30	Unable to pull images on Kubernetes v 1.10.2 : Error : unknown flag : -- config . On v 0.35.0 : Command : : /usr/local/bin/minikube start -- cpus = 4 -- memory = 4096 -- disk-size = 30g -- vm-driver = hyperkit -- kubernetes-version = v 1.10.12 -- extra-config = apiserver . authorization-mode = RBAC . Output : : 閴?Unable to pull images , which may be OK : running cmd : sudo kubeadm config images pull -- config /var/lib/kubeadm . yaml : command failed : sudo kubeadm config images pull -- config /var/lib/kubeadm . yaml stdout : stderr : Error : unknown flag : -- config Usage : . It seems that we need some logic to invoke the command-line differently in older versions of Kubernetes .	0	0
2 2 1.4 2.0 1.5 2.0 1.15 1.5 1.0 1.0 0.0 0 0.0 0 0 2 17	`Minikube VM` is missing a Netfilter `xt_socket` module required for Transparent Proxying ( TPROXY ) . Transparent Proxying ( TPROXY ) is a feature of Linux Kernel that is used by Service Meshes , such as Istio or Kong , to ' intercept ' traffic in a side car process . According to < URL > , Transparent Proxying requires 2 : Netfilter . modules to be present : 1 . : NETFILTER_X T_T ARGE T_T PROXY . 2 . : NETFILTER_XT_MATCH_SOCKET . At the moment , : Minikube VM . comes only with : NETFILTER_X T_T ARGE T_T PROXY . module and is missing : NETFILTER_XT_MATCH_SOCKET . . How to reproduce : minikube ssh . : sudo iptables -t mangle -N TPROXY_ISSUE . : sudo iptables -t mangle -A TPROXY_ISSUE -p tcp -m socket -j RETURN . The last command will fail with an error message : iptables : No chain/target/match by that name . . Reproducible on v 0.33.1	0	1
0 0 1.0 1.0 1.0 1.0 1.1 1.0 0.6666666666666666 1.0 0.2857142857142857 14 0.0 1 3 7 30	Run CPU benchmark daily via GitHub Actions . We have the ability to run CPU benchmarks via : make cpu-benchmark-idle . & : make cpu-benchmark-autopause . which creates a chart such as < URL > Two problems with this : 1 . This command has to be run manually , and then manually commit and create a PR 2 . This command only shows the latest run , it would be good to see the CPU usage over time So we should create a GCP bucket to store the run results , and then make a script that takes in past results and the results from the day and put them all together to make a chart like this < URL > and then push this chart and data to the bucket . And then run this script should automatically run daily via a GitHub Action so this can be as handsfree as possible .    	1	2
0 0 1.4 2.0 1.5 2.0 1.25 1.5 1.0 1.0 1.027027027027027 37 1.0 0 4 8 30	Add support for Kata containers . Similar to the gvisor addon included in minikube , minikube should also add support for < URL > The use of nested VM's might cause compatibility issues for some hypervisors , but if any incompatibilities are discovered , should can just identify those and error out . Similar to how gvisor errors out if a non-containerd runtime is requested . Help wanted !	2	1
1 1 1.6 2.0 1.6 2.0 1.3 1.5 1.3333333333333333 1.0 0.0 0 0.0 1 2 6 38	 Kernel's RPF check is set to ' loose '' prevents Calico from working . The exact command to reproduce the issue : Provision Minikube with CNI enabled : : minikube start -- memory 4096 / -- network-plugin = cni / -- extra-config = kubelet . network-plugin = cni / -- extra-config = kubelet . pod-cidr = 192.168.0.0 /16 / -- extra-config = controller-manager . allocate-node-cidrs = true / -- extra-config = controller-manager . cluster-cidr = 192.168.0.0 /16 / -- host-only-cidr = 172.17.17.1 /24 . Install Calico : : kubectl apply -f < URL > . At this point , the : calico-node . pod will be failing the readiness check . The full output of the command that failed : : int_dataplane . go 1037 : Kernel's RPF check is set to ' loose ' . This would allow endpoints to spoof their IP address . Calico requires net . ipv4 . conf . all . rp_filter to be set to 0 or 1 . If you require loose RPF and you are not concerned about spoofing , this check can be disabled by setting the IgnoreLooseRPF configuration parameter to ' true ' . . This issue can be worked around by using the following environment variable tweak : : kubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF = true . The output of the : minikube logs . command : N/A The operating system version : Windows 10 Professional using the HyperV driver .  	1	0
0 0 1.0 1.0 1.1 1.5 0.85 1.0 1.3333333333333333 2.0 1.0660377358490567 106 1.0 1 5 20 79	TestStartStop Containerd : unsupported CNI result version ' 0.4.0 ' . as seen : < URL > : Warning FailedCreatePodSandBox 3s kubelet , minikube Failed to create pod sandbox : rpc error : code = Unknown desc = failed to setup network for sandbox ' 37ed9c14865db6e40ce51512335a3365cd5b4e582b2a8fe826243fcc8232378e ' : unsupported CNI result version ' 0.4.0 ' . same problem containerd on hyperkit posibble cause it could related to the podman CNI change on the ISO < URL > on old ISO : +1 : : $ ls /etc/cni/net . d/ k8s . conf . but on the new ISO we have : $ ls /etc/cni/net . d/ 87-podman-bridge . conflist k8s . conf . could this be related to Preload ? I purged minikube and run with -- preload = false and I got exact same problem , so it is not related to to preload on containerd since we dont use it .	0	0
1 1 1.0 1.0 1.2 1.5 1.2 1.0 1.6666666666666667 2.0 0.8201754385964912 228 1.0 4 5 8 19	Flake Chart comment to incude Number of tests . currently it looks like this Originally posted by @minikube -bot in < URL > but I want to see the Summary that I see in the commit status : Docker_Linux - completed with 5 / 268 failures in 55.08 minute(s ) . ... then now put the table . that way I don't have to scroll down to see how long it took and ideally there should be a link to the gopogh on the First Title  	1	1
1 1 0.8 1.0 1.1 1.0 1.15 1.0 1.0 1.0 0.0 0 0.0 4 5 15 35	VBoxManage . exe : error : Details : code E_FAIL ( 0x80004005 ) , component MachineWrap , interface IMachine . Hi , I have installed minikube and kubernetes-cli in Windows 10 Enterprise OS using choco install After installing the same and restarting , when I am trying to create the VM , it is giving error : Unable to install as HyperV is already enabled . When I am disabling the HyperV feature and trying it once again after restart , below error is coming C : /WINDOWS/system32 > minikube start o minikube v 0.35.0 on windows ( amd64 ) Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... ! Unable to start VM : create : creating : Unable to start the VM : C : /Program Files/Oracle/VirtualBox/VBoxManage . exe startvm minikube -- type headless failed : VBoxManage . exe : error : The virtual machine ' minikube ' has terminated unexpectedly during startup with exit code -1073741819 ( 0xc0000005 ) . More details may be available in ' C : /Users/pabitra_b . minikube/machines/minikube/minikube/Logs/VBoxHardening . log ' VBoxManage . exe : error : Details : code E_FAIL ( 0x80004005 ) , component MachineWrap , interface IMachine Also note output pf below commands : C : /WINDOWS/system32 > minikube delete x Deleting ' minikube ' from virtualbox ... - The ' minikube ' cluster has been deleted . C : /WINDOWS/system32 > kubectl config use-context minikube error : no context exists with the name : ' minikube ' . C : /WINDOWS/system32 > minikube status host : kubelet : apiserver : kubectl :	2	0
1 1 1.0 1.0 0.6 0.0 0.95 1.0 1.0 1.0 1.0714285714285714 14 1.0 0 1 4 24	Make minikube ( and kvm2 ) installable with ' yum ' . When we do the apt indexing of the . deb packages * , we might as well do the yum indexing of the . rpm . See #3110 This should include both minikube and the kvm2 driver ... That is , the new docker-machine-driver-kvm2 package * . See #2982 Basically , provide something similar to what kubectl has : < URL > < URL > ( xenial ) < URL > ( el7 ) This includes signing the packages with a GPG key . Not sure if we can use the Google Cloud Packages ? < URL > < URL >	2	1
0 0 0.4 0.0 0.9 1.0 1.05 1.0 0.6666666666666666 1.0 0.9322033898305084 177 1.0 2 2 9 24	add average memory/cpu usage to minikube status .  	1	1
1 1 1.2 1.0 1.1 1.0 1.0 1.0 1.3333333333333333 1.0 0.6 10 0.0 7 8 10 58	docker driver : determine if there is any performance improvement with -- force-systemd . this will allow us to determine if we should set : -- force-systemd = true . by default	0	1
0 0 0.8 0.0 0.8 0.5 1.0 1.0 1.3333333333333333 2.0 0.0 0 0.0 2 4 8 22	NVIDIA support with docker driver . The < URL > covers the none and kvm2 drivers . The none driver is < URL > , while the kvm2 < URL > . Is support for the docker driver feasible/planned ?	2	1
0 0 0.6 0.0 0.6 0.0 0.75 0.0 0.6666666666666666 0.0 0.9622641509433962 106 1.0 3 8 15 48	-- extra-config = scheduler . LeaderElection . LeaderElect = false : unknown component . The examples on < URL > do not seem to work , and bomb out with a completely unusable error message : : minikube start -- extra-config = scheduler . LeaderElection . 	2	2
1 1 1.0 1.0 0.9 1.0 0.9 1.0 1.0 1.0 1.1967213114754098 61 1.0 2 9 20 54	docker driver should display cpu and ram availabe to docker-machine not system . docker system info -- format ' {{ json . NCPU }}' docker system info -- format ' {{ json . MemTotal }}'	0	1
1 1 1.2 1.0 1.2 1.5 0.9 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 3 16 44	Confusing error message when starting minikube with insufficient cpus . System : : $ cat /etc/os-release NAME='Ubuntu ' VERSION='20 . 04.2 LTS ( Focal Fossa )' ID = ubuntu ID_LIKE = debian PRETTY_NAME='Ubuntu 20.04.2 LTS ' VERSION_ID='20 . 04 ' HOME_URL='<URL>' SUPPORT_URL='<URL>' BUG_REPORT_URL='<URL>' PRIVACY_POLICY_URL='<URL>' VERSION_CODENAME = focal UBUNTU_CODENAME = focal . Minikube start : : $ minikube start -- memory = 6000 -- cpus = 6 -- kubernetes-version = v 1.14.1 * minikube v 1.17.1 on Ubuntu 20.04 ( amd64 ) * Automatically selected the docker driver . Other choices : ssh , none * Starting control plane node minikube in cluster minikube * Creating docker container ( CPUs = 6 , Memory = 6000MB ) ... X Exiting due to RSRC_INSUFFICIENT_CORES : Docker has less than 2 CPUs available , but Kubernetes requires at least 2 to be available . This is running on a VM with 4 vCPUs - clearly requesting 6 vCPUs was not sensible , but the error message could be made a little more abstract/generic as , in this case , Docker had more than 2 ( v)CPUs available . Looks like a change to this line is required : < URL > ( I originally posted < URL > but was advised this is a separate issue ) .  	1	0
1 1 1.6 2.0 1.4 2.0 1.35 2.0 1.6666666666666667 2.0 2.0 1 2.0 0 0 1 9	Tracker for CRI-O issues . Currently there are a lot of issues , when trying to use an < URL > ... < URL > Added this issue as a ' tracker ' , to the individual pull requests that I opened earlier . I believe all of them are needed , to again be able to use something other than Docker ? : minikube start -- container-runtime = cri-o -- network-plugin = cni . Especially once the default Kubernetes version is updated from 1.10 to 1.12 and beyond . Fixes ~ ~ #3154 Add config parameter for the cri socket path ~ ~ ~ ~ #3190 Improve the default crio-bin configuration ~ ~ ~ ~ #3194 Write /etc/crictl . yaml when starting ~ ~ ~ ~ #3211 Stop docker daemon , when running cri-o ~ ~ ~ ~ #3303 none driver : Only require docker for the docker runtime ~ ~ Features ~ ~ #2757 CRI : try to use ' sudo podman load ' instead of ' docker load'~~ ~ ~ #3225 CRI : Add Buildah to ISO , for building OCI images ~ ~     	1	2
0 0 1.4 2.0 1.3 2.0 1.1 1.0 1.0 1.0 0.9826086956521739 115 1.0 1 2 6 29	TestRunningBinaryUpgrade on Docker : [ ERROR Port-10259 ]: Port 10259 is in use . The TestRunningBinaryUpgrade test revealed flaky behavior with how we handle upgrading running Docker containers : < URL > : error execution phase preflight : [ preflight ] Some fatal errors occurred : [ ERROR Port-10259 ]: Port 10259 is in use [ ERROR Port-10257 ]: Port 10257 is in use [ ERROR Port-2379 ]: Port 2379 is in use [ ERROR Port-2380 ]: Port 2380 is in use [ preflight ] If you know what you are doing , you can make a check non-fatal with ` -- ignore-preflight-errors = ... ` . To replicate : Download minikube v 1.9.0 , run : : minikube start -- vm-driver = docker . Download minikube v 1.12.0 , run : : minikube start -- vm-driver = docker .	0	0
1 1 1.2 1.0 1.1 1.0 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 3 24	Cilium CNI does not use Minikube Default Pod CIDR . When using Cilium CNI , pods are not assigned to the < URL > but instead assigned to < URL > . Consequently , there is also a mismatch between the < URL > and the actual used < URL > since the < URL > . Steps to reproduce the issue : : minikube start -- driver = virtualbox -- cni = cilium . ( Note : MacOS with VirtualBox ) : kubectl run example -- image = busybox -- restart = Never -- sleep 1d . : kubectl get pod -o wide . Example output : : NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES example 1/1 Running 0 31s 10.88.0.3 minikube < none > < none > . It seems that Cilium CNI is not using the Kubeadm configuration . First quick-fix : Extract < URL > to a local file . Change : cluster-pool-ipv4-cidr : ' 10.0.0.0 /8 ' . to : cluster-pool-ipv4-cidr : ' 10.244.0.0 /16 ' . : minikube start -- driver = virtualbox -- cni = < path-to-modified-cilium-manifest > . : kubectl run example -- image = busybox -- restart = Never -- sleep 1d . : kubectl get pod -o wide . Example output : : NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES example 1/1 Running 0 8s 10.244.0.230 minikube < none > < none > .	0	0
0 0 1.0 1.0 1.4 2.0 1.4 2.0 0.6666666666666666 0.0 1.1951219512195121 41 1.0 1 6 10 32	Allow specific version of container run time in minikube . it would be nice that we allow minikube users create a cluster with exact specific version of container run time they desire . such as Docker version 18 ... 19 or container run time . this would be a good opportunity to decouple installation logic of container run time tools from minikube . and maybe use a light config management system ! this would mean minikube would be shipped with a default tested version but also allow users to specify their own desired version	2	1
2 2 1.0 1.0 1.0 1.0 1.2 1.0 1.3333333333333333 2.0 0.9333333333333333 30 1.0 1 1 5 29	Synchronize VM clocks at start time . Sometimes VM's may run into issues where their clocks fall behind the times . We should detect and fix when possible , and error when impossible . See #4263 for context .	0	1
0 0 1.2 1.0 1.2 1.0 1.25 1.0 1.0 1.0 1.1481481481481481 81 1.0 2 4 12 29	Podman driver doesn't remove volume or network on delete . This makes the next create attempt fail : : 棣冦亞 StartHost failed , but will try again : creating host : create : creating : setting up container node : creating volume for minikube container : sudo -n podman volume create minikube -- label name.minikube.sigs.k8s.io=minikube -- label created_by . minikube.sigs.k8s.io=true : exit status 125 stdout : stderr : Error : volume with name minikube already exists : volume already exists 棣冩敡 Restarting existing podman container for ' minikube ' ... 棣冩▼ Failed to start podman container . Running ' minikube delete ' may fix it : podman inspect ip minikube : sudo -n podman container inspect -f {{ . NetworkSettings . IPAddress }} minikube : exit status 125 stdout : stderr : Error : error inspecting object : no such container minikube .	0	0
0 0 1.2 2.0 1.1 1.5 1.15 2.0 1.3333333333333333 2.0 0.0 0 0.0 1 1 4 24	minikube cache add looking for remote images not local ? . the following is the ouput after the command : minikube cache add php : api . E0728 22:02:58 . 007461 352660 cache . go : 63 ] save image to file ' php : api ' -> ' /home/artem/ . minikube/cache/images/php_api ' failed : nil image for php : api : GET < URL > MANIFEST_UNKNOWN : manifest unknown ; map[Tag : api ] 棣冩寴 Failed to cache and load images : save to dir : caching images : caching image ' /home/artem/ . minikube/cache/images/php_api ' : nil image for php : api : GET < URL > MANIFEST_UNKNOWN : manifest unknown ; map[Tag : api ] : minikube cache add docker.io-image:tag . adds no problem any remote image , doesn't it contradict the description of : minikube cache -- help . where it says ' Add , delete , or push a local image into minikube ' it used to work fine couple of weeks ago and I could push local images to minikube node , seems like not any more ? could someone help to clarify on this please ? Best regards , Artem .	0	0
2 2 1.6 2.0 1.5 2.0 1.5 2.0 2.0 2.0 0.0 0 0.0 0 0 0 2	minikube not added to PATH : installer uses setx . exe which runcates PATHs longer than 1024 chars . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.25.0 - OS ( e.g. from /etc/os-release ): Windows 10 Enterprise - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): hyperv - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): minikube-v 0.25.1 . iso - Install tools : Windows Installer - Others : The above can be generated in one go with the following commands ( can be copied and pasted directly into your terminal ): : minikube version echo ' ; echo ' OS:'; cat /etc/os-release echo ' ; echo ' VM driver ' : grep DriverName ~ / . minikube/machines/minikube/config . json echo ' ; echo ' ISO version ' ; grep -i ISO ~ / . minikube/machines/minikube/config . json . What happened : Installed minikube using the Windows Installer . The update_path . bat script used setx . exe to add minikube's installation path to my system PATH variable . My system PATH is over 1024 chars long so setx . exe destructively truncated it . I had to restore my system PATH from registry backups . What you expected to happen : minikube is installed without destructively changing my system PATH . How to reproduce it ( as minimally and precisely as possible ): Start with a system PATH environment variable > 1024 chars . Run the minikube Windows installer . Output of : minikube logs . ( if applicable ) : N/A Anything else do we need to know : N/A  	1	0
0 0 1.2 2.0 0.7 0.0 1.1 1.0 0.6666666666666666 0.0 0.848780487804878 205 1.0 2 3 16 55	minikube v 1.18.0 version dirty . this is the script that runs the release < URL > : $ minikube version minikube version : v 1.18.0 commit : ec61815d60f66a6e4f6353030a40b12362557caa-dirty .	0	0
2 2 1.6 2.0 1.4 2.0 1.45 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 6 13	start should support -- mount-string argument . FEATURE REQUEST Environment : Minikube version : v 0.30.0 - OS : MacOS Darwin Kernel Version 18.2.0 - VM Driver : hyperkit - ISO version : minikube-v 0.30.0 . iso We have a well scripted process for spinning up our stack , currently it uses NFS shares . We are looking to refactor this to use the : minikube start -- mount -- mount-string ' /blah : /blah ' . We are struggling to get write access to the mount-point via this method . We have tested with : minikube mount . with the : -- 9p-version = 9p2000 . L . and this gives us write access . During : minikube start . we would like to make use of the : -- 9p-version . flag as per the : minikube mount . . seems some similar discussion was taking place on this issue : #2290	2	1
0 0 1.4 2.0 1.4 2.0 1.3 2.0 1.0 1.0 0.0 0 0.0 0 0 0 2	Feature Request - add profile listing command . FEATURE REQUEST : Minikube version : : v 0.20.0 . : minikube . doesn't tracks profiles right now . It would be great to track the existing profiles and be able to list them . Maybe the : profile . command can be changed to have : ls . and : set . subcommands to list and set current profile . I felt the need for a list of profiles when I came across #1683 . While deleting machine , profile/machine name could be passed . From my observation , machine name and profile names are the same . Ability to list the existing profiles would be helpful in this case .  	1	1
2 2 2.0 2.0 1.7 2.0 1.4 2.0 2.0 2.0 1.0076923076923077 130 1.0 0 2 4 28	Update NewestKubernetesVersion to v 1.19.0 -rc .	0	1
0 0 1.2 2.0 1.0 1.0 0.9 1.0 1.3333333333333333 2.0 0.9683544303797469 158 1.0 1 2 10 28	performance bot wait for binary before run . as seen in comment , performance bot is running and erroring < URL > : kvm2 Driver error collecting results for kvm2 driver : timing run 0 with Minikube ( PR 9693 ): timing cmd : [/home/performance-monitor/ . minikube/minikube-binaries/9693/minikube start -- driver = kvm2 ]: starting cmd : fork/exec /home/performance-monitor/ . minikube/minikube-binaries/9693/minikube : exec format error docker Driver error collecting results for docker driver : timing run 0 with Minikube ( PR 9693 ): timing cmd : [/home/performance-monitor/ . minikube/minikube-binaries/9693/minikube start -- driver = docker ]: starting cmd : fork/exec /home/performance-monitor/ . minikube/minikube-binaries/9693/minikube : exec format error . need to add logic to retry the test or wait till binary is available	0	0
0 0 0.4 0.0 0.3 0.0 0.65 0.0 0.6666666666666666 0.0 0.0 0 0.0 1 4 8 37	hyperkit : minikube VM time not in sync after host sleep . problem After the mac sleeps , the minikube time will stop running . After the mac is restored , the minikube time will be different from the mac time . This difference will cause the pods in the minikube that need time-dependent to not work properly . : fengxiangdeMacBook-Pro% date Sat Apr 6 20:28:56 CST 2019 fengxiangdeMacBook-Pro% minikube ssh -- date Fri Apr 5 21:56:46 CST 2019 . system version : : fengxiangdeMacBook-Pro% sw_vers ProductName : Mac OS X ProductVersion : 10.14.4 BuildVersion : 18E226 fengxiangdeMacBook-Pro% minikube version minikube version : v 0.35.0 fengxiangdeMacBook-Pro% kubectl version Client Version : version . Info{Major:' 1 ' , Minor:' 13 ' , GitVersion:' v 1.13.1 ' , GitCommit:' eec55b9ba98609a46fee712359c7b5b365bdd920 ' , GitTreeState:' clean ' , BuildDate:' 2018-12-13T 19:44:19 Z ' , GoVersion:' go 1.11.2 ' , Compiler:' gc ' , Platform:' darwin/amd64 ' } Server Version : version . Info{Major:' 1 ' , Minor:' 13 ' , GitVersion:' v 1.13.4 ' , GitCommit:' c27b913fddd1a6c480c229191a087698aa92f0b1 ' , GitTreeState:' clean ' , BuildDate:' 2019-02-28T 13:30:26 Z ' , GoVersion:' go 1.11.5 ' , Compiler:' gc ' , Platform:' linux/amd64 ' } . minikube vm-driver hyperkit	2	0
2 2 0.8 1.0 1.2 1.5 1.1 1.0 1.0 1.0 1.0333333333333334 120 1.0 3 7 15 47	add a new flag stop -- all . if you have multiple clusters and wanna stop all of them you should be able to do by minikube stop -- all  	1	1
2 2 0.8 0.0 1.1 1.5 1.05 1.0 1.3333333333333333 2.0 0.48 25 0.0 1 6 18 43	Add `minikube image load` command . This will load an image into the correct runtime but won't store it in the cache .	0	1
2 2 1.2 2.0 1.3 2.0 1.25 1.5 1.3333333333333333 2.0 2.0 1 2.0 0 0 2 9	version : consider moving to 1.0 . minikube has been a recommended stable tool for many releases of Kubernetes . Is there any reason to not start making 1.0 + releases ? Maybe even tracking Kubernetes releases ? If not what are the milestones for doing that ?	0	1
2 2 0.8 0.0 1.1 1.0 1.15 1.0 0.6666666666666666 0.0 0.0 0 0.0 2 4 7 25	How to mount host directory with executable permissions . : minikube mount -- ip 192.168.99.1 c :/ /host//directory : /home/minikube/vm/directory . : * Mount options : - Type : 9p - UID : docker - GID : docker - Version : 9p2000 . L - MSize : 262144 - Mode : 755 (-rwxr-xr-x ) - Options : map [] * Userspace file server : ufs starting * Successfully mounted c :/ /host//directory to /home/minikube/vm/directory * NOTE : This process must stay alive for the mount to be accessible ... . OS : Windows 10 minikube version : v 1.2.0 VM-Driver : virtualbox Unfortunately all the files mounted are not executable : -rw-rw-rw- 1 docker docker 4128 Jul 23 05:52 Dockerfile . When mounted in a pod : : containers : - name : nginx image : nginx-image ports : - containerPort : 80 volumeMounts : - name : hostdir mountPath : /app - name : fpm image : php-image args : - php-fpm ports : - containerPort : 9000 volumeMounts : - name : hostdir mountPath : /app volumes : - name : hostdir hostPath : path : /home/minikube/vm/directory . None of the files are executable . How to mount host directory allowing minikube and pods to execute files ?  	1	0
1 1 1.4 1.0 1.5 1.5 1.2 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 1 3 36	Unable to apply storage-provisioner . yaml due to yaml to JSON conversion error . . I tried to apply the storage-provisioner . yaml file but there is error related to YAML to JSON Conversion . : kubectl apply -f < URL > serviceaccount/storage-provisioner unchanged clusterrolebinding.rbac.authorization.k8s.io/storage-provisioner unchanged error : error parsing < URL > error converting YAML to JSON : yaml : line 13 : did not find expected key . It seems issue is with the ' Image ' key : : image : {{ default ' gcr.io/k8s-minikube ' . ImageRepository}}/storage-provisioner : v 1.8.1 . I was able to apply after changing the double quotes as below . : image : ' {{ default gcr.io/k8s-minikube . ImageRepository}}/storage-provisioner : v 1.8.1 ' . But i am not sure if it will produce the same result . I am new to this so any help will be appreciated . OS : Ubuntu 18.04.1 I was able apply by changing the image key as : image : ' gcr.io/k8s-minikube/storage-provisioner:v1.8.1 ' .	2	2
1 1 1.2 1.0 1.2 1.0 0.9 1.0 1.0 1.0 1.1343283582089552 67 1.0 1 4 10 56	Preload containerd images . implement preload for containerd for overlay2 . by generating a tar ball with all images for a kubernetes version , similar to our docker runtime .  	1	1
0 0 1.0 1.0 1.1 1.0 1.4 2.0 0.3333333333333333 0.0 1.196078431372549 51 1.0 0 3 16 48	Race condition between dockerd and containerd . Currently the VM startup procedure relies on Docker starting an internal version of containerd . But depending on timing , dockerd can choose to start using the systemd version instead ... See < URL > This leads to the docker daemon not responding , when minikube shuts down system containerd . We should probably do like the ubuntu packaging , and have docker . service use containerd . service : BindsTo = containerd . service . and : dockerd -- containerd = /run/containerd/containerd . sock .	0	0
0 0 0.8 0.0 1.2 2.0 1.05 1.0 0.6666666666666666 0.0 0.9782608695652174 46 1.0 0 1 5 28	Document how to run minikube in China . Crazy workarounds and all . Known issues : gcr.io isn't available kubectl can't be downloaded ISO can't be downloaded  	2	2
2 2 1.4 2.0 1.5 2.0 1.35 2.0 1.6666666666666667 2.0 1.2413793103448276 29 1.0 0 3 4 18	  Add more upstream documentation for kubeadm . Many people try to use the : none . driver , without first doing proper host preparations , and fail . We should probably refer to upstream documentation , as well as try to warn for common errors . < URL > Common problems : - not installing docker ( or other container runtime ) - unsupported graph driver - enabling swap - not disabling selinux - cgroup driver - old kernel / missing module - no memory cgroups - no iptables legacy - net . bridge . bridge-nf-call-iptables = 0 - missing br_netfilter module - cgroups v2   	1	2
0 0 1.0 1.0 1.1 1.0 1.0 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 2 8 53	Add Progress bar : Transferring cached image . I've recently started making use of the minikube image cache . When starting minikube , the startup stalls for a very long time at the following : : 棣冩 minikube v 1.8.2 on Linuxmint 19.3 閴?Using the kvm2 driver based on user configuration 棣冩暉 Creating kvm2 VM ( CPUs = 8 , Memory = 18432MB , Disk = 40000MB ) ... 棣冩儞 Preparing Kubernetes v 1.17.3 on Docker 19.03.6 ... 棣冩畬 Launching Kubernetes ... 棣冨皞 Enabling addons : default-storageclass , storage-provisioner . The first few times I encountered this , I though something had broken . It would be helpful to indicate to the user that progress is being made .	2	1
2 2 1.2 1.0 1.1 1.0 1.3 1.0 1.3333333333333333 1.0 1.0980392156862746 102 1.0 3 11 23 76	make verification UI beautiful . on the node pressure PR I made I did not want to finish working on the UI . I left it for a following up PR for someone with a better UI mindset . I know @tstromberg have been working on prototypoing new UI for minikube . so I assign it to him . : 棣冩 minikube v 1.9.2 on Ubuntu 20.04 閴?Using the docker driver based on user configuration 棣冩啢 Kubernetes 1.18.0 is now available . If you would like to upgrade , specify : -- kubernetes-version = 1.18.0 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 3900MB ) ... 棣冩憹 Preparing Kubernetes v 1.17.5 on containerd 1.3.3 -14-g449e9269 ... 閳?kubeadm . pod-network-cidr = 10.244.0.0 /16 棣冩殼閿?Verifying Kubernetes Components : 棣冩敺 verifying node conditions ... 棣冩敺 verifying api server ... 棣冩敺 verifying system pods ... 棣冨皞 Enabled addons : default-storageclass , storage-provisioner 棣冨及 Done ! kubectl is now configured to use ' minikube ' .	0	1
2 2 1.6 2.0 1.4 2.0 1.25 2.0 1.3333333333333333 2.0 0.9696969696969697 99 1.0 4 11 21 72	LoadBalancer : /etc/resolver/cluster . local created with wrong permissions . On macOS : : . /out/minikube start -- driver = hyperkit . : . /out/minikube tunnel . : ls -la /etc/resolver/cluster . local . : -rw ------- 1 root wheel 37 Apr 17 09:42 /etc/resolver/cluster . local . Users must be able to read this file for DNS resolution to function . NOTE : This may be possibly be dependent on my umask : : % umask 0022 .	0	0
2 2 1.0 1.0 1.2 1.5 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 2 9 43	k8s.io/minikube-hostpath : support for volumeMode : block . Steps to reproduce the issue : Create a minikube cluster v 1.8.2 using : minikube start -- vm-driver = virtualbox . Ssh in the vm and pull gcr.io/k8s-minikube/storage-provisioner:latest Update the storage-provisioner pod to use the latest image Modify RBAC as described < URL > Create the following PVC : : spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi storageClassName : standard volumeMode : Block . Full output of failed command : PVC remains pending and the provisioner gives the following logs : : I0528 09:44:51 . 316352 1 controller . go : 1196 ] provision ' default/set1-data-0-2tgr4 ' class ' standard ' : started E0528 09:44:51 . 332559 1 controller . go : 1220 ] provision ' default/set1-data-0-2tgr4 ' class ' standard ' : failed to provision volume : k8s.io/minikube-hostpath does not support block volume provisioning I0528 09:44:51 . 332919 1 event . go : 281 ] Event(v1 . ObjectReference{Kind:' PersistentVolumeClaim ' , Namespace:' default ' , Name:' set1-data-0-2tgr4 ' , UID:' 7e0b4207-f26b-4584-8251-7395095aac37 ' , APIVersion:' v1 ' , ResourceVersion:' 9186 ' , FieldPath:'}): type : ' Warning ' reason : ' ProvisioningFailed ' k8s.io/minikube-hostpath does not support block volume provisioning . Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command :	2	1
2 2 1.8 2.0 1.7 2.0 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 3 11 18 42	registry-creds : Add support for Google Artifact Registry . I'd love to have < URL > included in the < URL > addon . Authentication generally works the same way as GCR : < URL > I'm not sure how registry-creds works or how one might go about adding this . If it's not too difficult , I may be able to contribute given some pointers/guidance .  	1	1
1 1 1.6 2.0 1.5 2.0 1.45 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 1 2	Add cluster DNS to node resolv . conf ( cannot pull image from cluster-internal host name ) . Minikube version ( use : minikube version . ): v 0.23.0 - OS ( e.g. from /etc/os-release ): : Linux xps 4.9.58 #1 -NixOS SMP Sat Oct 21 15:21:39 UTC 2017 x86_64 GNU/Linux . - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): virtualbox - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): minikube-v 0.23.6 . iso What happened : : Failed to pull image ' docker-registry-luminous-parrot : 4000/todo-list @sha256 : c3fb64353659cad2e6e96af7b6d5e3e58340af74108a3e2b663f6df77debd872 ' : rpc error : code = Unknown desc = Error response from daemon : Get < URL > dial tcp : lookup docker-registry-luminous-parrot on 10.0.2.3 : 53 : no such host . even though this service is available : when I ssh into a pod in the same namespace : : # nslookup docker-registry-luminous-parrotServer : 10.0.0.10 Address : 10.0.0.10 #53 Name : docker-registry-luminous-parrot . default . svc . cluster . local Address : 10.0.0.178 . when I read /etc/resolv . conf from minikube : : $ minikube ssh $ cat /etc/resolv . conf nameserver 10.0.2.3 . It looks like minikube has the wrong dns server . : 10.0.0.10 . finds the service correctly . What you expected to happen : I expect kubernetes to be able to pull the image based on that registry host name . How to reproduce it ( as minimally and precisely as possible ): : minikube start -- insecure-registry 10.0.0.0 /24 -- disk-size 60g helm init helm install incubator/docker-registry # push an image to the registry # try to create a deployment with the image using the registry .	2	1
1 1 0.6 1.0 0.7 0.5 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 2 3 6 26	Creation of additional storage classes with dynamic volume provisioning : impossible or documentation lacking ? . Running minikube version : v 1.20.0 , commit : c61663e942ec43b20e8e70839dcca52e44cd85ae There is only one storage class created when starting a minikube : : $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard ( default ) k8s.io/minikube-hostpath Delete Immediate false 18h . Dynamic Provisioning in this storage class works fine . I would like to add more storage classes with dynamic volume provisioning , so that I could test my PersistentVolumeClaim manifests ( written for EKS ) on Minikube without modifications . These storage classes need not be different physically or programmatically from the standard Minikube sc , but I need to test manifests for other clusters without modifications . Those manifests have different ' storageClassName ' attributes , like ' gp3 ' , ' sc1 ' etc . If the creation of additional storage classes with dynamic volume provisioning is possible , I cannot find it anywhere if it's documented for Minikube .	2	1
1 1 0.8 1.0 1.2 1.0 1.2 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 0 1 11	Unable to locate files in cache for offline use . . I am trying to use minikube among a network of computers in an offline setting . When I go the the : ~ / . minikube/cache . directory , from an online computer already running minikube , the contents do not match up with the offline guide posted within the documentation . Not sure how to transfer to offline as the tutorial does not match < URL > . I am running Windows 10 OS . : PS C : /Users/me/ > minikube version minikube version : v 1.22.0 commit : a03fbcf166e6f74ef224d4a63be4277d017bb62e . Steps to reproduce the issue : : minikube start -- download-only = True . Inspect : ~ / . minikube/cache . directory - sub directories include : kic , preloaded-tarball , windows . These directories do not have the matching folders as the offline tutorial Sadness Any help would be greatly appreciated .  	2	2
1 1 0.8 1.0 1.1 1.0 1.3 2.0 1.0 1.0 1.0462962962962963 108 1.0 2 6 17 79	Tunnel : needs clean after delete router : conflicting route : 10.96.0.0 /12 via 172.17.0.3 dev docker0 . after deleting minikube , and starting it again and deploying my service I saw : : ~ $ minikube tunnel Status : machine : minikube pid : 764782 route : 10.96.0.0 /12 -> 172.17.0.2 minikube : Running services : [] errors : minikube : no errors router : conflicting route : 10.96.0.0 /12 via 172.17.0.3 dev docker0 loadbalancer emulator : no errors Status : machine : minikube pid : 764782 route : 10.96.0.0 /12 -> 172.17.0.2 minikube : Running services : [] errors : minikube : no errors router : conflicting route : 10.96.0.0 /12 via 172.17.0.3 dev docker0 loadbalancer emulator : no errors . I was able to fix it by running : minikube tunnel -c . minikube should do an auto-clean up , so the user doesnt have to do a clean up after each minikube run or delete .  	1	0
0 0 1.4 2.0 1.4 2.0 1.5 2.0 1.3333333333333333 2.0 0.8571428571428571 203 1.0 0 1 15 53	gcp auth avoid mounting empty credentials .	0	0
0 0 0.2 0.0 0.8 1.0 1.15 1.0 0.3333333333333333 0.0 1.0263157894736843 76 1.0 1 1 3 24	delete with kic : stuck on ' docker inspect -f {{ . State . Status }} minikube ' . : . /out/minikube delete -- alsologtostderr -v =3 . It doesn't show any output , but I can see it's blocked on inspect : : 19267 7810 6099 0 7:57 AM ttys001 0:00 . 06 . /out/minikube delete -- alsologtostderr -v =3 19267 7811 7810 0 7:57 AM ttys001 0:00 . 06 docker inspect -f {{ . State . Status }} minikube . Related to #6305	0	0
0 0 0.8 1.0 1.0 1.0 0.9 1.0 0.6666666666666666 0.0 1.0 47 1.0 1 2 6 29	Choose smarter defaults for -- memory . While working with @priyawadhwa on a demo , I noticed that minikube was locking up . Increasing the memory size fixed the problem . Also , the first thing our documentation states after installation is to increase the memory size . What if we were able to improve the first-start experience for most users by default , by dynamically selecting a more appropriate option for : -- memory . ? My recommendation is to change the default 2GB setting to 37.5% of available memory by default : but never less than 2.25 GB or more than 8GB . For instance : 4GB host -> 2.25 GB VM 6GB host -> 2.25 GB VM 8GB host -> 3GB VM 16GB host -> 6GB VM 32GB host -> 8GB VM 64GB host -> 8GB VM The balance can be fine tuned , but my main argument is that a developer on a 16GB machine can probably live with a 6GB VM .	0	1
2 2 0.4 0.0 0.4 0.0 0.65 0.0 0.6666666666666666 0.0 1.1066666666666667 75 1.0 4 5 12 54	minikube not respecting vm-driver flag if existing profile . this is on 1.8.2 I am not sure if this has been fixed on HEAD but I create a bug so I dont forget about it : medya@ ~ $ minikube start -- vm-driver = docker 棣冩 minikube v 1.8.2 on Darwin 10.15.3 閴?Using the hyperkit driver based on existing profile 棣冩崙 Downloading driver docker-machine-driver-hyperkit : > docker-machine-driver-hyperkit . sha256 : 65 B / 65 B [ --- ] 100.00% ? p/s 0s > docker-machine-driver-hyperkit : 6.28 MiB / 10.90 MiB 57.61% 10.26 MiB p/^C .	0	0
1 1 1.4 2.0 1.2 1.5 1.25 1.5 1.6666666666666667 2.0 0.36363636363636365 11 0.0 1 4 6 22	Try running minikube on Windows 11 beta . Windows 11 is coming out later this year , we should check if minikube works on the beta .	0	1
1 1 1.4 2.0 1.2 1.5 1.4 2.0 1.6666666666666667 2.0 1.0921052631578947 76 1.0 0 4 11 45	Define imaginary user character for minikube website . How about we define a few characters with names that who are the ppl the we create minikube for ? So when we redesign our document website we find a real user from that audience to try the website and tell us their frictions . Here is a quick example . User A Wants to try kubernetes for first time and wants to install minikube User B A software developer who wants to use minikube for local development and wants learn how to deploy their app to minikube . ( Build image , push image and deploy to minikube and then hit the url of their app ) User C A software engineer working at a corp with vpn and proxy and corp certs . They wanna use minikube on corp machines . Wants to figure out how to add corp certs to minikube . User D: Wants to explore and try different things that minikube is capable of . And try different features with examples User E : Wants to troubleshoot their not working minikuke . User F : Wants to contribute to minikube and wants to learn how to build or add something to minikube . User G : Wants to create an addon for minikube . So their cool service be easily installed on minikube for other users  	2	2
0 0 0.6 0.0 0.8 0.5 0.7 0.5 0.0 0.0 0.8490566037735849 212 1.0 0 0 4 52	minikube status : show timeToStop only if it exists . $ minikube status minikube type : Control Plane host : Running kubelet : Running apiserver : Running kubeconfig : Configured timeToStop : Nonexistent	0	1
0 0 1.2 2.0 0.9 0.5 1.0 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 2 14 34	Make MAC address settable . I use the : kvm2 . driver on debian linux but when I create a new minikube instance I do not get the ability to specify the IP or macaddress for the : -- kvm-network . interface . This is annoying because I need to take the extra step of : minikube ssh . and check the IP address of : eth0 . . It would be nice to be able to supply a static address of some sort so I can have a DHCP reservation or a well-known address . One other option would be to offer the IP address of : eth0 . as an option for : minikube ip . . Thanks for minikube , its very nice for small deployments who do not want to fight the battle of the entire stack .	2	1
2 2 1.6 2.0 1.3 1.5 1.15 1.0 1.6666666666666667 2.0 0.0 0 0.0 3 12 16 49	 minikube-linux-ppc64le release binary . Currently < URL > does not offer ppc64le binary . Can we add this platform into the release pipeline too ?  	1	1
1 1 1.6 2.0 1.3 1.5 1.05 1.0 1.3333333333333333 1.0 0.8356164383561644 219 1.0 1 4 9 32	minikube logs output takes a long long time . even when there is no minikube running : $ minikube profile list 棣冦仚 Exiting due to MK_USAGE_NO_PROFILE : No minikube profile was found . 棣冩寱 Suggestion : You can create one using ' minikube start ' . . on my mac it is taking more than a few minutes and sitll running : real 5m 31.022 s user 0m 4.015 s sys 0m 5.576 s .	0	0
2 2 1.4 2.0 1.4 2.0 1.35 2.0 1.0 1.0 0.0 0 0.0 2 7 18 79	The image source from the Hello Minicube tutorial doesn't work . Steps to reproduce the issue : Launch the shell from < URL > tutorial : kubectl create deployment hello-node -- image = gcr.io/hello-minikube-zero-install/hello-node . : kubectl expose deployment hello-node -- type = LoadBalancer -- port = 8080 . : minikube service hello-node . Full output of failed command : : | -----------|------------ | -------------|-------------------------- | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|------------ | -------------|-------------------------- | | default | hello-node | | < URL > | | -----------|------------ | -------------|-------------------------- | Opening service default/hello-node in default browser ... Minikube Dashboard is not supported via the interactive terminal experience . Please click the ' Preview Port 30000 ' link above to access the dashboard . This will now exit . Please continue with the rest of the tutorial . X open url failed : < URL > exit status 1 minikube is exiting due to an error . If the above message is not useful , open an issue : - < URL > . Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : I think the issue comes from revoked permissions to an image gcr.io/hello-minikube-zero-install/hello-node .	0	0
2 2 1.4 2.0 1.3 1.5 1.35 1.5 2.0 2.0 2.0 1 2.0 0 0 3 25	Site : fix casing for tutorials . > the sidebar linkf or telemetry , Using custom TLS certificate with ingress addon etc are not following camel casing which is used throughout the site for links and tags .  	2	2
0 0 1.2 2.0 1.1 1.5 0.95 1.0 1.3333333333333333 2.0 1.1481481481481481 54 1.0 1 4 6 46	Unclear how the container images are updated . The process of updating the minikube . iso is somewhat clear , from the Makefile etc . But for the ' kicbase ' and the ' storage-provisioner ' , things are not so straight-forward They seem to only be building snapshot and latest , but those don't go into release So it would be good to document , how to make changes to the the container images .    	1	2
0 0 1.0 1.0 0.9 1.0 0.85 1.0 0.3333333333333333 0.0 1.096774193548387 93 1.0 1 5 19 64	updated screenshot on minikube docs . the current screenshot does not have the improvements @afbjorklund made . < URL >  	2	2
2 2 0.6 0.0 0.9 0.5 1.2 1.5 0.6666666666666666 0.0 2.0 1 2.0 2 6 9 36	Cannot change apiserver-ips after initial minikube start . Steps to reproduce the issue : : minikube start -- vm-driver = none -- apiserver-ips = 1.1.1.1 ... . : minikube stop . : minikube start -- vm-driver = none -- apiserver-ips = 2.2.2.2 ... . The apiserver certificate has still the initial address in its SAN : : openssl x509 -in /var/lib/minikube/apiserver . crt -text .... X509v3 extensions : ... X509v3 Subject Alternative Name : ... IP Address : 1.1.1.1 , .... . It seems the behavior changed in v 1.10 with < URL > While it is possible to modify : apiserver-names . ( that had also issues but fixed recently with < URL > I cannot change : apiserver-ips . . What was the rationale behind this change ? Is this by design ? Is there any way to work around this regression ? Maybe related to < URL >   	1	0
2 2 0.8 0.0 1.2 1.5 1.4 2.0 1.3333333333333333 2.0 1.144736842105263 76 1.0 0 3 6 22	Add architecture to the binaries and images in the cache . When supporting remote clusters of a different architecture , we need to add the arch in addition to the OS : : ~ / . minikube/cache/linux/v 1.19.3 閳规壕鏀㈤埞鈧?kubeadm 閳规壕鏀㈤埞鈧?kubectl 閳规柡鏀㈤埞鈧?kubelet . Simplest would be to add it after the OS , but we could support both in order to preserve existing user caches ? : ~ / . minikube/cache/linux 閳规壕鏀㈤埞鈧?amd64 閳?閳规柡鏀㈤埞鈧?v 1.19.3 閳?閳规壕鏀㈤埞鈧?kubeadm 閳?閳规壕鏀㈤埞鈧?kubectl 閳?閳规柡鏀㈤埞鈧?kubelet 閳规柡鏀㈤埞鈧?arm64 閳规柡鏀㈤埞鈧?v 1.19.3 閳规壕鏀㈤埞鈧?kubeadm 閳规壕鏀㈤埞鈧?kubectl 閳规柡鏀㈤埞鈧?kubelet . For instance when running an : amd64 . host , towards a : arm64 . machine . Either local hardware or cloud instance . Raspberry Pi AWS Graviton Same thing for the images , if not using the preload ( the preloaded-tarball files already include the arch , though ) : ~ / . minikube/cache/images 閳规壕鏀㈤埞鈧?gcr.io 閳?閳规柡鏀㈤埞鈧?k8s-minikube 閳?閳规柡鏀㈤埞鈧?storage-provisioner_v3 閳规壕鏀㈤埞鈧?k8s.gcr.io 閳?閳规壕鏀㈤埞鈧?coredns_ 1.7.0 閳?閳规壕鏀㈤埞鈧?etcd_ 3.4.13 -0 閳?閳规壕鏀㈤埞鈧?kube-apiserver_v 1.19.2 閳?閳规壕鏀㈤埞鈧?kube-controller-manager_v 1.19.2 閳?閳规壕鏀㈤埞鈧?kube-proxy_v 1.19.2 閳?閳规壕鏀㈤埞鈧?kube-scheduler_v 1.19.2 閳?閳规柡鏀㈤埞鈧?pause_ 3.2 閳规柡鏀㈤埞鈧?kubernetesui 閳规壕鏀㈤埞鈧?dashboard_v 2.0.3 閳规柡鏀㈤埞鈧?metrics-scraper_v 1.0.4 . : ~ / . minikube/cache/images/amd64 ~ / . minikube/cache/images/arm64 .	2	1
2 2 1.2 2.0 1.1 1.5 1.2 2.0 2.0 2.0 0.0 0 0.0 0 0 0 13	  Document uninstall procedures . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Feature Request There doesn't seem to be documentation on how to properly uninstall minikube . #1043 has a bunch of different sets of commands , depending on what OS and shell you're using . I ran into the problem of an incompatibility between a year-old version of minikube and current ( v30 ) . It took a long time to figure out how to clear out all of minikube's old state , before finding I needed to delete ~ / . minikube ( linux ) .	0	2
2 2 2.0 2.0 1.6 2.0 1.15 1.0 2.0 2.0 2.0 1 2.0 2 4 12 48	Add overwrite flag into start command to change values in config files . . Currently it looks we can overwrite flags in config file only when first start fail . < URL > How about add : -- overwrite . flag to intentionally update value ? Then we can solve some problem occurred by machine or nodes internal change ( #10578 )   	1	1
1 1 1.4 2.0 1.2 1.5 1.15 1.0 1.0 1.0 0.0 0 0.0 4 14 19 50	  Document that minikube does not support IPv6 . It would be great if there were instructions for running minikube and kubernetes in IPv6 only mode . Now it could be that that's not even possible from my tests as with the virtualbox driver at least ( which is what I tested ) , the master node has IPv6 disabled in the sysctls ( net . ipv6 . conf . all . disable_ipv6 is set to 1 by default ) - which I wouldn't expect to be default kubernetes behaviour . Also , minikube only has one form of the host-only-cidr config item , which is IPv4 only ( always in vbox driver context ) , as it tries to pass the configured cidr to the ' VBoxManage hostonlyif ipconfig ' command with the ' -- ip ' flag , which is IPv4 only . For Ipv6 Vbox requires the ' -- ipv6 ' flag : VBoxManage hostonlyif Usage : VBoxManage hostonlyif ipconfig [ -- dhcp | -- ip [ -- netmask ( def : 255.255.255.0 )] | -- ipv6 [ -- netmasklengthv6 ( def : 64 )]] create | remove I've stopped trying stuff out here , and maybe I'm doing it wrong , but IF IPv6 is not supported in minikube or minikube and Vbox as driver ( or other drivers ) that should be clearly stated in the handbook .	0	2
1 1 1.6 2.0 1.5 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 3 8 16 39	efk : Discover times out , more info says : ' Error : Request Timeout after 30000ms ' . I'm trying to use EFK on minikube on OSX . Is there any kind of how-to on how this should work ? It seems all the pods come up ( in kube-system namespace ) . Then , I go to kibana-logging endpoint ( minikube-ip : 30003 in this case ) , and I need to set up an index . I do that , then go to discover , and it times out after 30 seconds ? If I expand the ' More info ' at the top , it says Error : Request Timeout after 30000ms at < URL > at < URL >	2	0
1 1 1.0 1.0 0.9 1.0 1.1 1.0 0.6666666666666666 1.0 0.0 0 0.0 0 1 2 16	minikube start -- memory flag is broken in v 1.12.3 . Steps to reproduce the issue : Run a command which sets a custom memory amount for the cluster with minikube start minikube start -- driver = virtualbox -- cpus = 6 -- memory = 15g -- disk-size = 50g Full output of failed command : 棣冩 minikube v 1.12.3 on Fedora 32 閴?Using the virtualbox driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating virtualbox VM ( CPUs = 6 , Memory = 6000MB , Disk = 51200MB ) ... 棣冩儞 Preparing Kubernetes v 1.18.3 on Docker 19.03.12 ... 棣冩敺 Verifying Kubernetes components ... 棣冨皞 Enabled addons : default-storageclass , storage-provisioner 棣冨及 Done ! kubectl is now configured to use ' minikube ' The memory flag in minikube start is ignored and defaults to 6gb . Note the Memory = 6000MB in the command output . This command worked as expected in v 1.12.0 . I recently updated to v 1.12.3 Note : I also tried -- memory='15g ' and obtained the same output .	0	0
2 2 1.4 1.0 1.1 1.0 1.15 1.0 1.6666666666666667 2.0 0.9615384615384616 104 1.0 1 1 4 52	 minikube addons open ' only works for kube-system namespace . This bug has probably always been this way , but I noticed it this morning while working on addons docs .  	1	0
2 2 1.4 2.0 1.1 1.0 1.2 1.0 2.0 2.0 1.1818181818181819 11 1.0 1 2 5 21	The none driver is showing imaginary numbers for the VM . : * Creating none VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... . These are just from the config of the VM that we did not create . It has nothing to do with the VM that we are actually running on ? We should either remove this information , or show the live data . Related implementation : #3574 : github.com/shirou/gopsutil .	0	0
1 1 0.8 1.0 1.1 1.0 0.9 1.0 1.0 1.0 0.9803921568627451 51 1.0 0 2 6 30	Add -- interactive flag to avoid future interactive prompts . We don't have any interactive prompts right now , but we will soon for cases where the user needs to use : sudo . , but does not have rights to do so without interaction . For example : Giving the hyperkit binary special permissions . I suggest allowing users to select : -- interactive = false . , to avoid the complication of weird flag combinations like : -- no-interactive = false . .  	1	1
1 1 1.4 2.0 1.3 1.5 1.15 1.0 1.6666666666666667 2.0 0.9230769230769231 26 1.0 1 2 5 25	Fetch Kubernetes images using tag @hash rather to prevent invalid downloads . Currently , if a user uses -- image-repository , they may end up with images that do not match the official Kubernetes ones . We should use digests or some other mechanism to prevent mirror poisoning .	2	1
0 0 1.2 2.0 0.9 0.5 1.0 1.0 1.3333333333333333 2.0 1.0 2 1.0 2 7 14 50	inner docker systemd service : move ' StartLimitIntervalSec ' param from [ Service ] to [ Unit ] section . repeating errors shown in logs : : Feb 07 01:57:49 functional-20210207015702-1428035 systemd[1 ]: /lib/systemd/system/docker . service : 13 : Unknown key name ' StartLimitIntervalSec ' in section ' Service ' , ignoring . . advising that the StartLimitIntervalSec param is ignored this param was used for resolving issue #9691 in pr #9775 i've been also checking it , but missed the conversation discussing the changes in systemd ( ref : < URL > : ' StartLimitInterval was moved from [ Service ] to [ Unit ] section in 6bf0f408e4833152197fb38fb10a9989c89f3a59 , but the old location was still accepted for compatibility . The new StartLimitIntervalSec name is valid only in [ Unit ] . ' . indeed : < URL > so , I'm creating a pr to move StartLimit * param from [ Service ] to [ Unit ] section and eliminate this error	0	0
2 2 1.8 2.0 1.9 2.0 1.25 1.5 1.6666666666666667 2.0 0.0 0 0.0 3 4 7 26	[ FEATURE ] insecure-registry via minikube config set ... . I would love to be able to set defaults for -- insecure-registry just like for -- vm-driver etc . ``` $ minikube config set insecure-registry foo 棣冩寴 Set failed : property name ' insecure-registry ' not found ```` ( Similar to #3805 , hopefully same simple & easy fix as #3861 ? )	2	1
0 0 1.0 1.0 1.1 1.5 1.25 2.0 1.3333333333333333 2.0 0.6666666666666666 6 0.5 3 7 17 50	Add number of nodes for cluster in `minikube profile list` . Currently if you have two clusters running and one of them has two nodes , : minikube profile list . looks like : : | ----------|----------- | ---------|------------ | ------|--------- | --------- | | Profile | VM Driver | Runtime | IP | Port | Version | Status | | ----------|----------- | ---------|------------ | ------|--------- | --------- | | minikube | docker | docker | 172.17.0.2 | 8443 | v 1.19.0 | Running | | p1 | docker | docker | 172.17.0.4 | 8443 | v 1.19.2 | Running | | ----------|----------- | ---------|------------ | ------|--------- | --------- | 閴?Found 1 invalid profile(s ) ! p1-m02 棣冩寱 You can delete them using the following command(s ): $ minikube delete -p p1-m02 . We'd like it to look like : : | ----------|----------- | ---------|------------ | ------|--------- | ---------|--------- | | Profile | VM Driver | Runtime | IP | Port | Version | Status | Nodes | | ----------|----------- | ---------|------------ | ------|--------- | ---------|--------- | | minikube | docker | docker | 172.17.0.2 | 8443 | v 1.19.0 | Running | 1 | | p1 | docker | docker | 172.17.0.4 | 8443 | v 1.19.2 | Running | 2 | | ----------|----------- | ---------|------------ | ------|--------- | ---------|--------- | . So , we want to not have the spurious error at the end of the table , and to add the Nodes column with the number of nodes the cluster has .   	1	1
1 1 1.0 1.0 1.3 1.5 1.25 2.0 0.3333333333333333 0.0 0.0 0 0.0 1 3 10 27	docker registry addon changed port missing documentation . : $ minikube addons enable registry . currently outputs : : Registry addon on with docker uses 32781 please use that instead of default 5000 For more information see : < URL > Verifying registry addon ... The ' registry ' addon is enabled . I had to work out the edit required : : $ docker run -- rm -it -- network = host alpine ash -c ' apk add socat && socat TCP-LISTEN : 5000 , reuseaddr , fork TCP :$( minikube ip ): 32781 ' . # <-- port changed at the end . This could do with a note at the suggested URL .  	2	2
1 1 1.0 1.0 1.1 1.0 1.0 1.0 1.0 1.0 1.0 43 1.0 2 2 4 25	Prototype Docker deployment model . Tracking work for initial work toward #4389	0	1
2 2 1.4 2.0 1.3 2.0 1.15 1.5 1.6666666666666667 2.0 0.0 0 0.0 5 5 18 58	windows MK_WRONG_BINARY_WSL even when not in powershell . The offending function below is incorrectly assuming that if : WSLENV . has a non-empty value , that the shell is an WSL shell . According to < URL > post , this environment variable is used for sharing variables into new WSL sessions , and is set within Windows too . < URL > Steps to reproduce the issue : Ensure the : WSLENV . environment variable has a non-empty value . Mine is : WT_SESSION :: WT_PROFILE_ID . Run : minikube . from a regular Windows : cmd . exe . shell Observe the incorrect output : > minikube docker-env 閴?Exiting due to MK_WRONG_BINARY_WSL : You are trying to run windows . exe binary inside WSL , for better integration please use Linux binary instead ( Download at < URL > Otherwise if you still want to do this , you can do it using -- force .	0	0
1 1 0.8 1.0 0.9 1.0 1.1 1.0 0.6666666666666666 1.0 0.0 0 0.0 3 4 9 59	docker fails if -- docker-opt storage-driver = overlay2 is set . Started having this problem after upgrading to 1.8.2 . It disappeared after downgrading to back to 1.7 . x . Could not find anything recent regarding this anywhere else . I've tried deleting the : ~ / . minikube . folder and reinstalling minikube via homebrew . Didn't help . Example of output ( see link to full logs below ) : W0317 10:02:52 . 019784 1773 exit . go : 101 ] Unable to start VM . Please investigate and run ' minikube delete ' if possible : creating host : create : Error creating machine : Error running provisioning : ssh command error : command : sudo systemctl -f restart docker err : Process exited with status 1 output : Job for docker . service failed because the control process exited with error code . See ' systemctl status docker . service ' and ' journalctl -xe ' for details . . The exact command to reproduce the issue : : minikube start -- v 10 -- logtostderr / -- disk-size='20G ' / -- cpus='4 ' / -- docker-opt storage-driver = overlay2 / -- extra-config = apiserver . authorization-mode='RBAC ' / -- kubernetes-version v 1.15.5 / -- memory='5120 ' / -- network-plugin = cni / -- enable-default-cni / -- vm-driver='hyperkit ' . The full output of the command that failed : See here : < URL > The operating system version : macOS 10.15.3  	1	0
2 2 0.8 0.0 1.0 1.0 1.05 1.0 0.6666666666666666 0.0 0.0 0 0.0 10 12 18 55	ssh-user is not used in native mode . Steps to reproduce the issue : minikube start -- driver = ssh -- ssh-ip-address = 192.168.122.171 -- ssh-user = centos -p kubevirt Full output of failed command : 棣冩 [ kubevirt ] minikube v 1.17.1 on Fedora 33 閴?Using the ssh driver based on existing profile 棣冩啢 Starting control plane node kubevirt in cluster kubevirt 棣冨籍 Updating the running ssh ' kubevirt ' bare metal machine ... 棣冦亞 StartHost failed , but will try again : provision : fast detect : OS type not recognized 棣冨籍 Updating the running ssh ' kubevirt ' bare metal machine ... Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : Full of logs in the target host : Feb 25 11:31:24 kubevirt sshd[65242 ]: Connection closed by authenticating user ** root ** 192.168.122.1 port 42952 [ preauth ]   	1	0
1 1 1.0 1.0 1.0 1.0 0.9 1.0 1.3333333333333333 1.0 0.8557213930348259 201 1.0 8 10 16 54	kicbase automation : include the changelog in the PR since last stable kic release . I think we can still the Git logic from the dependbot code example < URL > < URL >  	1	1
2 2 1.4 2.0 1.4 2.0 1.25 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 2 14 33	restart : waiting for k8s-app = kube-proxy : timed out waiting for the condition . When i minikube start , i get error : boby @sok -01 : ~ $ minikube start 棣冩 minikube v 0.35.0 on linux ( amd64 ) 棣冩寱 Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Restarting existing virtualbox VM for ' minikube ' ... 閳?Waiting for SSH access ... 棣冩懕 ' minikube ' IP address is 192.168.99.100 棣冩儞 Configuring Docker as the container runtime ... 閴?Preparing Kubernetes environment ... 棣冩 Pulling images required by Kubernetes v 1.13.4 ... 棣冩敡 Relaunching Kubernetes v 1.13.4 using kubeadm ... 閳?Waiting for pods : apiserver proxy棣冩寴 Error restarting cluster : wait : waiting for k8s-app = kube-proxy : timed out waiting for the condition 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > boby @sok -01 : ~ $ . I am using Ubuntu 18.04 Attached minikube logs < URL >	0	0
2 2 1.8 2.0 1.4 2.0 1.25 2.0 1.6666666666666667 2.0 0.0 1 0.0 0 1 2 27	Addon manager presumes port 8443 -- fails when used with different -- apiserver-port . If minikube is started with the -- apiserver-port directive set to a non-default value , the addon-manager pod fails to connect to the apiserver . The culprit seems to be a hardcoding of port 8443 here : < URL > The fix is a one-liner--I'll submit a pull request .	2	0
2 2 1.6 2.0 1.4 2.0 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 3 8 26	Add -- file flag to minikube logs . . We should have a -- file flag in : minikube logs . so that logs output is sent directly to a file for easy transfer . Before : : $ minikube logs < tons of output > . After : : $ minikube logs -- file out . File ' out ' written full of logs .	0	1
0 0 0.4 0.0 1.0 1.0 1.2 2.0 0.0 0.0 0.7142857142857143 7 1.0 0 5 9 60	docker : service command hangs for docker driver in Windows . The exact command to reproduce the issue : Following the docs tutorial : : minikube start -- vm-driver = docker kubectl create deployment hello-minikube -- image = k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube -- type = NodePort -- port = 8080 minikube service hello-minikube . The full output of the command that failed : : * minikube v 1.7.3 on Microsoft Windows 10 Enterprise 10.0.18362 Build 18362 * Using the docker ( experimental ) driver based on user configuration * Creating Kubernetes in docker container with ( CPUs = 2 ) ( 2 available ) , Memory = 2000MB ( 1989MB available ) ... * Preparing Kubernetes v 1.17.3 on Docker 19.03.2 ... - kubeadm . pod-network-cidr = 10.244.0.0 /16 * Launching Kubernetes ... * Enabling addons : default-storageclass , storage-provisioner * Waiting for cluster to come online ... * Done ! kubectl is now configured to use ' minikube ' deployment . apps/hello-minikube created service/hello-minikube exposed | -----------|---------------- | -------------|------------------------- | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|---------------- | -------------|------------------------- | | default | hello-minikube | | < URL > | | -----------|---------------- | -------------|------------------------- | * Opening service default/hello-minikube in default browser ... . Then the browser times out .	0	0
1 1 1.4 1.0 1.3 1.0 0.95 1.0 1.0 1.0 0.8208333333333333 240 1.0 0 0 2 12	add FAQ how to access service on physical host from inside a VM driver's container .    	1	2
1 1 1.6 2.0 1.5 2.0 1.35 2.0 1.6666666666666667 2.0 0.9818181818181818 55 1.0 2 4 8 22	Add VPN interference check : HTTP fetch before installing Kubernetes . Here's an idea to address #4302 Add a lightweight HTTP service to our VM that unobtrusively reports system stats , such as load average . As soon as we can confirm SSH access to our VM , and the VM appears to be of the correct version , try accessing this service . Port 8443 would be ideal for catching the issue , but lets say a nearby port to avoid conflicts . This would kill two birds with one stone : A lightweight way to gather system metrics without impacting VM load ( compared to ssh+uptime command ) A way to detect connectivity issues separate from Kubernetes . If it fails , we could report a very clear message , such as : : Unable to make HTTP requests to the VM ( < URL > To use minikube , please adjust your firewall or VPN to allow these connections . . We can always add : -- force . to workaround issues with it .	0	1
0 0 0.8 1.0 0.8 1.0 1.05 1.0 0.6666666666666666 0.0 1.1029411764705883 68 1.0 3 4 10 33	Deprecate the old tcp port for docker-env . Currently we are using a standalone : tcp :// . docker daemon , listening on port 2376 . : export DOCKER_TLS_VERIFY='1 ' export DOCKER_HOST='tcp://192 . 168.99.100 : 2376 ' export DOCKER_CERT_PATH='/home/anders/ . minikube/certs ' . We can use : ssh :// . and connect directly to the unix socket instead , simplifying things . This could use either the current ssh shell tunnel , or we could use the regular address ... : export DOCKER_HOST='ssh://docker@192 . 168.99.100 : 22 ' ssh-add /home/anders/ . minikube/machines/minikube/id_rsa . But we wouldn't have to manage all the extra ssl certificates for https , when using ssh . : /home/anders/ . minikube/certs 閳规壕鏀㈤埞鈧?ca-key . pem 閳规壕鏀㈤埞鈧?ca . pem 閳规壕鏀㈤埞鈧?cert . pem 閳规柡鏀㈤埞鈧?key . pem 0 directories , 4 files . And this allows for having the docker-daemon socket-activated ( on-demand ) in the future ... Requirements : Docker 18.09 or later Note that we already support both methods of connecting , so it can be a gradual change . Current config : : /usr/bin/dockerd -H tcp :/ / 0.0.0.0 : 2376 -H unix :// /var/run/docker . sock . < URL > See #9232  	1	1
2 2 2.0 2.0 1.4 2.0 1.45 2.0 2.0 2.0 1.7777777777777777 9 2.0 1 3 5 24	Delete profile should Unset the current profile . Delete profile should Unset the current profile to reproduce : 1- create a minikube with profile p1 and p2 : . /out/minikube start -p p1 -- vm-driver = hyperkit . : . /out/minikube start -p p2 -- vm-driver = hyperkit . 2- set the current profile to p1 : . /out/minikube profile p1 . 3- delete profile p1 : . /out/minikube delete -p p1 棣冩暉 Deleting ' p1 ' in hyperkit ... 棣冩寖 The ' p1 ' cluster has been deleted . . 4- the current profile is gone from profile list ( correctly ) however if u check for current profile it shows the deleted one : : medya@ ~ /workspace/ppl-minikube ( fix-glog-parse-error ) $ . /out/minikube profile list | ----------|----------- | ----------------|----------- | -------------------- | | Profile | VM Driver | NodeIP | Node Port | Kubernetes Version | | ----------|----------- | ----------------|----------- | -------------------- | | minikube | hyperkit | 192.168.64.102 | 8443 | v 1.15.0 | | p3 | hyperkit | 192.168.64.118 | 8443 | v 1.15.0 | | ----------|----------- | ----------------|----------- | -------------------- | . 5- check for current profile : $ . /out/minikube profile p1 . The fix should , after delete unset the current config .	0	0
2 2 1.6 2.0 1.6 2.0 1.55 2.0 2.0 2.0 0.0 0 0.0 0 0 1 8	hyperkit : external DNS resolution fails : conflict with DNS server running on host . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : MacOS High Sierra Minikube version ( use : minikube version . ): 0.25.0 and above - OS ( e.g. from /etc/os-release ): MacOS High Sierra , version 10.13.6 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): hyperkit - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): . minikube/cache/iso/minikube . 25.0.0 . iso - Install tools : curl - #Lo minikube < URL > && chmod +x minikube && sudo mv minikube /usr/local/bin/ - Others : What happened : dns in minikube does not work . DNS cluster can not start , because can not download images from registry . 192.168.64.1 - it is my host IP : nslookup ya.ru Server : 192.168.64.1 Address 1 : 192.168.64.1 nslookup : can't resolve ' ya.ru ' . What you expected to happen : DNS works . If I change DNS server in minikube to 8.8.8.8 , it works . : su root rm -f /etc/resolv . conf && echo nameserver 8.8.8.8 > /etc/resolv . conf nslookup ya.ru Server : 8.8.8.8 Address 1 : 8.8.8.8 google-public-dns-a.google.com Name : ya.ru Address 1 : 87.250.250.242 ya.ru Address 2 : 2a 02:6 b8 :: 2:242 ya.ru . How to reproduce it ( as minimally and precisely as possible ): : minikube stop && minikube delete minikube start -- vm-driver hyperkit . Output of : minikube logs . ( if applicable ) : Anything else do we need to know :  	1	0
2 2 1.6 2.0 1.4 2.0 1.4 2.0 2.0 2.0 0.0 0 0.0 2 4 9 44	add podman support in windows WSL . Steps to reproduce the issue : minikube start -- vm-driver = podman minikube tunnel start a loadbalancer Get an ip that maps to a similar subnet , rather than my local subnet . Results of usual commands are normal . : 閴?minikube service -n kong kong-kong-proxy | -----------|----------------- | --------------------|------------------------ | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|----------------- | --------------------|------------------------ | | kong | kong-kong-proxy | kong-proxy/80 | < URL > | | | | kong-proxy-tls/443 | < URL > | | -----------|----------------- | --------------------|------------------------ | . Returns a good result ! ( from wsl ) : curl -i -H ' Host : api . gengo . dev ' < URL > . : minikube dashboard . gives a good url that I can access on localhost . Some investigation : : sudo nc -l 80 . binds to localhost correctly and the browser can hit it . Am I missing a step ? Thanks !  	1	1
2 2 0.6 0.0 0.8 0.5 0.85 1.0 0.6666666666666666 0.0 2.0 1 2.0 2 4 10 31	xhyve suggests obsolete WantShowDriverDeprecationNotification setting . minikube v 1.0.0 I use : minikube start -- vm-driver xhyve . which tells me : 閳跨媴绗?The xhyve driver is deprecated and support for it will be removed in a future release . Please consider switching to the hyperkit driver , which is intended to replace the xhyve driver . See < URL > for more information . To disable this message , run [ minikube config set WantShowDriverDeprecationNotification false ] . but : $ minikube config set WantShowDriverDeprecationNotification false 棣冩寴 Set failed : Property name WantShowDriverDeprecationNotification not found .	2	0
0 0 1.6 2.0 1.7 2.0 1.4 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 3 14 32	Changing service cidr range breaks DNS resolution . Our company intranet uses large parts of the 10.0.0.1 /8 subnet . Alas , the default service CIDR range used by minikube ( resp . kubeadm ? ) is 10.96.0 . * . To avoid problems , I wanted to change pod and service CIDR range with these two options passed to minikube start : -- service-cluster-ip-range='172 . 20.0.0 /14 ' -- extra-config = kubelet . pod-cidr = 172.16.0.0 /14 The VM seemed to startup fine , but after deploying our application , I noticed that the DNS names of services could not be resolved by our pods , e.g. : nslookup our-service nslookup : can't resolve ' ( null )': Name does not resolve . I checked /etc/resolv . conf inside the pod , and indeed , it still pointed to the default IP of the kubedns service , i.e. , 10.96.0.10 . I would expect that changing the service CIDR range would also take care of that . Looking for a workaround , I tried using extra-config also for the service-cidr range , i.e. , : -- extra-config = kubelet . pod-cidr = 172.16.0.0 /14 -- extra-config = kubelet . service-cidr = 172.20.0.0 /14 . Now the minikube start would fail cause kubelet wouldn't start : : [ ... ] Oct 25 11:40:15 minikube kubelet[24917 ]: F1025 11:40:15 . 308498 24917 server . go : 156 ] unknown flag : -- service-cidr Oct 25 11:40:15 minikube systemd[1 ]: kubelet . service : Main process exited , code = exited , status = 255/n/a [ ... ] . .	2	0
1 1 1.4 1.0 1.2 1.0 1.35 1.5 1.3333333333333333 1.0 2.0 1 2.0 1 1 4 16	Remove polling from the addon manager to reduce overhead . In < URL > I pointed out that the addon manager polls to make sure addon configuration is correct every 5 seconds . This can result in CPU spikes up to ~ 80% of a core every 5 seconds . Every 5 seconds , the addon manager runs the same command : : /usr/local/bin/kubectl apply -f /etc/kubernetes/addons -l kubernetes.io/cluster-service!=true,addonmanager.kubernetes.io/mode=Reconcile -- prune = true -- prune-whitelist core/v1/ConfigMap -- prune-whitelist core/v1/Endpoints -- prune-whitelist core/v1/Namespace -- prune-whitelist core/v1/PersistentVolumeClaim -- prune-whitelist core/v1/PersistentVolume -- prune-whitelist core/v1/Pod -- prune-whitelist core/v1/ReplicationController -- prune-whitelist core/v1/Secret -- prune-whitelist core/v1/Service -- prune-whitelist batch/v1/Job -- prune-whitelist batch/v1beta1/CronJob -- prune-whitelist apps/v1/DaemonSet -- prune-whitelist apps/v1/Deployment -- prune-whitelist apps/v1/ReplicaSet -- prune-whitelist apps/v1/StatefulSet -- prune-whitelist extensions/v1beta1/Ingress -- recursive . Instead of running the addon manager as a pod by default , we can turn the addon manager addon off by default and run the : kubectl apply . command ourselves whenever a user changes the state of addons via : minikube addons enable/disable .	0	1
2 2 0.8 0.0 0.8 0.5 1.25 2.0 0.6666666666666666 0.0 0.0 0 0.0 5 7 10 41	latest storage-provisioner : cannot get resource ' endpoints ' in API group ' in the namespace ' kube-system ' . Not sure , but it seems related to < URL > The exact command to reproduce the issue : : minikube start kubectl create namespace test kubectl apply -f pvc . yaml . The full output of the command that failed : storage provisioner logs : : E0206 10:46:39 . 353949 1 leaderelection . go : 331 ] error retrieving resource lock kube-system/ k8s.io-minikube-hostpath : endpoints ' k8s.io-minikube-hostpath ' is forbidden : User ' system : serviceaccount : kube-system : storage-provisioner ' cannot get resource ' endpoints ' in API group ' in the namespace ' kube-system . The output of the : minikube logs . command : The operating system version : : minikube version minikube version : v 1.5.2 commit : 792dbf92a1de583fcee76f8791cff12e0c9440ad-dirty . : cat /etc/os-release NAME='Linux Mint ' VERSION='19 . 3 ( Tricia )' . : pvc . yaml . : : kind : PersistentVolumeClaim apiVersion : v1 metadata : name : test namespace : test spec : accessModes : - ReadWriteOnce resources : requests : storage : 1Gi .  	1	0
2 2 1.4 1.0 1.2 1.0 1.25 1.0 1.6666666666666667 2.0 0.0 0 0.0 2 8 11 42	minikube scp . It would be nice to be able to have : minikube scp . like : minikube ssh . , to copy files in the the minikube node . An use case would be : copying Dockerfiles to the node to build custom docker images for Kubernetes . In the meantime , for all of you that want this feature , here's a workaround : : scp -i $(minikube ssh-key ) < file > docker@$(minikube ip ): . edit : you can also use : minikube docker-env . to set up the : docker . client to point to minikube's docker daemon , and build the image locally . However , this still doesn't work for copying files ...	2	1
2 2 0.8 1.0 1.0 1.0 1.15 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 4 6 20	kvm2 : Get < URL > net/http : request canceled while waiting for connection ( Client . Timeout exceeded while awaiting headers ) . The exact command to reproduce the issue : : minikube start -- vm-driver = kvm2 . The full output of the command that failed : 棣冩 minikube v 1.4.0 on Debian rodete 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 2000MB , Disk = 20000MB ) ... 棣冩儞 Preparing Kubernetes v 1.16.0 on Docker 18.09.9 ... 棣冩 Pulling images ... 閴?Unable to pull images , which may be OK : running cmd : sudo env PATH = /var/lib/minikube/binaries/v 1.16.0 : $PATH kubeadm config images pull -- config /var/tmp/minikube/kubeadm . yaml : command failed : sudo env PATH = /var/lib/minikube/binaries/v 1.16.0 : $PATH kubeadm config images pull -- config /var/tmp/minikube/kubeadm . yaml stdout : stderr : failed to pull image ' k8s.gcr.io/kube-apiserver:v1.16.0 ' : output : Error response from daemon : Get < URL > net/http : request canceled while waiting for connection ( Client . Timeout exceeded while awaiting headers ) , error : exit status 1 To see the stack trace of this error execute with -- v = 5 or higher : Process exited with status 1 棣冩畬 Launching Kubernetes ... 閳?Waiting for : apiserver proxy etcd scheduler controller dns 棣冨及 Done ! kubectl is now configured to use ' minikube ' A system restart fixes this , but we should look into what's happening .  	1	0
0 0 0.8 1.0 0.9 1.0 1.1 1.0 1.0 1.0 0.0 0 0.0 0 1 3 21	Feature : setting minikube profile should set k8s current context . . minikube version : : minikube version : v 1.1.1 . problem : when changing profile , it doesn't change the k8s context . reproduce : Create two profiles p1 , p2 : minikube start -p p1 -- vm-driver virtualbox minikube start -p p2 -- vm-driver virtualbox . set the profile to p1 : : minikube profile p1 . : $ minikube profile p1 -- alsologtostderr -v = 8 I0615 16:11:49 . 939954 94839 notify . go : 128 ] Checking for updates ... 閴?minikube profile was successfully set to p1 . the kubectl context : : $ kubectl config current-context p2 . expect the context to be p1 but it is still p2 . ( it only gets set by start cmd ) os : mac os high sierra 10.13.6	2	1
2 2 1.4 2.0 1.2 1.5 1.5 2.0 1.3333333333333333 2.0 1.3043478260869565 23 1.0 0 0 9 25	sudo crictl : not found if installed to a directory outside of sudoers secure_path . We have some things breaking on CentOS , which does not have : /usr/local/bin . in the PATH . That is : users do have it in their PATH , but root does not . And : sudo . runs with the root PATH . So if you are trying to run something that is not from a package , you will get a ' command not found ' This goes both for ' crio version ' ( when run as root ) and for ' sudo crictl ' , only work with : /usr/bin . . One workaround is to move everything to : /usr/bin . , which is violating the < URL > but whatever . It would be nice if minikube was able to find : crio . and : crictl . - also in their default location ? This would have worked better if crio and cri-tools would be available in the default distribution . But it doesn't ( at the moment ) , and when you try to install it locally you run into these issues ...  	1	1
0 0 0.6 0.0 1.1 1.5 0.8 0.5 0.6666666666666666 0.0 0.0 0 0.0 2 8 15 40	-- apiserver-name has no effect on ~ / . kube/config . To reproduce : : powtest @chlorine : ~ $ minikube version minikube version : v 0.35.0 ````console powtest @chlorine : ~ $ sudo -E minikube start -- vm-driver = none -- apiserver-name = chlorine.acdlab	2	0
2 2 0.8 0.0 0.9 0.5 1.1 1.5 1.3333333333333333 2.0 0.9854014598540146 137 1.0 6 7 10 24	docker : minikube delete should delete the network . I notice minikube delete doesn't remoev the network after itself . created this issue . : medya @medya : ~ /workspace/minikube ( master)$ . /out/minikube delete -- all medya @medya : ~ /workspace/minikube ( master)$ docker network ls NETWORK ID NAME DRIVER SCOPE fdc833b38ce5 bridge bridge local 43cf4c7ad471 host host local 0e6c505503f2 minikube bridge local 2d7623149fc1 none null local .	0	0
1 1 1.2 1.0 1.2 1.0 1.15 1.0 1.3333333333333333 1.0 1.1538461538461537 117 1.0 1 5 7 24	Using the none driver sets the hostname to ' control-plane ' . This is not desired , the hostname should stay the way it is . For multi-node VMs that we create ourselves , then sure . But existing machines get to keep their current hostnames . Seen in #11174 with CentOS 8 ( systemd 239 )  	1	0
0 0 0.8 0.0 1.1 1.5 1.1 1.0 0.6666666666666666 0.0 1.588235294117647 17 2.0 0 0 3 23	Improve error message for service not in default namespace . . 1- create a NodePort service in not default namespace : : $ kubectlget svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S ) AGE default kubernetes ClusterIP 10.96.0.1 < none > 443/TCP 15d dgeek-v1 dgeek-helm-dgeek-helm NodePort 10.103.186.237 < none > 80:32316 /TCP 3m40s . 2- try to open the service url : : $ minikube service dgeek-helm-dgeek-helm 棣冩寴 Error opening service 閴?Error : [ SERVICE_NOT_FOUND ] Could not find finalized endpoint being pointed to by dgeek-helm-dgeek-helm : Temporary Error : Error getting service dgeek-helm-dgeek-helm : services ' dgeek-helm-dgeek-helm ' not found Temporary Error : Error getting service dgeek-helm-dgeek-helm : services ' dgeek-helm-dgeek-helm ' not found Temporary Error : Error getting service dgeek-helm-dgeek-helm : services ' dgeek-helm-dgeek-helm ' not found Temporary Error : Error getting service dgeek-helm-dgeek-helm : services ' dgeek-helm-dgeek-helm ' not found 棣冩寱 Suggestion : Please make sure the service you are looking for is deployed or is in the correct namespace . 閳﹀绗?Related issues : 閳?< URL > 棣冩▼ If the above advice does not help , please let us know : 棣冩啝 < URL > . the error message is big and suggestion gets over-shaddowed . I suggest the error message say : service X was not found in default namespace , please try with ' minikube service X -n Y ' .	2	0
1 1 1.4 1.0 1.2 1.0 1.25 1.0 1.0 1.0 2.0 1 2.0 0 2 5 24	parallels : `minikube mount` : attempted to get host ip address for unsupported driver . Running : minikube mount . with the parallels driver gives this error message : : 棣冩寴 Error getting the host IP address to use from within the VM : Error , attempted to get host ip address for unsupported driver . According to < URL > , this is preferred over relying on driver-specific mounting . It's an issue for me right now because I can't figure out how to get share folders enabled through the Parallels UI to show up on the Minikube VM .	2	1
1 1 1.4 2.0 1.2 1.5 1.15 1.0 1.6666666666666667 2.0 2.0 2 2.0 1 2 4 20	feature : set ingress class for ingress plugin ( for traefik ) . If someone is using a different ingress class , like : traefik . , then the ingress configurations won't work . We should be able to set the ingress class for the ingress plugin to resolve this with an option like : -- ingress-class ={{ default ' nginx ' . IngressClass }} . as described here : < URL >	2	1
2 2 1.4 2.0 1.5 2.0 1.5 2.0 1.3333333333333333 2.0 0.875 24 1.0 2 3 6 39	update-context is confusing with profiles : ' IP was already configured ' . : $ . /out/minikube update-context -p minikube 棣冩 IP was already correctly configured for 192.168.39.232 $ kubectl config view | grep current current-context : v1134 $ . /out/minikube update-context -p v1134 棣冩 IP was already correctly configured for 192.168.39.241 $ kubectl config view | grep current current-context : v1134 . Notice how the context never changes , but minikube thinks it is already correctly configured ?	2	0
2 2 1.8 2.0 1.7 2.0 1.45 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 3 4 8	controlPlaneEndpoint = localhost breaks clusterapi-apiserver : webhook . go connection refused . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : x86_64 linux Minikube version ( use : minikube version . ): > = v 0.28.1 - OS ( e.g. from /etc/os-release ): ubuntu , varying versions - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): kvm2 - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): > = v 0.28.1 What happened : cluster-api errors with the following : ERROR : logging before flag . Parse : E0814 22:20:52 . 810904 1 webhook . go : 192 ] Failed to make webhook authorizer request : Post < URL > dial tcp 127.0.0.1 : 8443 : connect : connection refused ERROR : logging before flag . Parse : E0814 22:20:52 . 811051 1 errors . go : 90 ] Post < URL > dial tcp 127.0.0.1 : 8443 : connect : connection refused . What you expected to happen : With 0.28.0 it connected to the apiserver as expected . How to reproduce it ( as minimally and precisely as possible ): Start minikube and install the clusterapi controller : curl -L < URL > | kubectl apply -f - Output of : minikube logs . ( if applicable ) : Anything else do we need to know : This behavior changed with the merge of this line . Removing the line reverts the behavior and allows the application to function correctly . < URL >  	1	0
2 2 1.0 1.0 0.7 0.0 1.05 1.0 1.6666666666666667 2.0 0.0 0 0.0 2 2 5 30	Windows : addons disable failed : [ disabling addon xxx : Process exited with status 1 . The exact command to reproduce the issue : minikube addons disable heapster The full output of the command that failed : ! disable failed : [ disabling addon deploy/addons/heapster/influx-grafana-rc . yaml : Process exited with status 1 ] * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL > The output of the : minikube logs . command : The operating system version : Windows 10  	1	0
2 2 1.4 2.0 1.3 2.0 1.3 2.0 1.6666666666666667 2.0 1.0813953488372092 86 1.0 1 12 28 57	site : create addon gallery on the website . we could have a an addon gallery on the website , where people can see addons , and filter by Operating system or driver , and a link to Add your software to minikube addons ... here is how .  	2	2
0 0 1.2 2.0 1.5 2.0 1.4 2.0 0.6666666666666666 0.0 2.0 1 2.0 4 5 8 22	save cache files under ~ / . cache/minikube . minikube writes a lot to : ~ / . minikube/cache . , but other programs have agreed to use : ~ / . cache/ < program > . instead . Consider switching to : $XDG_CACHE_HOME . ( default : ~ / . cache . ) for files that I can freely delete if I want to trade free disk for some repeated downloads or computation . Perhaps the rest of : ~ / . minikube . should go under : $XDG_CONFIG_HOME . ( default : ~ / . config/minikube . ) to help curb the homedir dotfile mess , but that's not as important to me . Maybe some people are syncing : ~ / . config . between multiple hosts ?	2	1
0 0 0.8 0.0 0.9 0.5 1.3 2.0 0.6666666666666666 0.0 0.0 0 0.0 0 0 5 16	minikube can't re-configure ingress to use a different ssl certificate . Steps to reproduce the issue : Tutorial that was followed : < URL > start minikube locally : minikube start . enable ingress : minikube addons enable ingress . add the auto-generated certificate in . pem : : kubectl -n kube-system create secret tls mkcert -- key ~ / . minikube/key . pem -- cert ~ / . minikube/cert . pem . : minikube addons configure ingress . In the prompt to add the certificate : be stupid and have a typo : : bogus-namespace/mkcert . Re-enable the ingress : : minikube addons disable ingress . : minikube addons enable ingress . Notice the mistake Try to re-configure the ingress : : minikube addons configure ingress . instead of getting the prompt again , it auto-proceeds to : ingress was successfully configured . <--- this is the bug ! Why would I be stuck with this typo forever ? Run : minikube logs -- file = logs . txt . and drag and drop the log file into this issue Full output of failed command if not : minikube start . :	0	0
2 2 1.2 1.0 1.4 1.5 1.25 1.0 1.0 1.0 1.0 1 1.0 4 7 13 44	sometimes K8s still try to pull cached images . When I am verifying #10044 , I found sometimes k8s still try to pull cached images .  	1	0
0 0 1.0 1.0 1.0 1.0 0.95 1.0 1.0 1.0 1.2702702702702702 37 1.0 0 2 6 27	Make minikube work in Google Cloud Shell ( using kic ) . also related < URL > : medya @cloudshell : ~ ( k8s-minikube)$ . /minikube start -- vm-driver = docker * minikube v 1.7.0 -beta . 0 on Debian 9.11 * Selecting experimental ' docker ' driver from user configuration ( alternates : []) * Creating Kubernetes in docker container with ( CPUs = 2 ) , Memory = 2000MB ( 1995MB available ) ... * X Failed to enable container runtime : br_netfilter : command failed : [ docker exec -- privileged minikube sudo modprobe br_netfilter ]: exit status 1 * * minikube is exiting due to an error . If the above message is not useful , open an issue : - . currently it fails at this line : : func enableIPForwarding(cr CommandRunner ) error { c : = exec . Command('sudo ' , ' modprobe ' , ' br_netfilter ' ) if _ , err : = cr . RunCmd(c ); err ! = nil { return errors . Wrap(err , ' br_netfilter ' ) } } . running the command manually I get this output : $ docker exec -it minikube sudo modprobe br_netfilter modprobe : ERROR : .. /libkmod/libkmod . c : 586 kmod_search_moddep () could not open moddep file ' /lib/modules/ 4.19.79 +/modules . dep . bin ' modprobe : FATAL : Module br_netfilter not found in directory /lib/modules/ 4.19.79 + .	0	1
1 1 1.0 1.0 1.1 1.5 1.2 1.5 1.0 1.0 0.0 0 0.0 0 3 9 32	missing tag for nvidia-driver-installer addon . Steps to reproduce the issue : minikube start minikube addons enable nvidia-driver-installer kubectl get pod -A | grep nvidia-driver-installer : kube-system nvidia-driver-installer-v9fxt 0/1 Init : ImagePullBackOff . This is a regression after #9551  	1	0
1 1 0.6 0.0 1.0 1.0 1.2 1.5 0.3333333333333333 0.0 1.1 80 1.0 4 7 10 46	do not crash , when missplelled container runtime . : . /out/minikube start -- driver = docker -- container-runtime = conatinerd 棣冩 minikube v 1.9.2 on Darwin 10.13.6 閴?Using the docker driver based on user configuration 棣冩寴 Failed to generate config : unknown runtime type : ' conatinerd ' 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > . in this case i misspelled containerd and instead of crying and crashing and asking user to create an issue , we could say ' conatinerd ' is not a valid container-runtime , the valid options are ...  	1	1
2 2 1.4 2.0 1.5 2.0 1.3 2.0 1.3333333333333333 2.0 1.1428571428571428 119 1.0 0 3 9 35	The kicbase image downloads twice . There is a regression in minikube 1.20.0 , that the kicbase image will download twice : once to cache , once to daemon The workaround is to do minikube start -- download-only ( or maybe : docker pull . ) , or to just let it download The fix was in PR ~ ~ #11063 ~ ~ But it didn't make it to release	0	0
0 0 0.6 0.0 0.8 0.5 1.1 1.0 0.6666666666666666 0.0 1.024390243902439 41 1.0 4 5 6 30	Download our own kvm2 driver if installed version is missing or out of date . Related : #3975 #4387 Also , mentioned in our roadmap : < URL >  	1	1
1 1 1.2 1.0 1.1 1.0 0.9 1.0 1.3333333333333333 1.0 1.0238095238095237 42 1.0 5 6 7 31	Write a script to automatically propose PR's due to a new Kubernetes release . I would love a script that we can run from a cronjob or Jenkins that monitors < URL > for new releases . The script should propose a PR for any release newer than NewestKubernetesVersion : < URL > If the release is a non-beta release , it should also change DefaultKubernetesVersion in the same PR : < URL > This script should either build upon or replace < URL > - which makes the changes necessary in our code base , but is designed for interactive use and does not know how to look for new Kubernetes releases . We can provide a github token via an environment variable .	0	1
2 2 1.4 2.0 1.1 1.5 1.15 1.0 1.3333333333333333 2.0 0.9888888888888889 90 1.0 3 8 12 50	Add validation to : minikube config set memory . Figured this out while writing documentation : Don't allow the user to specify more memory than is available Don't allow the user to specify less memory than is possible If the user is using Docker , and their maximum exceeds Docker's limit , point them to the appropriate Docker documentation to increase it  	1	1
0 0 1.0 1.0 1.0 1.0 1.05 1.0 1.0 1.0 1.0 1 1.0 2 2 2 5	Make minikube offline-friendly by default ( needs testing ) . We get many bug reports where users have issues accessing the internet due to proxy issues . I'm trying to collect them here : < URL > This somewhat goes along with #1623 - but requires making sure we test that minikube works offline .	0	1
2 2 1.4 2.0 1.4 2.0 1.5 2.0 1.0 1.0 0.0 0 0.0 3 4 6 40	how can I configure kube-scheduler in minikube ? . I want configure a special kube-scheduler in minikube . I'm following this article in minikube ( < URL > In step2 modify-scheduler-configuration , I must specify a kube-scheduler with a yaml , how can I do this ? First , I try to execute the command : kubectl create -f kube-scheduler . yaml . , but the pod startup failure . Then , I read the following link < URL > and this link < URL > So I try to execute the command : minikube start -- extra-config = scheduler . config = kube-scheduler . yaml . , But the pod/kube-scheduler-minikube status is CrashLoopBackOff , I don't think I used the : -- extra-config . command properly , please give me some help . thanks very much  	2	2
2 2 1.0 1.0 1.2 1.5 1.35 1.5 0.6666666666666666 0.0 0.9541984732824428 131 1.0 0 4 10 27	Investigate configuring Kubernetes to pull from the host Docker daemon . This was an idea from Brian De Alwis over chat : Could we change minikube so that Kubernetes pulls from the host Docker daemon ? Similar to how Docker Desktop works today . This would allow image caches to persist across clusters , and remove the latency of pushing images from the host into the cluster .  	1	1
2 2 1.8 2.0 1.4 2.0 1.55 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 1 3 10	kube-dns still deployed with coredns . BUG REPORT : Minikube version ( use : minikube version . ): v 0.30.0 - OS ( e.g. from /etc/os-release ): Windows - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): Virtualbox - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.30.0 What happened : After default installation ( no specific arguments passed ) , minikube has both kube-dns and coredns deployments with one kube-dns service pointing on the 2 implementations What you expected to happen : After default installation , minikube addons list indicates coredns is enabled and kube-dns is disabled . I expect to have kube-dns service that targets only coredns pod How to reproduce it ( as minimally and precisely as possible ): : $ minikube start [ ... ] $ kubectl -n kube-system get deploy | grep dns coredns 1 1 1 1 8h kube-dns 1 1 1 1 8h . Anything else do we need to know : Everything seems to work fine with 2 implementations at the same time . If I delete kube-dns deployment , it seems to still work ( not yet extensively tested BTW )	0	0
2 2 1.0 1.0 1.2 1.5 1.0 1.0 1.3333333333333333 2.0 2.0 1 2.0 2 2 4 14	dashboard integration test leaks kubectl processes . There is a process leak related to the dashboard & minikube proxy on the mac mini integration test node . : $ ps ax | grep proxy 8063 ?? S 0:00 . 07 /usr/libexec/networkserviceproxy 8498 ?? S 0:05 . 18 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 9142 ?? S 0:05 . 22 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 9905 ?? S 0:05 . 12 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 10639 ?? S 0:05 . 07 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 11704 ?? S 0:04 . 54 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 12482 ?? S 0:04 . 45 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 13045 ?? S 0:04 . 40 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 13684 ?? S 0:04 . 41 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 14275 ?? S 0:04 . 36 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 14875 ?? S 0:04 . 26 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 15466 ?? S 0:03 . 96 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 16032 ?? S 0:03 . 88 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 16641 ?? S 0:03 . 46 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 17228 ?? S 0:03 . 43 /usr/local/bin/kubectl -- context minikube proxy -- port = 0 .	2	0
0 0 0.4 0.0 0.7 0.5 1.0 1.0 0.3333333333333333 0.0 0.9512195121951219 164 1.0 3 7 8 36	site : Add more troubleshooting and debugging commands . when someone asks how to debug a problem in minikube , we should provide all the deubbing commands we do , our current troubleshooting page doesn't have all the commands an advanced user can do < URL > we should add all the post mortem commands we do in our integration tests to our troubleshoting page here < URL > < URL >  	2	2
2 2 1.2 2.0 1.0 1.0 1.15 1.5 1.3333333333333333 2.0 0.0 0 0.0 1 4 11 30	`minikube config set embed-certs true` does have no effect . Steps to reproduce the issue : Run : minikube config set embed-certs true . . Verify that it has been set : : minikube config get embed-certs . Restart minikube . View : $HOME/ . kube/config . . There is no field : user . client-certificate-data . in the : minikube . profile . Expected behavior The certificate should be embedded directly in the : minikube . profile inside : $HOME/ . kube/config . . However , starting the cluster using : minikube start -- embed-certs . will work .  	1	1
2 2 1.4 2.0 1.1 1.0 1.05 1.0 1.0 1.0 1.75 4 2.0 0 1 5 27	hyperv : failure : start : exit status 1 ( not running minikube as admin ? ) . The exact command to reproduce the issue : : minikube start -- vm-driver = hyperv . The full output of the command that failed : : $ minikube start -- vm-driver = hyperv * minikube v 1.4.0 on Microsoft Windows 10 Enterprise 10.0.17763 Build 17763 * Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . E1015 10:10:29 . 795414 2708 cache_images . go : 79 ] CacheImage kubernetesui/dashboard : v 2.0.0 -beta4 -> C : /Users/balintp/ . minikube/cache/images/kubernetesui/dashboard_v 2.0.0 -beta4 failed : fetching image : unrecognized HTTP status : 503 Service Unavailable * Starting existing hyperv VM for ' minikube ' ... * Retriable failure : start : exit status 1 * Deleting ' minikube ' in hyperv ... * Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . * Starting existing hyperv VM for ' minikube ' ... * Retriable failure : start : exit status 1 * Deleting ' minikube ' in hyperv ... * Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . * Starting existing hyperv VM for ' minikube ' ... * Retriable failure : start : exit status 1 * Deleting ' minikube ' in hyperv ... . The output of the : minikube logs . command : n/a The operating system version : Microsoft Windows 10 Enterprise 10.0.17763 Build 17763  	1	0
1 1 1.4 2.0 1.4 2.0 1.4 2.0 1.0 1.0 0.8994708994708994 189 1.0 0 3 11 51	improve UI advice when user needs to delete the cluster . here we confuse the user with two things 閴?You cannot change the memory size for an exiting minikube cluster . Please first delete the cluster . and 棣冩▼ Failed to start ssh bare metal machine . Running ' minikube delete -p foo ' may fix it : config : please provide an IP address we should had sticked with first advice : drewpca(pts/0 ): ~ % minikube start -p foo -- memory 6GB 棣冩 [ foo ] minikube v 1.17.1 on Debian rodete 閴?Using the ssh driver based on existing profile 閴?You cannot change the memory size for an exiting minikube cluster . Please first delete the cluster . 棣冩啢 Starting control plane node foo in cluster foo 棣冦亞 StartHost failed , but will try again : config : please provide an IP address 棣冩▼ Failed to start ssh bare metal machine . Running ' minikube delete -p foo ' may fix it : config : please provide an IP address 閴?Exiting due to GUEST_PROVISION : Failed to start host : config : please provide an IP address 棣冩▼ If the above advice does not help , please let us know : 棣冩啝 < URL > .   	1	0
1 1 1.6 2.0 1.3 1.5 1.3 1.5 1.6666666666666667 2.0 0.0 0 0.0 1 2 3 17	ingress-dns addon docs/setup . The documentation for ingress-dns is confusing . It looks like it is instructing users to use resolvconf to configure the new DNS server and then disable resolvconf . It would be nice to either ( or both ) explain what is being done for each of these steps and why , as well as have some systemd only instructions , like : : sudo systemd-resolve -- interface $BRIDGEINTERFACE -- set-dns $MINIKUBE_IP -- set-domain test . I also noticed that dnsmasq was running in the background , and that there are minikube configurations there . This obviously cannot run at the same time with systemd-resolved running . There's little docs on that setup or how to use it , or trade-offs between this solution and using dnsmasq . Steps to reproduce the issue : minikube addons enable ingress-dns Follow Docs : < URL >  	2	2
0 0 0.8 1.0 1.3 1.5 0.95 1.0 1.0 1.0 2.0 1 2.0 3 4 10 30	`minikube kubectl` -- arguments and stdin for `apply -f -` not possible . The exact command to reproduce the issue : : $ minikube kubectl version -- short . : $ echo ' foo:' | minikube kubectl apply -f - . : $ echo ' foo:' | minikube kubectl -- apply -f - . : $ echo ' foo:' | minikube kubectl apply -- -f - . : $ minikube -- kubectl version -- short . The full output of the command that failed : : Error : unknown flag : -- short . : Error : unknown shorthand flag : ' f ' in -f . : error : no objects passed to apply . : error : no objects passed to apply . : minikube version : v 1.1.0 . The expected output of the command that failed : Just like : minikube kubectl -- version -- short . : : Client Version : v 1.11.8 Server Version : v 1.11.8 . Just like : echo ' foo:' | kubectl apply -f - . : : error : error validating ' STDIN ' : error validating data : [ apiVersion not set , kind not set ]; if you choose to ignore these errors , turn validation off with -- validate = false . Just like : echo ' foo:' | kubectl apply -f - . : : error : error validating ' STDIN ' : error validating data : [ apiVersion not set , kind not set ]; if you choose to ignore these errors , turn validation off with -- validate = false . Just like : echo ' foo:' | kubectl apply -f - . : : error : error validating ' STDIN ' : error validating data : [ apiVersion not set , kind not set ]; if you choose to ignore these errors , turn validation off with -- validate = false . Just like : minikube kubectl -- version -- short . : : Client Version : v 1.11.8 Server Version : v 1.11.8 . The output of the : minikube logs . command : The operating system version : MacOS vmdriver = hyperkit kubectl version v 1.11.8 on v 1.11.8 k8s cluster , installed via : minikube kubectl .	2	0
2 2 0.4 0.0 0.7 0.0 0.95 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 4 20	Add embedding gcflags from environment variable in Makefile . While hacking around the codebase I noticed there's no way to embed gcflags into binary build command in Makefile : < URL > I wanted to debug the code via GoLand ( < URL > which requires the user to set certain gcflags and I had to hardcode the flags into Makefile and then remove them before submiting a PR . This is cumbersome . What I propose is to enable embedding gcflags in Makefile via environment variable , similarly to : -ldflags='$(MINIKUBE_LDFLAGS )' . Let me know what you think and I can take a stab at implementing this . Thanks !	2	1
2 2 1.4 2.0 1.5 2.0 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 2 4 6 12	Support multiple -- mount-string arguments . When run with the docker driver , minikube takes a : -- mount-string . argument . Provided the minikube container does not yet exist , it is created with the specified host folder or docker volume mounted at the specified location e.g. : minikube start -- mount -- mount-string ' host_volume : /mount_point ' . Mounts the : host_volume . docker volume at : /mount_point . in the minikube container . In fact the : -- mount-string . flag can be specified multiple times , but only the last specified is used . A very small change to : cmd/minikube/cmd/start_flags . go . would allow multiple : -- mount-string . arguments to be used .	2	1
2 2 1.6 2.0 1.5 2.0 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 3 7 13 52	Minikube fails on KVM2 on fresh minikube installation on Fedora 33 ( with solution ) . Steps to reproduce the issue : Follow instructions from < URL > Install minikube instructions from < URL > to install minikube Run minikube with : $ minikube config set vm-driver kvm2 $ minikube start -- memory 4096 Full output of failed command : Exiting due to PROVIDER_KVM2_ERROR : /usr/bin/virsh domcapabilities -- virttype kvm failed Full output of : minikube start . command used , if not already included : Optional : Full output of : minikube logs . command : The resolution for the problem was doing two things : 1 . Adding systemd . unified_cgroup_hierarchy = 0 in the kernel command line ( via grubby ) . 2 . Adding my users to appropriate libvirt and kvm groups sudo usermod -aG kvm $USER sudo usermod -aG libvirt $USER Perhaps it's worth adding somewhere to documentation , but i don't know where - there is no page with ' common installation problems ' .  	2	2
2 2 1.6 2.0 1.7 2.0 1.45 2.0 2.0 2.0 0.9741935483870968 155 1.0 1 1 7 23	Add support for windows server data center for HyperV driver .  	1	1
0 0 0.8 0.0 1.0 1.0 0.85 0.5 0.6666666666666666 0.0 1.11 100 1.0 1 9 21 74	document in the contributing section about cgo and libvirt . on a fresh ubuntu computer I had to install these to be able to make lint install cgo < URL > install libvirt sudo apt-get install -y libvirt-dev    	1	2
2 2 1.4 2.0 1.2 1.5 1.15 1.0 1.0 1.0 0.0 0 0.0 6 7 9 57	Docker : Failed to setup kubeconfig : inspect IP bridge network . srahmed @hp : ~ $ minikube start 棣冩 minikube v 1.10.1 on Ubuntu 20.04 閴?Automatically selected the docker driver 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 2200MB ) ... 棣冩儞 Preparing Kubernetes v 1.18.2 on Docker 19.03.2 ... E0513 19:32:35 . 665322 20138 start . go : 95 ] Unable to get host IP : inspect IP bridge network ' de7f841a3590/n13c6d9205f9b ' . : docker inspect -- format ' {{( index . IPAM . Config 0 ) . Gateway }}' de7f841a3590 13c6d9205f9b : exit status 1 stdout : stderr : Error : No such object : de7f841a3590 13c6d9205f9b 棣冩寴 failed to start node : startup failed : Failed to setup kubeconfig : inspect IP bridge network ' de7f841a3590/n13c6d9205f9b ' . : docker inspect -- format ' {{( index . IPAM . Config 0 ) . Gateway }}' de7f841a3590 13c6d9205f9b : exit status 1 stdout : stderr : Error : No such object : de7f841a3590 13c6d9205f9b 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL >	0	0
1 1 1.2 1.0 1.4 1.5 1.4 2.0 1.0 1.0 1.1875 48 1.0 0 1 7 63	Increase default disk image allocation ? . Something that we discussed about the Docker VM , which allocates 2 GB RAM + 1 GB swap : As discussed in < URL > Maybe it is time to increase our default memory allocation to 4 GB , at least where available ? Similarly , we could up the default disk image to 40 GB , since it will only occupy as much as used . The idea would be to have something like computer games , ' recommended ' vs ' minimum ' : < URL > Minimum : 2 CPUs or more 2GB of free memory 20GB of free disk space Recommended : 4 CPUs or more 4GB of free memory 40GB of free disk space Currently we follow the recommendations from : kubeadm . , but it seems to be a bit low ? < URL > Before you begin : 2 GB or more of RAM per machine ( any less will leave little room for your apps ) 2 CPUs or more . We already use more memory than this , so it is mostly about the documentation . : 棣冩暉 Creating virtualbox VM ( CPUs = 2 , Memory = 6000MB , Disk = 20000MB ) ... . : 棣冩暉 Creating docker container ( CPUs = 2 , Memory = 8000MB ) ... . The hard requirement is currently 1000MB , but that requires swap to even ' function ' . Ideally this would be followed up by an analysis of where the memory is actually going ... Like monitor the memory usage over time , like we did previously with the cpu usage ?  	2	2
0 0 0.8 0.0 1.1 1.5 1.2 2.0 0.6666666666666666 0.0 0.9824561403508771 57 1.0 0 2 9 20	logs no longer include libmachine commands , making hyperv impossible to debug . We used to have logs for Hyper-V and Virtualbox machine drivers that showed each command run . Where'd they go ?	0	0
2 2 1.8 2.0 1.6 2.0 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 1 4 31	update-context not working when using KUBECONFIG . The exact command to reproduce the issue : 閴?sudo -i env CHANGE_MINIKUBE_NONE_USER = true MINIKUBE_HOME = /home/somewhere KUBECONFIG = /tmp/ . somewhereelse/config minikube start -- vm-driver = none ... 閴?KUBECONFIG = /tmp/ . somewhereelse/config minikube start ... 閴?KUBECONFIG = /tmp/ . somewhereelse/config minikube status host : Running kubelet : Running apiserver : Running kubeconfig : Configured 閴?KUBECONFIG = /tmp/ . somewhereelse/config minikube update-context The full output of the command that failed : 棣冩寴 update config : Kubeconfig does not have a record of the machine cluster 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The output of the : minikube logs . command : No new logs are logged when : update-context . is run . The operating system version : centos 7 If I run : KUBECONFIG = /tmp/ . somewhereelse/config kubectx . , : minikube . context is present though . The idea was to run : minikube update-context . ( without the KUBECONFIG env variable ) so that my default kubectx would know about : minikube . context too . But that fails too with the same output pasted above .	2	0
1 1 0.6 0.0 0.8 0.5 1.05 1.0 1.0 1.0 1.2727272727272727 11 2.0 0 0 1 6	Allow users to pass kubeadm flags to ignore SystemVerification . To help with future situations such as < URL >	2	1
2 2 1.6 2.0 1.4 2.0 1.2 1.0 1.6666666666666667 2.0 0.0 0 0.0 3 9 17 41	Include JSON Output as a standard option . . the : -- output . flag does not seem to be offered with most of the commands . : start . has it , but : pause . , : unpause . , and : stop . do not . Is it possible to make this flag available to all commands as an inherited option ? or at least more commands ?	0	1
0 0 0.6 1.0 0.5 0.0 0.85 1.0 0.3333333333333333 0.0 0.0 0 0.0 3 6 10 23	Add ability to run amd64 binary on M1 . Steps to reproduce the issue : 1 . Install docker for m1 : < URL > . 2 . Install Minikube : curl -LO < URL > sudo install minikube-darwin-amd64 /usr/local/bin/minikube . run minikube start minikube start log < URL > minikube logs < URL >	0	0
1 1 1.0 1.0 0.9 1.0 1.05 1.0 1.3333333333333333 1.0 0.9912280701754386 114 1.0 0 2 6 28	-- delete-on-failure does not regenerate configuration . -- delete-on-failure is unable to recover from issues like #8325 because the configuration file is not regenerated .	0	0
0 0 1.0 1.0 1.5 2.0 1.15 1.5 1.0 1.0 0.8387096774193549 217 1.0 2 4 6 23	Periodically tell user about minikube features/tips and tricks . we should have a Random Tips that on each time they start minikube it will tell them about a feature of minikube ... ( the Tips of the day should be a way that u could disable it ) : $ minikube start 棣冩 minikube v 1.19.0 on Darwin 11.2.3 閴?Using the docker driver based on existing profile 棣冩啢 Starting control plane node minikube in cluster minikube 棣冨籍 Updating the running docker ' minikube ' container ... 棣冩儞 Preparing Kubernetes v 1.20.2 on Docker 20.10.5 ... 棣冩敺 Verifying Kubernetes components ... 閳?Using image gcr.io/k8s-minikube/storage-provisioner:v5 棣冨皞 Enabled addons : storage-provisioner , default-storageclass 棣冨及 Done ! kubectl is now configured to use ' minikube ' cluster and ' default ' namespace by default ----- Tip of the day --------------------------------------------------------------------------- | Some Fancy fox UI : | Did you know you can specify any kubernetes version you want ? for example minikube start -- kubernetes-version = v 1.19.0 | | to disable this Tips Run .... minikube config set BLAHBLAH ----------------------------------------------------------------------------------------------- . or : ----- Tip of the day --------------------------------------------------------------------------- Did you know you can enroll getting notifcation for beta releases by minkube config set .... ? | to disable tips of the day run .... minikube config set BLAHBLAH ----------------------------------------------------------------------------------------------- . this will require us to have a database that has list of tips and tricks , file that can be added to our website automatically using : make generate-doc .  	1	1
2 2 1.4 2.0 1.3 2.0 1.35 2.0 1.6666666666666667 2.0 0.0 2 0.0 8 8 14 37	Add last start ( verbose ) logs to minikube logs command .	0	1
1 1 0.8 1.0 0.8 1.0 0.95 1.0 1.3333333333333333 1.0 0.0 0 0.0 1 3 4 9	Debian package should include the kvm driver ( docker-machine-driver-kvm2 ) . Is this a BUG REPORT or FEATURE REQUEST ? : FEATURE REQUEST The minikube-XXX- . deb package is intended to be used on debian systems . To simplify setup and usage of kvm2 driver , it could be include within the package ( maybe provided through private debian repository ) . Thus the installation and upgrade of minikube will be more convenient . Please provide the following details : Environment : Linux Minikube version : 0.27 and 0.28 - OS : Ubuntu 18.04 - VM Driver : KVM2 What happened : I need to install and maintain ( upgrade ) docker-machine-driver-kvm2 manually as sudoer . What you expected to happen : Installing/upgrading the minikube through debian package , the docker-machine-driver-kvm2 is installed/upgraded accordingly .	0	1
1 1 1.2 1.0 0.6 0.5 1.0 1.0 1.3333333333333333 1.0 1.0952380952380953 84 1.0 6 15 26 57	docs : links to netowkring is broken . here < URL > the link < URL > is broken addtitioanlly all of the links in the website , that links to our own website should be changed to this format : more information on < URL > instead of the full url < URL >    	1	2
0 0 0.6 1.0 0.6 0.5 0.9 1.0 0.6666666666666666 1.0 0.0 1 0.0 0 0 0 0	Support upgrading minikube VM with contents from a newer ISO . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): feature request There should be a feature for upgrading the minikube-iso without deleting k8s state .	2	1
2 2 1.2 2.0 1.0 1.0 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 2 3 27	minikube tunnel doesn't appear to support UDP LoadBalancers . Steps to reproduce the issue : minikube start -- vm-driver = docker -- memory 8192 -- cpus 6 kubectl apply -f : kind : Service apiVersion : v1 metadata : name : udp spec : ports : - name : nginx-udp protocol : UDP port : 11500 targetPort : 11500 type : LoadBalancer . minikube tunnel : 閴?minikube tunnel 棣冨籍 Starting tunnel for service nginx-udp . . sudo lsof -i -P -n | grep 11500 : ssh 14238 user 5u IPv6 0xe5c1af091405c8ef 0t0 TCP [: : 1 ]: 11500 ( LISTEN ) ssh 14238 user 6u IPv4 0xe5c1af092c227f77 0t0 TCP 127.0.0.1 : 11500 ( LISTEN ) . As observed here , the opened tunnel is TCP and not UDP which means I'm unable to route UDP traffic to my nginx load balancer .	2	1
2 2 1.2 1.0 1.1 1.0 0.9 1.0 1.0 1.0 1.1875 16 1.5 1 5 7 13	Hyper-V : Stable integration testing . Not sure what the blockers are here . There were some Jenkins communication issues .  	1	1
2 2 1.2 2.0 1.0 1.0 0.95 1.0 1.3333333333333333 2.0 0.8217391304347826 230 1.0 1 2 3 21	discourge and add warnning for vbox driver if there are other healthier options available . some users might have set config to use vbox driver in the past and minikube respects it and wont change to better drivers in these cases we should show a Box UI warnining that there are better optionns avilable and here are the commandns to use it  	1	1
1 1 1.2 1.0 1.3 1.0 1.25 1.0 0.6666666666666666 1.0 2.0 1 2.0 1 2 5 16	CRI-O systemd unit not picking up /etc/sysconfig/crio . minikube and thus not setting -- insecure-registry . Steps to reproduce the issue : Started thus : : minikube start / -- vm-driver = kvm2 / -- container-runtime = cri-o / -- cni = bridge / -- addons = registry / -- cpus = 4 / -- service-cluster-ip-range = 10.96.0.0 /12 . Now , do : minikube ssh . and : : sudo systemctl status crio . You will see that the command used is : /usr/bin/crio -- log-level = debug . . However , this is incomplete . Looking at : /etc/systemd/system/multi-user . target . wants/crio . service . you will see that it refers to : /etc/sysconfig/crio . minikube . , which importantly has extra arguments for : -- insecure-registry . . However , it seems that : /etc/sysconfig/crio . minikube . was not used . This breaks the registry add-on out of the box . As a workaround , do a quick : : sudo systemctl restart crio sudo systemctl status crio . And you will see that : -- insecure-registry . has now been set properly . My guess is that somehow the ' crio . minikube ' file was create too late during the setup process .  	1	0
1 1 0.8 1.0 1.1 1.0 1.15 1.0 0.3333333333333333 0.0 1.0 1 1.0 2 2 6 18	Add binary size diff to PR performance bot . . Related : #5398	2	1
0 0 1.4 2.0 1.2 1.0 1.1 1.0 1.3333333333333333 2.0 1.56 25 2.0 0 3 8 21	add gopogh for prettifying windows logs . simmilar to what I did for linux in common.sh < URL > to get pretty html integration test logs we need need to do follow for windows scripts : - Pipe the output of tests to a file tests . out - Generate : test . json . output using : go tool test2json . - install gopogh v 0.0.17 using go get - run gopogh -in : test . json . -o ' test . html ' ( add other optional args ) - upload to gcs ...	2	1
0 0 0.8 1.0 1.2 1.5 1.0 1.0 1.0 1.0 1.1521739130434783 46 1.0 1 2 8 79	Add root or sudo requirements for the KIC drivers to the documentation . Currently we just refer to the upstream documentation , but don't mention the extra step needed : < URL > and < URL > : $ sudo apt-get install docker-ce docker-ce-cli containerd.io . < URL > and < URL > : $ sudo apt-get -qq -y install podman . Then it will fail on the first : minikube start . instead , mentioning what needed to be done before . : 閴?' docker ' driver reported an issue : ' docker version -- format {{ . Server . Version }}' exit status 1 : Got permission denied while trying to connect to the Docker daemon socket at unix :// /var/run/docker . sock : Get < URL > dial unix /var/run/docker . sock : connect : permission denied 棣冩寱 Suggestion : Add your user to the ' docker ' group : ' sudo usermod -aG docker $USER && newgrp docker ' 棣冩憣 Documentation : < URL > . : 閴?' podman ' driver reported an issue : ' sudo -n podman version -- format {{ . Version }}' exit status 1 : sudo : a password is required 棣冩寱 Suggestion : Add your user to the ' sudoers ' file : ' $USER ALL =( ALL ) NOPASSWD : /usr/local/bin/podman ' 棣冩憣 Documentation : < URL > . So we might as well add those steps to the documentation , in the first place ? Note that : sudo docker . and : sudo podman . do work fine , but with a password prompt ... We should also take care to mention the security implications of removing the password . It means that anything can run : docker . and : podman . commands - not only minikube .  	2	2
1 1 0.4 0.0 1.0 1.0 1.3 1.5 0.3333333333333333 0.0 0.0 0 0.0 0 0 6 32	dashboard : Could not decode config /home/user/ . minikube/config/config . json : EOF . : user @ubu18041 : ~ $ minikube status host : Running kubelet : Running apiserver : Running kubectl : Correctly Configured : pointing to minikube-vm at 192.168.39.226 user @ubu18041 : ~ $ . : user @ubu18041 : ~ $ minikube dashboard - Enabling dashboard ... ! Unable to enable dashboard : Could not decode config /home/user/ . minikube/config/config . json : EOF . I am running Ubuntu 18.04.1	2	0
0 0 0.8 1.0 0.8 1.0 0.95 1.0 1.0 1.0 0.0 0 0.0 0 2 3 26	How to enable auditing ? . The exact command to reproduce the issue : : minikube start / -- vm-driver hyperkit / -- extra-config = apiserver . authorization-mode = RBAC / -- extra-config = apiserver . Audit . LogOptions . Path = /var/log/apiserver/audit . log / -- extra-config = apiserver . Audit . PolicyFile = /etc/kubernetes/addons/audit-policy . yaml . I tried several extra-config variants : - apiserver . Audit . LogOptions . Path , apiserver . Audit . PolicyFile - apiserver . audit-logoptions-path , apiserver . audit-policyfile - apiserver . audit-log-options-path , apiserver . audit-policy-file - apiserver . audit-policy-path , apiserver . audit-log-dir , apiserver . audit-log-max-age together with different values : - log options path / log path / path - /var/log/kube-apiserver-audit . log - /etc/kubernetes/logs/apiserver-audit . log - log dir - /var/log/kubernetes/ - policy path - /etc/kubernetes/addons/audit-policy . yaml I tried also : - : -- feature-gates = Auditing = true . - : -- feature-gates = AdvancedAuditing = true . It does not matter how many CPUs or memory I give , minikube does not start in any case :( The full output of the command that failed : Please see all attached files The output of the : minikube logs . command : No output because minikube does not start . The operating system version : MacBook Pro - macOS Mojave 10.14.5 Minikube v 1.1.1 I already had a look at the issue #1609 , but it doesn't helped :( I attached all attempts I did . If someone has ideas or can help it would be great ! Thanks guys !! < URL > < URL > < URL > < URL > < URL > < URL > < URL > < URL > < URL > < URL >	2	2
1 1 1.0 1.0 0.8 0.5 0.75 0.5 1.0 1.0 0.9661016949152542 59 1.0 1 2 6 27	Save info logs to ~ / . minikube/logs by default . : ~ / . minikube/logs . is created , but never used , which is a shame . It would be nice if switching to klog ( #5318 ) or some other PR address this so that we can always tell users to send us logs from their most recent run .  	1	0
0 0 0.8 1.0 1.1 1.0 1.05 1.0 0.3333333333333333 0.0 1.0458715596330275 109 1.0 5 9 20 82	ingress on kubernetes docs is broken . following the docs on linux with KVM2 driver . < URL > ( the docs has a few syntax errorr too but after fixing them ) curl hello-world.info would just hang .	0	0
1 1 1.8 2.0 1.7 2.0 1.3 2.0 1.6666666666666667 2.0 0.0 0 0.0 1 2 7 31	Consider adding JSON/key value pair output to docker-env command . We use python to do a good amount of automation at my shop . When iterating locally it would be nice to be able to have a cross platform/language agnostic output of the : docker-env . command that a tool written in a non-shell script language could use . : $ minikube docker-env You can further specify your shell with either ' cmd ' or ' powershell ' with the -- shell flag . SET DOCKER_TLS_VERIFY = 1 SET DOCKER_HOST = tcp :/ / 127.0.0.1 : 61041 SET DOCKER_CERT_PATH = C : /Users/user/ . minikube/certs SET MINIKUBE_ACTIVE_DOCKERD = minikube REM To point your shell to minikube's docker-daemon , run : REM @FOR /f ' tokens=*' %i IN (' minikube -p minikube docker-env ' ) DO @%i . Ideally something like this : : $ minikube docker-env -- output-kvp { ' DOCKER_TLS_VERIFY ' : 1 , ' DOCKER_HOST ' : ' tcp :/ / 127.0.0.1 : 61041 ' , ' DOCKER_CERT_PATH ' : ' C :/ /Users//user// . minikube//certs ' , ' MINIKUBE_ACTIVE_DOCKERD ' : ' minikube ' } . I realize this could be done with regex libraries in the language of choice but having a convenience function would be fantastic .	2	1
0 0 1.0 1.0 0.9 0.5 0.85 1.0 1.0 1.0 0.0 0 0.0 1 3 9 30	Improve and standardize low-fi output prefixes ( MINIKUBE_IN_STYLE = 0 ) . Moved from < URL > The change to logging has cluttered output with worthless characters , or even worse emoji . Please add an environment variable to shut this off . Example : : $ minikube start -- container-runtime = cri-o -- cache-images o minikube v 0.34.1 on linux ( amd64 ) $ Caching images in the background ... > Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... - ' minikube ' IP address is 192.168.99.100 - Configuring CRI-O as the container runtime ... - Preparing Kubernetes environment ... : Waiting for image caching to complete ... - Pulling images required by Kubernetes v 1.13.3 ... - Launching Kubernetes v 1.13.3 using kubeadm ... - Configuring cluster permissions ... - Verifying component health ..... + kubectl is now configured to use ' minikube ' = Done ! Thank you for using minikube ! . should be : : $ export MINIKUBE_REASONABLE_OUTPUT = true $ minikube start -- container-runtime = cri-o -- cache-images minikube v 0.34.1 on linux ( amd64 ) Caching images in the background ... Creating virtualbox VM ( CPUs = 2 , Memory = 2048MB , Disk = 20000MB ) ... ' minikube ' IP address is 192.168.99.100 Configuring CRI-O as the container runtime ... Preparing Kubernetes environment ... Waiting for image caching to complete ... Pulling images required by Kubernetes v 1.13.3 ... Launching Kubernetes v 1.13.3 using kubeadm ... Configuring cluster permissions ... Verifying component health ..... kubectl is now configured to use ' minikube ' Done ! Thank you for using minikube ! . These symbols provide no useful information and clutter the output .	0	1
2 2 1.4 2.0 1.3 1.5 1.3 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 4 5 27	[ FEATURE ] allow to set container-runtime via config . I know there is a < URL > : minikube start -- container-runtime = containerd . for start to use containerd but I would like to save the option to persistent config in hy $HOME directory along with other options I use all the time like cpu , memory , kubenertes-version , and vm-driver The exact command to reproduce the issue : : minikube config set container-runtime containerd . The full output of the command that failed : 棣冩寴 Set failed : property name ' container-runtime ' not found 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL >	0	1
1 1 1.0 1.0 1.1 1.0 1.1 1.0 1.3333333333333333 1.0 1.1229508196721312 122 1.0 0 1 4 18	The check for memory limits is currently broken for cgroups v1 . Currently it checks ' ( empty string ) , if cgroup2 is false : : // HasMemoryCgroup checks whether it is possible to set memory limit for cgroup . func HasMemoryCgroup () bool { memcg : = true if runtime . GOOS = = ' linux ' { var memory string if cgroup2 , err : = IsCgroup2UnifiedMode (); err = = nil && cgroup2 { memory = ' /sys/fs/cgroup/memory/memsw . limit_in_bytes ' } if _ , err : = os . Stat(memory ); os . IsNotExist(err ) { klog . Warning('Your kernel does not support memory limit capabilities or the cgroup is not mounted . ' ) memcg = false } } return memcg } . Since 7b0bf57f4c9b87a7b0c9362bdfbb5c0add2094d7 Most likely the code is also wrong for cgroups v2 as well ? The end result is no memory limits , for the docker driver . Seems to be the same issue as in #10935  	1	0
1 1 1.2 1.0 1.0 1.0 1.1 1.0 1.3333333333333333 1.0 0.3333333333333333 9 0.0 2 2 6 25	  Create public charts for time-to-k8s . Create a GitHub Action workflow that runs time-to-k8s benchmark daily against HEAD Store the results of the job as JSON and upload it to bucket Use the data in the bucket to generate a chart and upload it to the website	0	2
0 0 0.6 0.0 1.0 1.0 1.0 1.0 0.3333333333333333 0.0 1.135135135135135 74 1.0 0 2 7 22	Supply package cache for minikube container runtimes . Since we can't rely on the vendor packages being available , we should make sure to host our own package cache ? < URL > This would mostly include the container runtimes , but also other packages we install that are not in the distribution . Docker ( docker.com, < URL > | < URL > docker-ce docker-ce-client containerd.io Podman ( opensuse.org, < URL > podman cri-o conmon CRI cri-tools CNI plugins There are distribution packages available for docker/containerd/runc , but we're not happy about the versions ... ~ ~ Currently using a personal package account on bintray.io ( for podman and crio ) , but that is not maintainable ... ~ ~ ~ < URL > ~ ( 1.9.3 ) ~ < URL > ~ ( 1.18.2 ) We probably also need to document and automate the packaging building better , currenly using : pbuilder . . For the Docker packages we don't know if we can build them from source code at all ( that would be Moby ) Compare < URL >	2	1
0 0 1.0 1.0 0.9 1.0 0.7 0.5 0.6666666666666666 1.0 1.2222222222222223 9 1.0 0 6 7 28	Running containers with podman doesn't work . We install the : podman . tool into the VM , to do things such as loading or building OCI images . But currently there are some issues with the CNI configuration , if you try to use it for running : : $ minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_) | |/') _ _ | |_ __ /' _ ` _ `/| |/' _ `/| || , < ( ) ( )| ' _`/ /'__`/ | ( ) ( ) || || ( ) || || |/`/ | (_) || |_) )( ___/ (_) (_) (_) (_) (_) (_) (_) (_) (_) `/___/'(_ , __/'`/____) $ sudo podman run -it busybox Trying to pull docker.io/library/busybox ... Getting image source signatures Copying blob 53071b97a884 done Copying config 64f5d945ef done Writing manifest to image destination Storing signatures Error : error parsing CNI plugin result ' IP4 :{ IP :{ IP : 10.1.0.7 Mask : ffff0000 } Gateway : 10.1.0.1 Routes :[{ Dst :{ IP : 0.0.0.0 Mask : 00000000 } GW : 10.1.0.1 }]} , DNS :{ Nameservers :[ ] Domain : Search :[ ] Options:[]}': cannot convert version [' ' 0.1.0 ' ' 0.2.0 ' ] to 0.4.0 : cannot convert version [' ' 0.1.0 ' ' 0.2.0 ' ] to 0.4.0 . Even though it is not the primary use-case ( normally use crictl/crio ) , this should work ... Maybe we can get the standard minikube CNI configuration compatible with Podman ?	2	0
2 2 1.0 1.0 0.8 0.5 1.0 1.0 0.6666666666666666 0.0 0.65 20 0.0 0 1 2 22	  Add explanations for all minikube error codes . We now populate documentation for all error/exit codes at < URL > but most of them don't have a comment explaining what they are . We should add those .	0	2
2 2 0.6 0.0 0.9 1.0 1.0 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 1 5 21	PersistentVolume claim doesn't work inside a namespace . System Info : Ubuntu 16.04.3 LTS kubectl client v 1.13.2 kubectl server v 1.13.4 minikube version : v 0.33.1 . Steps to reproduce create a namespace and then try to use a persistentvolumeclaim within that namespace Reproduced using < URL > : minikube start -- vm-driver none -- kubernetes-version v 1.13.4 -- memory 5120 -- cpus = 4 kubectl create namespace testing kubectl create -n testing -f < URL > kubectl create -n testing -f < URL > kubectl describe -n testing pod/cassandra-0 . Expected Result : it creates the persistentvolume and it gets assigned Actual Result : : Events : Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 26s ( x7 over 26s ) default-scheduler pod has unbound immediate PersistentVolumeClaims Normal Scheduled 26s default-scheduler Successfully assigned testing/cassandra-0 to minikube Normal Pulling 19s ( x2 over 25s ) kubelet , minikube pulling image ' gcr.io/google-samples/cassandra:v13 ' Normal Pulled 19s ( x2 over 25s ) kubelet , minikube Successfully pulled image ' gcr.io/google-samples/cassandra:v13 ' Normal Created 19s ( x2 over 25s ) kubelet , minikube Created container Normal Started 19s ( x2 over 25s ) kubelet , minikube Started container Warning BackOff 14s kubelet , minikube Back-off restarting failed container .	2	0
0 0 0.6 0.0 0.8 0.5 1.2 1.0 1.0 1.0 1.1774193548387097 62 1.0 3 10 21 55	handle more error type for oci . when docker is restarting ... and minikube starts we error this way : ' Error response from daemon : dial unix docker . raw . sock : connect : connection refused ' : make : `out/minikube ' is up to date . 棣冩 minikube v 1.7.3 on Darwin 10.13.6 閴?Using the docker ( experimental ) driver based on user configuration 閳跨媴绗?' docker ' driver reported an issue : exit status 1 棣冩寱 Suggestion : Docker is not running or is responding too slow . Try : restarting docker desktop . 棣冩暉 Creating Kubernetes in docker container with ( CPUs = 2 ) ( 0 available ) , Memory = 2000MB ( 0MB available ) ... 棣冩寴 Unable to start VM . Please investigate and run ' minikube delete ' if possible : creating host : create : creating : create kic node : creating volume for minikube container : output Error response from daemon : dial unix docker . raw . sock : connect : connection refused : exit status 1 .	0	0
2 2 1.2 1.0 1.1 1.0 1.0 1.0 1.0 1.0 0.0 0 0.0 4 8 8 25	Mac M1 : hyperkit not supported . . Mac M1 Steps to reproduce the issue : arch -x86_64 brew install hyperkit minikube start -- vm = true minikube start -- driver = hyperkit Full output of failed command : $ minikube start -- driver = hyperkit 棣冩 minikube v 1.21.0 on Darwin 11.4 ( arm64 ) 閴?Using the hyperkit driver based on user configuration 閴?Exiting due to DRV_UNSUPPORTED_OS : The driver ' hyperkit ' is not supported on darwin/arm64	2	1
2 2 1.8 2.0 1.4 2.0 1.2 1.5 1.6666666666666667 2.0 0.75 4 0.5 0 11 19 55	[ Bump-up ] VolumeSnapshot from v1beta1 to v1(GA ) . /area addons Minikube can use VolumeSnapshot as an addon . Its version is v1beta1 api version . < URL > In Kubernetes v 1.20 , the VolumeSnahot moved to GA(v1 ) . < URL > We need to upgrade volumesnapshot versions .	0	1
0 0 0.6 0.0 1.0 1.0 0.95 1.0 0.3333333333333333 0.0 0.0 0 0.0 0 0 0 1	Support For Windows Containers . Feature Request Currently , I know that Minikube is a single node cluster , and that node is Linux so it wont be able to run Windows Containers , but it would be better if Minikube is able to run Windows Containers also by being able to add a Windows node .	2	1
0 0 1.2 1.0 1.0 1.0 1.0 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 2 2 15	minikube start -- image-repository with port does not work without scheme . Related to PR < URL > which was release on v 1.23.0 Steps to reproduce the issue : 1 . Run command : minikube start -- image-repository < repository > : < port > . for example : : minkube start -- image-repository repository : 8080 . 2 . Observe the output : The -- image-repository flag your provided contains Scheme : repository , which will be removed automatically . and on the next line `Using image repository ' without the repository . If I run : minkube start -- image-repository < URL > . it complains about the scheme but trims it out and works with : repository : 8080 . . Running minikube without port also works without the schema . I'll try to have by the EOD a PR to address this issue .	0	0
0 0 0.6 0.0 0.9 1.0 0.9 1.0 0.3333333333333333 0.0 1.1627906976744187 43 1.0 3 8 19 80	Upgrade Buildroot to 2020.02 LTS . The current release is nearing EOL . We should upgrade to the next LTS . Notice that the 2019.02 . x series is now end of life unless somebody steps up to take over maintenance . Please migrate to the 2020.02 series instead which will be supported until April 2021 . ' Buildroot 2020.02 released ' < URL > ' Buildroot 2019.02.11 released , 2019.02 . x is EOL ' < URL > This will feature upgrade Linux to 5.4 ( ~ 4.19 ~ ) and systemd to 244 ( ~ 240 ~ ) Also upgrades Binutils to 2.32 ( ~ 2.30 ~ ) and GCC to 8.4.0 ( ~ 7.5.0 ~ ) and the system Go compiler is bumped from 1.11.13 to 1.13.14 Also need to upgrade VirtualBox , to get guest additions support for Linux 5.4	0	1
1 1 0.8 1.0 0.6 0.0 0.65 0.0 0.3333333333333333 0.0 1.0 2 1.0 0 1 4 35	Minikube status shows paused cluster as running . Steps to reproduce the issue : Pause the cluster . run the following commands in the following versions . : 閴?Downloads . /minikube12-2 status -- layout cluster -- output json {' Name ' : ' minikube ' , ' StatusCode ' : 418 , ' StatusName ' : ' Paused ' , ' BinaryVersion ' : ' v 1.12.2 ' , ' Components ' :{ ' kubeconfig ' :{ ' Name ' : ' kubeconfig ' , ' StatusCode ' : 200 , ' StatusName ' :'}} , ' Nodes ' :[{ ' Name ' : ' minikube ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' , ' Components ' :{ ' apiserver ' :{ ' Name ' : ' apiserver ' , ' StatusCode ' : 418 , ' StatusName ' : ' Paused ' } , ' kubelet ' :{ ' Name ' : ' kubelet ' , ' StatusCode ' : 405 , ' StatusName ' : ' Stopped ' }}}]} 閴?Downloads . /minikube13-1 status -- layout cluster -- output json {' Name ' : ' minikube ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' , ' BinaryVersion ' : ' v 1.13.1 ' , ' Components ' :{ ' kubeconfig ' :{ ' Name ' : ' kubeconfig ' , ' StatusCode ' : 200 , ' StatusName ' :'}} , ' Nodes ' :[{ ' Name ' : ' minikube ' , ' StatusCode ' : 200 , ' StatusName ' : ' OK ' , ' Components ' :{ ' apiserver ' :{ ' Name ' : ' apiserver ' , ' StatusCode ' : 418 , ' StatusName ' : ' Paused ' } , ' kubelet ' :{ ' Name ' : ' kubelet ' , ' StatusCode ' : 405 , ' StatusName ' : ' Stopped ' }}}]} . As you can see , the most recent version does not correctly output the status as paused .	0	0
0 0 0.6 0.0 0.6 0.0 0.6 0.0 0.3333333333333333 0.0 0.0 0 0.0 0 4 7 30	UI progress bars stepping on each other . I noticed that : preloaded-images-k8s . and : index.docker.io/kicbase . progress bars step on each other when download is in progress and it is confusing for the user and might be interpreted as a glitch . When the download is finished , they are displayed in separate lines : : minikube start -- driver = docker 棣冩 minikube v 1.19.0 on Darwin 10.15.7 閴?Using the docker driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩 Pulling base image ... 棣冩崙 Downloading Kubernetes v 1.20.2 preload ... > gcr.io/k8s-minikube/kicbase ... : 357.67 MiB / 357.67 MiB 100.00% 396.74 K > preloaded-images-k8s-v10-v1 ... : 491.71 MiB / 491.71 MiB 100.00% 411.47 K > index.docker.io/kicbase/sta ... : 357.67 MiB / 357.67 MiB 100.00% 417.22 K .	2	0
0 0 0.4 0.0 1.0 1.0 1.2 1.5 0.3333333333333333 0.0 1.1041666666666667 96 1.0 5 8 20 69	feature : add doc URL to the addon list table . while reviwing this PR : < URL > since the URL will be different based on driver , it would be nice that if we have add the URL of the addon to the list , so they dont have to dig into the documentaiton and pain to find the URL for the regsiry : medmac@ ~ /Desktop/md1 $ . /minikube-darwin-amd64 addons list | -----------------------------|---------- | -------------- | | ADDON NAME | PROFILE | STATUS | | -----------------------------|---------- | -------------- | | dashboard | minikube | disabled | | default-storageclass | minikube | enabled 閴?| | efk | minikube | disabled | | freshpod | minikube | disabled | | gvisor | minikube | disabled | | helm-tiller | minikube | disabled | | ingress | minikube | disabled | | ingress-dns | minikube | disabled | | istio | minikube | disabled | | istio-provisioner | minikube | disabled | | logviewer | minikube | disabled | | metrics-server | minikube | disabled | | nvidia-driver-installer | minikube | disabled | | nvidia-gpu-device-plugin | minikube | disabled | | registry | minikube | enabled 閴?| | registry-aliases | minikube | disabled | | registry-creds | minikube | disabled | | storage-provisioner | minikube | enabled 閴?| | storage-provisioner-gluster | minikube | disabled | | -----------------------------|---------- | -------------- | .  	2	2
2 2 0.8 0.0 1.3 2.0 0.95 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 5 10 31	Minikube keeps on reporting ' Unable to fetch image ' from remote server while I have the image set up locally . Steps to reproduce the issue : I'm testing with this repo : < URL > And what I did was : Configure docker to use minkube's registry by : : eval $(minikube -p minikube docker-env ) . Build the image with : docker build -t docker.io/test/helloworld-python . . Now if I run : : minikube image list . I will see : docker.io/test/helloworld-python:latest in the list . I have to update the yaml file accordingly by making the lines : : - image : docker.io/test/helloworld-python:latest imagePullPolicy : Never . But when I deploy the service with : : kubectl create -f service . yaml . I keep on getting the same error from the log : : Unable to fetch image ' docker.io/test/helloworld-python:latest ' : failed to resolve image to digest : HEAD < URL > unexpected status code 401 Unauthorized ( HEAD responses have no body , use GET for details ) . Is there a document somewhere that details how local registery works with minkube ? There seems to be scattered information all over the interent but not sure about the official steps . Below is the version I use : : minikube version : v 1.19.0 commit : 15cede53bdc5fe242228853e737333b09d4336b5 . And I'm using Ubuntu 20.04 kernel 5.4.0 -72-generic Full output of : minikube logs . command : Full output of failed command :	2	0
0 0 1.4 2.0 1.5 2.0 1.0 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 0 5 28	minikube doesn't start in VM with no virtualisation support . I'm not sure if this is a bug or a feature request . We have available corporate VMs which have virtualisation support in the ( virtual ) CPU disabled . I have tried for a week to get minikube with docker running , and each time I fix one blocker , there's another waiting . This should be easy . :-) But it's not . Please include this scenario in the test sets for minikube . I'm working on Ubuntu 16.04 , with the latest ( as of yesterday ) minikube .  	2	2
0 0 1.0 1.0 1.2 1.5 1.25 1.5 0.6666666666666666 0.0 1.0 7 1.0 0 0 0 11	hyperkit : `minikube ip` shows an IP even if VM is stopped . As reported on #minikube by @dhs227 . It's noteworthy that : status . shows that minikube is stopped : : [ ~ /c/m/hellonode ] : minikube status 10:40:28 PM minikube : Stopped cluster : kubectl : .	2	0
0 0 1.0 1.0 1.0 1.0 0.95 1.0 1.0 1.0 0.0 0 0.0 2 3 5 22	Documentation : change registries title from ' docker on macOS ' to ' docker on Linux/macOS ' . Isn't it worth mentioning something about Linux ? I managed to make the private registry work on my Ubuntu 20.04 machine by following the steps listed in ' docker on macOS ' . Maybe it would be interesting to change that title to ' Docker on Linux/macOS ' . That said , I haven't followed the whole handbook . I just read the ' Registries ' page .  	2	2
2 2 1.6 2.0 1.4 1.5 1.4 2.0 1.6666666666666667 2.0 0.9807692307692307 52 1.0 1 3 7 31	Add -- dry-run flag to surface configuration warnings . For embedding within an API , it would be handy to have a dry run mode to surface issues a user may have with the current configuration . For example : Warning if the user will need sudo permissions : minikube start -- dry-run . Resource warnings ( RAM , CPU ) Basically , anything that would normally cause : minikube start . to exit due to an invalid configuration should do so and output why - and do so quickly .   	1	1
0 0 1.2 2.0 1.2 2.0 1.15 1.5 1.3333333333333333 2.0 0.0 0 0.0 0 0 1 5	Unable to enable AppArmor in minikube ( works in minishift ) . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : Both Mac and Ubuntu What happened : I tried from minikube 0.28 - 0.23 , they all don't work After I : minikube start . a kubernetes cluster . the apparmor should be enabled by default . But after I : minikube ssh . to the node and check , the apparmor is not installed and enabled . And when I deploy any kube app , then I : kube exec . to access the pod of the kube app , I also cannot see the apparmor is enabled in the pod . But I tried minishift today , they enable by default . I also found we can enable it like this : < URL > But I tried : : minikube start -- feature-gates = AppArmor = true . and : minikube start -- feature-gates AppArmor = true . , they all don't work . What you expected to happen : after : minikube start . , the apparmor should be enabled by default . Or can you please tell me how to enable : apparmor . for my minikube ? Thanks a lot !	2	0
1 1 1.8 2.0 1.8 2.0 1.45 2.0 1.6666666666666667 2.0 1.0 1 1.0 1 3 8 30	Feature request : Allow customizing host path used for dynamically provisioned PersistentVolumes . Dynamic persistent volume provisioning works great , but uses paths like : /tmp/hostpath-provisioner/pvc-433c40c9-7cf1-40a8-91ab-f9210dceec50 . which are not easy to remember and type . For development and testing , it would be much more convenient if the path could be customized to something along the lines of : : /data/ < claim > . This would make a PVC named : mysqldb . resolve to an automatically generated PV that stores the data in : /data/mysqldb . . Much easier to remember and type ! :)	2	1
0 0 0.2 0.0 0.9 0.5 0.75 0.0 0.3333333333333333 0.0 0.9032258064516129 186 1.0 3 14 18 42	support ancient versions of docker again ( template parsing issue ) . we broke it since 16.01 the ancient verions of docker 18.09.7 fails at template parse : cmd : = exec . Command(Docker , ' network ' , ' inspect ' , name , ' -- format ' , `{'Name ' : ' {{ . Name }}' , ' Driver ' : ' {{ . Driver }}' , ' Subnet ' : ' {{ range . IPAM . Config }}{{ . Subnet}}{{end }}' , ' Gateway ' : ' {{ range . IPAM . Config }}{{ . Gateway}}{{end }}' , ' MTU ' : {{ if ( index . Options ' com . docker . network . driver . mtu')}}{{(index . Options ' com . docker . network . driver . mtu')}}{{else}}0{{end }} , {{$first : = true }} ' ContainerIPs ' : [{{ range $k , $v : = . Containers }}{{ if $first}}{{$first = false}}{{else }} , {{ end}}'{{$v . IPv4Address}}'{{end}}]}`) . errors out : : Template parsing error : template : : 1 : unexpected ' = ' in operand .	0	0
2 2 1.6 2.0 1.1 1.0 1.05 1.0 1.3333333333333333 1.0 0.0 0 0.0 0 1 2 26	MountVolume . SetUp failed for volume : couldn't propagate object cache : timed out waiting for the condition . < I have a problem after exposing an image in kubernetes . As soon as I checked the minikube dashboard an error occurred . Later on when I back to minikube dashboard later it was working fine without error . > kubectl expose deployment hello-node -- type = LoadBalancer -- port = 8080 : from minikube dashboard this error was found on the pod for the image I exposed : MountVolume . SetUp failed for volume ' defaul t-t oken-kd9zk ' : couldn't propagate object cache : timed out waiting for the condition minikube version : v 1.2.0 ubuntu 18.04	2	0
1 1 1.4 2.0 1.1 1.5 1.0 1.0 1.6666666666666667 2.0 1.1111111111111112 9 1.0 2 11 19 57	Minikube ( re)start on docker driver breaks docker-env . : minikube start -p docker3 -- container-runtime docker -- vm-driver = docker . Restarting caused some issue with the IP ( minikube docker-env command ) that was working before - this is a skaffold log output : : ' docker load ' command failed with error : error during connect : Post < URL > EOF . : minikube version 6.7 s 椤?Thu Feb 27 10:59:00 2020 minikube version : v 1.7.3 commit : 436667c819c324e35d7e839f8116b968a2d0a3ff . Originally posted by @balopat in < URL >	0	0
2 2 1.6 2.0 1.5 2.0 1.25 1.5 2.0 2.0 0.0 0 0.0 1 1 2 17	CRI-O config (/etc/crio/crio . conf ) should not be hardcoded ( nor overwritten without a prompt ) . With : minikube start -- vm-driver = none -- container-runtime = crio . , minikube attempts to overwrite any pre-existing CRI-O configuration at : /etc/crio/crio . conf . with a hardcoded template . First , this has a potential for user data loss ( if the user has a nontrivial configuration in place ) . Second , this makes minikube/crio unusable on any systems that do not adhere to the hardcoded paths ( e . g . Arch Linux , which has : /usr/bin/conmon . and : /usr/lib/cni . instead of : /usr/libexec/crio/conmon . and : /opt/cni/bin . ) . Finally , this makes it impossible to use any container runtime other than : crun . ( which is often the motivation to use CRI-O in the first place ) .  	1	0
0 0 1.0 1.0 1.4 2.0 1.35 2.0 0.3333333333333333 0.0 1.15 40 1.0 4 12 17 50	The podman driver should not require sudo or root . Even though podman is run with sudo ( docker uses a group instead ) , the driver should not . : 閴?Using the podman ( experimental ) driver based on user configuration 棣冩寴 The ' podman ' driver requires root privileges . Please run minikube using ' sudo minikube -- driver = podman ' . . This will lead to the same kind of ownership and path issues that is plaguing the none driver ... Instead only the : podman . commands should be wrapped in : sudo . ( to not try to run as rootless )  	1	0
2 2 0.8 0.0 0.9 0.5 0.9 0.5 1.3333333333333333 2.0 0.0 2 0.0 0 0 5 24	docker-machine-driver-kvm2_ 1.22.0 -0_arm64 . deb has wrong package architecture . Steps to reproduce the issue : Download : docker-machine-driver-kvm2_ 1.22.0 -0_arm64 . deb . from the v 1.22.0 release Try to install it on an arm64 system , or ( my initial case ) try to add it to an apt repo that hosts arm64 packages Errors out because the package internally says that it's : aarch64 . when it should be : arm64 . The : minikube . package correctly has the : arm64 . architecture . Full output of : minikube logs . command : N/A Full output of failed command : : Error looking at ' docker-machine-driver-kvm2_ 1.22.0 -0_arm64 . deb ' : ' aarch64 ' is not one of the valid architectures : ' i386 amd64 armhf arm64 source ' . : new Debian package , version 2.0 . size 4649436 bytes : control archive = 423 bytes . 446 bytes , 12 lines control Package : docker-machine-driver-kvm2 Version : 1.22.0 Section : base Priority : optional Architecture : aarch64 Depends : libvirt0 (>= 1.3.1 ) Recommends : minikube Maintainer : Thomas Str鏋歮berg < t+minikube@stromberg.org > Description : Machine driver for KVM minikube uses Docker Machine to manage the Kubernetes VM so it benefits from the driver plugin architecture that Docker Machine uses to provide a consistent way to manage various VM providers . .	0	0
1 1 1.6 2.0 1.2 1.0 1.0 1.0 1.6666666666666667 2.0 0.0 0 0.0 1 3 9 35	Minikube image zsh completion broken . The : minikube image . tab completion on ZSH doesn't work : : $ minikube image < tab here > _minikube_image : 10 : bad math expression : operand expected at `'ls''tion-name: zsh : do you wish to see all 195 possibilities ( 98 lines ) ? . For some reason , part of the previously added image name is part of the error message ( : application-name . )  	1	0
2 2 1.6 2.0 1.6 2.0 1.4 2.0 1.3333333333333333 2.0 0.0 0 0.0 3 5 6 33	VM starts but the download of `preloaded-images-k8s` was not completed . ( This is not a regular issue , but a support request ) Steps to reproduce the issue : : minikube start minikube delete minikube start . 棣冩 minikube v 1.19.0 on Darwin 10.14.6 閳?MINIKUBE_ACTIVE_DOCKERD = minikube 閴?Using the virtualbox driver based on user configuration 棣冩啢 Starting control plane node minikube in cluster minikube 棣冩崙 Downloading Kubernetes v 1.20.2 preload ... > preloaded-images-k8s-v10-v1 ... : 308.70 MiB / 491.71 MiB 62.78% 84.05 KiB 棣冩暉 Creating virtualbox VM ( CPUs =3 , Memory = 5120MB , Disk = 20000MB ) ... ... Questions : Does Minikube has any way to avoid re-downloading the : preloaded-images-k8s . , every time that a VM is created ? Where does Minikube stores the : preloaded-images-k8s . ? / So I can manually copy it before : delete . and restore it before : start . . I am having connectivity problems and I would like to reuse the images that were already downloaded on the first execution of : minikube start . , when re-creating the Virtual Machine . Is there any way to achieve this ?	2	0
0 0 1.4 2.0 1.2 1.5 0.95 1.0 1.0 1.0 0.0 1 0.0 1 2 4 18	  Broken hyperlinks at < URL > . < URL > lists methods for uploading images : 1 . docker-env command which is a hyperlink to < URL > 2 . podman-env command , which is a hyperlink to < URL > . and so on These and the following hyperlinks do not work . The correct hyperlinks would have been < URL > < URL > < URL >	0	2
2 2 0.8 0.0 1.0 1.0 0.9 1.0 1.3333333333333333 2.0 1.2105263157894737 38 1.0 4 8 11 47	update-context with missing entry : panic : runtime error : invalid memory address or nil pointer dereference . minikube 1.9.2 , ubuntu 16.04 , virtualbox : $ minikube status E0405 16:02:32 . 312506 22446 status . go : 233 ] kubeconfig endpoint : extract IP : ' minikube ' does not appear in /home/anders/ . kube/config m01 host : Running kubelet : Running apiserver : Running kubeconfig : Misconfigured WARNING : Your kubectl is pointing to stale minikube-vm . To fix the kubectl context , run `minikube update-context` $ minikube update-context panic : runtime error : invalid memory address or nil pointer dereference [ signal SIGSEGV : segmentation violation code = 0x1 addr = 0x18 pc = 0x148ecb0 ] goroutine 1 [ running ]: k8s.io/minikube/pkg/minikube/kubeconfig.UpdateEndpoint(0x1a823d1 , 0x8 , 0xc00050a1b0 , 0xe , 0x20fb , 0xc0000437e0 , 0x19 , 0x1d82160 , 0xc00012c060 , 0xc00050a1b0 ) /app/pkg/minikube/kubeconfig/kubeconfig . go : 122 +0x290 k8s.io/minikube/cmd/minikube/cmd.glob .. func30(0x2a6e180 , 0x2aa53b0 , 0x0 , 0x0 ) /app/cmd/minikube/cmd/update-context . go : 37 +0x10a github.com/spf13/cobra.(* Command ) . execute(0x2a6e180 , 0x2aa53b0 , 0x0 , 0x0 , 0x2a6e180 , 0x2aa53b0 ) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:830 +0x2aa github.com/spf13/cobra.(* Command ) . ExecuteC(0x2a6c380 , 0x0 , 0x1 , 0xc00040f500 ) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:914 +0x2fb github.com/spf13/cobra.(* Command ) . Execute (...) /go/pkg/mod/ github.com/spf13/cobra@v0.0.5/command.go:864 k8s.io/minikube/cmd/minikube/cmd.Execute () /app/cmd/minikube/cmd/root . go : 108 +0x6a4 main . main () /app/cmd/minikube/main . go : 66 +0xea .	0	0
0 0 0.4 0.0 0.8 0.5 0.95 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 1 3 13	multi-nodes-minikube loss node label after restart . Steps to reproduce the issue : minikube start -n 2 kubectl label node minikube-m02 nsid_public = open -- overwrite minikube stop minikube start kubectl get node -A -o wide -- show-labels : kubectl get node -A -o wide -- show-labels NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME LABELS minikube Ready control-plane , master 29d v 1.20.2 192.168.49.2 < none > Ubuntu 20.04.2 LTS 4.14.81 . bm . 26-amd64 docker :/ / 20.10.6 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=c61663e942ec43b20e8e70839dcca52e44cd85ae,minikube.k8s.io/name=minikube,minikube.k8s.io/updated_at=2021_05_20T13_05_08_0700,minikube.k8s.io/version=v1.20.0,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master= minikube-m02 Ready < none > 9d v 1.20.2 192.168.49.3 < none > Ubuntu 20.04.2 LTS 4.14.81 . bm . 26-amd64 docker :/ / 20.10.6 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube-m02,kubernetes.io/os=linux .  	1	0
0 0 1.0 1.0 1.1 1.0 0.85 1.0 0.6666666666666666 0.0 1.2432432432432432 37 1.0 3 4 6 43	Not possible to translate new start message . The string ' control plane ' is hard-coded : : cp : = ' if apiServer { cp = ' control plane ' } out . T(out . ThumbsUp , ' Starting {{ . controlPlane}}node {{ . name }} in cluster {{ . cluster }}' , out . V{'controlPlane ' : cp , ' name ' : n . Name , ' cluster ' : cc . Name }) . So it comes out as a mix with english : : 棣冩啢 Startar control plane nod m01 i kluster minikube . ( on a side note , maybe the names ' m01 ' and ' minikube ' should be in quotes ? ) Unfortunately , the translation framework strips out any trailing spaces in strings . So if you try to wrap it in : translate . T . , the string comes out as : ' control plane ' . Probably the best is to use two separate strings , even if it is a little redundant . : if apiServer { out . T(out . ThumbsUp , ' Starting control plane node {{ . name }} in cluster {{ . cluster }}' , out . V{'name ' : n . Name , ' cluster ' : cc . Name }) } else { out . T(out . ThumbsUp , ' Starting node {{ . name }} in cluster {{ . cluster }}' , out . V{'name ' : n . Name , ' cluster ' : cc . Name }) } .	0	0
0 0 1.0 1.0 1.2 1.5 1.0 1.0 0.6666666666666666 0.0 0.0 2 0.0 0 1 5 19	Implement -- force-systemd into cri-o . Minikube version 1.23.1 does not START work with crio and podman , because crio does not switch to systemd cgroup manager . According to < URL > : minikube start -- force-systemd . and : minikube start -- force-systemd = true . is supposed to enforce systemd閳ユ獨 cgroup manager and be workaround for < URL > However , : minikube start -- force-systemd = true . does not start , which means that cgroup is not enforced .	0	1
0 0 1.0 1.0 0.8 0.5 0.9 1.0 1.3333333333333333 2.0 0.0 0 0.0 5 8 18 67	Podman is OOM-killed when loading images . Steps to reproduce the issue : : minikube start -- container-runtime = crio -- disk-size = 100GB -- memory = 3900m -- driver = kvm2 . ( on minikube VM ) : watch -d ' df -h ' . : podman save < image > | minikube ssh ' sudo podman load ' . Repeat , tmpfs mounted on / fills , eventually podman will be killed inside the minikube VM Maybe I'm misunderstanding how the CRI-O driver works , but it seems like : podman . is using : /var/lib/containers/storage . as the GraphRoot instead of : /mnt/vda1/var/lib/containers/storage . . Full output of : minikube start . command used , if not already included : : 棣冩 minikube v 1.9.2 on Ubuntu 18.04 閴?Using the kvm2 driver based on user configuration 棣冩啢 Starting control plane node m01 in cluster minikube 棣冩暉 Creating kvm2 VM ( CPUs = 2 , Memory = 3900MB , Disk = 102400MB ) ... 棣冨返 Preparing Kubernetes v 1.18.0 on CRI-O 1.17.1 ... 棣冨皞 Enabling addons : default-storageclass , storage-provisioner 棣冨及 Done ! kubectl is now configured to use ' minikube ' . dmesg output : : [ 340.592651 ] Out of memory : Kill process 10107 ( podman ) score 9 or sacrifice child [ 340.592696 ] Killed process 10107 ( podman ) total-vm : 830324kB , anon-rss : 7212kB , file-rss : 4kB , shmem-rss : 28888kB [ 340.594962 ] oom_reaper : reaped process 10107 ( podman ) , now anon-rss : 0kB , file-rss : 0kB , shmem-rss : 0kB .   	1	0
1 1 1.4 2.0 1.3 1.5 1.2 1.0 1.0 1.0 0.0 0 0.0 1 3 10 56	Using a proxy with containerd runtime . Now I use the default container runtime ( : docker . ) in minikube . And at the start , I specify the option to use a proxy : : minikube start -- docker-env HTTP_PROXY = < my proxy > . But what about using : containerd . ? How can I specify the environment setting in this case ? : minikube start -- container-runtime = containerd . In production we are already using a containerd . Therefore , I need to use it in minikube too . I only found < URL > when it is used without minikube .	2	1
0 0 0.6 1.0 1.0 1.0 1.15 1.0 0.6666666666666666 1.0 1.0 2 1.0 0 0 0 2	Create installer to remove the necessity for users to download additional binaries . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): Feature Request The current installation process for minikube is : Download and install a hypervisor driver ( kvm , xhyve , hyper-v ) Set up hypervisor driver ( setup qemu-kvm , root own xhyve driver , set switches on hyper-v ) Download and install minikube We've tried in the past to vendor the drivers directly - unfortunately there were two issues : xhyve driver - must have root permissions , calls the hyperkit CLI which os . Exits kvm driver - dynamically linked against libvirt.so, which can't be statically linked The feature request is that we change the installation process to include the non-virtualbox drivers for each OS , xhyve and kvm . The extraction script will extract and set up the drivers correctly . Then , the installation process would be Download and extract minikube . tar Run script to set up hypervisor driver Of course , package managers could automate this process .  	1	1
2 2 1.6 2.0 1.4 2.0 1.45 2.0 2.0 2.0 0.0 0 0.0 0 0 0 1	Proposal : Suggest NFS as default filesystem for local dev . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Please provide the following details : Environment : Minikube version : v 0.21.0 - OS : OS X - VM Driver : Virtualbox - ISO version : minikube-v 0.23.0 . iso - Install tools : brew - Others : What happened : We noticed our application ran significantly slower in k8s/minikube compared to vagrant . We changed our local files to mount via NFS instead of 9p and our dev system is now much faster than vagrant . What you expected to happen : We should update the docs to recommend using NFS for local dev instead of 9p , at least for OS X , but probably makes sense on linux , too . We're going to test on Windows shortly to see if it has the same effect which I assume it will . How to reproduce it ( as minimally and precisely as possible ): Set up a persistent volume store using NFS and use it instead of 9p : 1 ) Update your : /etc/exports . to allow minikube to access it : : $ echo ' /Users -alldirs -mapall='$(id -u )': ' $(id -g )' 192.168.99.100 ' | sudo tee -a /etc/exports . : apiVersion : v1 kind : PersistentVolume metadata : name : nfs-volume spec : capacity : storage : 15Gi accessModes : - ReadWriteMany persistentVolumeReclaimPolicy : Retain storageClassName : standard nfs : server : 192.168.99.1 path : /Users/Shared/Sites/ --- kind : PersistentVolumeClaim apiVersion : v1 metadata : name : cr-volume spec : storageClassName : standard accessModes : - ReadWriteMany resources : requests : storage : 15Gi . Anything else do we need to know : If you think this is a good idea , I'll make a PR for the update docs . It's been life changing for our devs .  	2	2
0 0 0.6 0.0 1.1 1.0 1.2 1.5 0.6666666666666666 0.0 0.0 0 0.0 0 1 15 45	minikube kubect : permission denied . After upgrading to minikube v . 1.7.2 I'm not longer able to use : minikube kubectl . . The exact command to reproduce the issue : : rm -rf ~ / . minikube minikube start minikube kubectl . The full output of the command that failed : : Error running /Users/ < USER > / . minikube/cache/v 1.17.2 /kubectl : fork/exec /Users/ < USER > / . minikube/cache/v 1.17.2 /kubectl : permission denied . Contents of the : ~ / . minikube/cache/v 1.17.2 /kubectl . : : ls -l ~ / . minikube/cache/v 1.17.2 -rw ------- 1 < USER > staff 39342080 11 lut 10:27 kubeadm -rw ------- 1 < USER > staff 43491328 11 lut 10:27 kubectl -rw ------- 1 < USER > staff 111568408 11 lut 10:27 kubelet . : file ~ / . minikube/cache/v 1.17.2 /kubectl /Users/ < USER > / . minikube/cache/v 1.17.2 /kubectl : ELF 64-bit LSB executable , x86-64 , version 1 ( SYSV ) , statically linked , Go BuildID = itoADCJwxOm7z3ymMN2Z/yoeLs_MYMz8Q5D0z_bWX/K9sqErNfMu6XJcqYVzr4/nHZFbmivkI7eipSyzj5i , stripped . The operating system version : macOS Catalina 10.15.3 ( 19D76 ) It looks like minikube downloaded executables for Linux operating system , where it should download executable to macOS . Currently the workaround is to delete files from : ~ / . minikube/cache/v 1.17.2 . folder and execute : minikube kubectl . which will download correct executables for macOS .	0	0
1 1 1.2 1.0 1.3 1.5 1.3 1.5 1.3333333333333333 1.0 0.0 0 0.0 2 5 9 30	none : reusing node : detecting provisioner : Too many retries waiting for SSH to be available . Environment : minikube version : : v 1.0.0 . OS : : Ubuntu 16.04 LTS ( Xenial Xerus ) . VM Driver : : none . What happened : ``` Created a VM with none driver , stopped it , then started it again . The VM failed to start and minikube reported that it crashed . : What I expected to happen : the VM created by the first minikube start command is started . Output from the second minikube start command : 棣冩 minikube v 1.0.0 on linux ( amd64 ) 棣冦仚 Downloading Kubernetes v 1.14.0 images in the background ... 棣冩寱 Tip : Use ' minikube start -p < name>' to create a new cluster , or ' minikube delete ' to delete this one . 棣冩敡 Restarting existing none VM for ' minikube ' ... 閳?Waiting for SSH access ... 棣冩寴 Unable to start VM : detecting provisioner : Too many retries waiting for SSH to be available . Last error : Maximum number of retries ( 60 ) exceeded 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > Output from ' sudo minikube start -- alsologtostderr -v = 8 -- vm-driver = none ' : 閳?Waiting for SSH access ... Waiting for SSH to be available ... Getting to WaitForSSH function ... Error getting ssh command ' exit 0 ' : driver does not support ssh commands . To reproduce : sudo minikube start -- vm-driver = none sudo minikube stop sudo minikube start -- vm-driver = none Starting a stopped VM was working in minikube v 0.28 .  	1	0
2 2 1.4 2.0 1.1 1.0 1.0 1.0 1.3333333333333333 2.0 1.1333333333333333 120 1.0 1 3 5 32	  Provide links back to ' Production environment ' , when finished with ' Learning environment ' . Currently the Kubernetes documentation is divided into : < URL > Learning environment < URL > kubectl kind minikube + < URL > ( Katacoda ) kubeadm Production environment < URL > kubeadm kops ( AWS ) kubespray ( Ansible ) cloud We should provide some exit links , how to proceed to production ... Note : these are external resources , outside minikube Too many to mention : < URL > This would be an improvement over trying to run minikube in production . < URL > ( also ~ ~ #10097 ~~) Non-Goals Simplifying Kubernetes production deployment experience Linking back to the regular Kubernetes documentation should be enough .   	1	2
0 0 1.4 2.0 1.5 2.0 1.4 2.0 1.0 1.0 0.0 0 0.0 0 1 1 9	Failed to create symbolic link on mount : Operation not permitted . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): BUG REPORT Please provide the following details : Environment : OSX Minikube version ( use : minikube version . ): v 0.28.2 - OS ( e.g. from /etc/os-release ): Mac OS X High Sierra - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): hyperkit - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): . minikube/cache/iso/minikube-v 0.28.1 . iso - Install tools : What happened : ln : failed to create symbolic link ' test1 ' : Operation not permitted What you expected to happen : I would expect the symbolic link to create without a problem . How to reproduce it ( as minimally and precisely as possible ): : docker run -- rm -it / -- volume ${PWD }: /usr/src/test / -- workdir /usr/src/test / node : latest / bash . : root @deca1a6f4779 : /usr/src/test# touch test root @deca1a6f4779 : /usr/src/test# ln -s test test1 ln : failed to create symbolic link ' test1 ' : Operation not permitted . Output of : minikube logs . ( if applicable ) : Anything else do we need to know :	2	0
2 2 0.8 1.0 1.2 1.0 1.25 1.0 1.0 1.0 0.9548872180451128 133 1.0 2 4 13 26	DEB_REVISION is not propagated into . deb metadata . Are you mentioning the versions reported by : apt . / : dpkg . , or the one reported by : minikube version . ? The former -- the version info in the : . deb . package metadata . A quick check identifies the issue here : < URL > That should be : sed -E -i ' s/--VERSION--/'$(DEB_VERSION)-$(DEB_REVISION)'/g ' ... . Originally posted by @mgabeler -lee-6rs in < URL >	0	0
2 2 1.0 1.0 0.8 1.0 1.25 1.5 1.0 1.0 0.9520958083832335 167 1.0 3 9 12 43	make perfomrance pr-bot to use json output . for more information ask @priyawadhwa  	1	1
2 2 1.4 2.0 1.2 1.5 1.35 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 1 23	Exit with error when Docker version is too old . 10369 dropped the error in the Docker version validation to support old Docker versions . With this change , the users get a confusing : docker info -- format : exit status 2 . error when they're using ancient Docker versions ( < 1.13.0 ) that don't even support : docker info -- version . , see 11480 11538 Should we exit with an error and tell users to upgrade their Docker version when it's way too old ? One option is to define two constants : : minDockerVer . ( e.g. 1.13.0 ) and : recommendedDockerVer . ( 18.09.0 ): 1 . If user Docker version < : minDockerVer . : Exit with error message 2 . If user Docker version < : recommendedDockerVer . : Only show suggestions to upgrade Docker 3 . If user Docker version > = : recommendedDockerVer . : Do nothing  	1	1
1 1 1.0 1.0 1.5 2.0 1.3 1.5 0.6666666666666666 1.0 1.2222222222222223 9 2.0 2 3 3 11	macOS Install User Journey .  	2	2
0 0 1.4 2.0 1.3 2.0 1.2 2.0 1.0 1.0 2.0 1 2.0 1 1 2 6	Retain folder structure when copying files from ~ / . minikube/files . Environment : Minikube version : v 0.26.1 OS : Ubuntu 16.04 VM Driver : virtualbox ISO version : v 0.26.0 What happened : When Minikube copies files onto the VM from ~ / . minikube/files , the folder structure is not retained . What you expected to happen : On the host : : ~ / . minikube/files$ find . | sort . . /abc . /abc/1 . txt . /def . /def/2 . txt . /def/ghi . /def/ghi/3 . txt . However , when the VM is provisioned , all the files are put onto the first directory with content . : $ minikube ssh find /abc /abc /abc/3 . txt /abc/2 . txt /abc/1 . txt . I expect the copied files to match the structure on my host . How to reproduce it : 1 . Create a nested folder structure in : ~ / . minikube/files . with some files . : cd ~ / . minikube/files mkdir -p { abc , def/ghi } touch abc/1 . txt touch def/2 . txt touch def/ghi/3 . txt . Start Minikube with : minikube start -- kubernetes-version v 1.10.0 -- bootstrapper = kubeadm . . After completion , check the VM for the files with : minikube ssh find /abc . .	2	1
2 2 1.4 2.0 1.4 2.0 1.4 2.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 9 24	-- image-mirror-country='cn ' still fetches binary SHA's from Google . Already used : -- image-mirror-country='cn ' . or : -- image-repository . to start minikube , but still failed in the preparing on Docker stage due to fetching sha files from google ap	2	0
2 2 1.6 2.0 1.6 2.0 1.4 2.0 1.3333333333333333 2.0 0.0 0 0.0 2 2 5 42	Add command for a ' Getting Started ' workshop using eduk8s as addon . /kind feature Hello , I was thinking that it would be nice having a katacoda-like tutorial for beginners for local Kubernetes with Minikube , available from cli command and implemented as addon . One good candidate for this could be creating an addon for < URL > , a workshop framework installable via Operator and providing CR for creating interactive content , containing the < URL > 1h workshop ( or creating a new one ) . The flow would be similar to : minikube dashboard . command which activates dashboad addon , in this case it would be : : $ minikube getstarted . or : $ minikube tutorial . activating : eduk8s . addon or : getstarted . addon ( containing eduk8s + workshop ) , opening the browser pointing to eduk8s ingress . This would help people that'd love to learn Kubernetes doing it from their workstation with Minikube , having so a local persistent env were to execute exercises , and taking benefit from storage and ingress components already available , needed by the workshop framework in this case . How does that sound ?	2	1
0 0 1.0 1.0 1.0 1.0 1.25 1.5 0.3333333333333333 0.0 0.45454545454545453 22 0.0 3 5 9 23	Add validatePodmanEnv subtest for crio runtime to TestFunctional . This goes hand in hand with validateDockerEnv which currently runs for the docker runtime  	1	1
2 2 1.0 1.0 1.0 1.0 1.15 1.0 1.6666666666666667 2.0 2.0 1 2.0 2 6 8 40	unknown runtime type : ' nvidia ' . I have installed the nvidia-driver and nvidia-docker2 ( < URL > I modify the default-runtime in /etc/docker/daemon . json , : [ root @localhost ~ ]# cat /etc/docker/daemon . json { ' default-runtime ' : ' nvidia ' , ' runtimes ' : { ' nvidia ' : { ' path ' : ' nvidia-container-runtime ' , ' runtimeArgs ' : [] } } } . I want start minikube as runtime using nvidia , but it occurs an error : [ root @localhost ~ ]# minikube start -- container-runtime = nvidia o minikube v 0.35.0 on linux ( amd64 ) ! Failed to generate config : unknown runtime type : ' nvidia ' * Sorry that minikube crashed . If this was unexpected , we would love to hear from you : - < URL > .	2	2
1 1 0.8 1.0 1.1 1.0 1.3 1.5 0.6666666666666666 1.0 1.2058823529411764 34 1.0 2 6 15 47	Default to bash shell when $SHELL is missing . The current message is somewhat confused : : $ unset SHELL $ minikube docker-env The default lines below are for a sh/bash shell , you can specify the shell you're using , with the -- shell flag . 棣冩寴 Error detecting shell : Error : Unknown shell 棣冩▼ minikube is exiting due to an error . If the above message is not useful , open an issue : 棣冩啝 < URL > .	2	0
2 2 1.4 1.0 1.2 1.0 1.35 1.5 1.3333333333333333 1.0 0.0 0 0.0 2 7 17 37	open < user > . minikube/machines/minikube/config . json : The system cannot find the file specified . . Command : : minikube start . Returns o minikube v 0.35.0 on windows ( amd64 ) ! Unable to start VM : Error loading existing host . Please try running [ minikube delete ] , then run [ minikube start ] again . : filestore : open C : /Users/goodw . minikube/machines/minikube/config . json : The system cannot find the file specified . Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL > minikube delete ! Failed to delete cluster : open C : /Users/goodw . minikube/machines/minikube/config . json : The system cannot find the file specified . Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL > I've tried deleting the . minikube file manually and no luck . Says I require permission from DESKTOP-UTI3DC7/username to make changes to this folder . I'm assuming that's the VM ? It seems like minikube is in some half installed state and I would like to just start from scratch again . : minikube logs . returns ! api load : filestore : open C : /Users/goodw . minikube/machines/minikube/config . json : The system cannot find the file specified . Sorry that minikube crashed . If this was unexpected , we would love to hear from you : < URL > Using windows 10 pro . I tried to start minikubes originally with : minikube start vm-driver = hyperv hyper v-v irtual-switch = my_network_switch . It seemed to have made the cluster but then hung . And then I couldn't run any minikube commands . Let me know if there is any other information I can provide .  	1	0
0 0 1.0 1.0 1.2 1.5 1.35 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 6 9 33	Minikube - Enable colors error message . Steps to reproduce the issue : Run ' minikube status ' The message ' Enable colors error ' shoud not be displayed in output . This issue occurs in releases 1.20 and 1.19 . Full output of : minikube logs . command : Full output of failed command :	2	0
2 2 1.4 1.0 1.1 1.0 1.2 1.0 1.6666666666666667 2.0 0.9416058394160584 137 1.0 3 4 16 55	logs command outputs lines slowly , takes > 45 seconds to run . I realized that on macOS , with minikube v 1.18.1 , the logs command runs so slowly that I can literally see the lines drawn individually . A ran it with the : time . command , and it can take over a minute to run . Here is output from my last , slightly faster run : : ________________________________________________________ Executed in 53.95 secs fish external usr time 0.99 secs 101.00 micros 0.99 secs sys time 1.08 secs 1410.00 micros 1.08 secs . It feels like the buffer is flushed between each line , but I don't see the CPU load to indicate that is in fact the case . We should throw a profiler at the : logs . command to better understand why it's taking so much time . To duplicate : : minikube start -- container-runtime = containerd . : minikube logs . As a comparison , this command runs in 1.2 seconds : : time minikube ssh ' sudo journalctl -u kubelet -- no-pager ' > /tmp/x .   	1	0
2 2 1.6 2.0 1.5 2.0 1.45 2.0 2.0 2.0 0.8193832599118943 227 1.0 0 3 3 22	add detailed info about what each integration test is doing to the site . currently our : make generate-docs . creates this page < URL > which generates the site based on the Comments on the Test and validate the func , I would like these descriptions to exaclty say what it is doing instead of a < URL > : validateBuildImage makes sures that minikube image build works as expected . it should say : validateBuildImage makes sures that `minikube image build` works as expected builds an image using `minikube image build` and the Dockerfile located in test/integration/testdata/build/Dockerfile which adds a relative file into a busybox container and then ensures the image was build using image inspect inside minikbue special case : in case of contained it starts the buildkit inside minikube currently skipped on None Driver and Github Actions on MacOs , .    	1	2
1 1 1.6 2.0 1.2 1.5 1.15 1.5 1.6666666666666667 2.0 1.1595744680851063 94 1.0 1 4 8 38	Memory requirements do not match kubeadm any longer ( since 1.20 ) . Minikube still has : : minUsableMem = 953 // 1GB In MiB : Kubernetes will not start with less minRecommendedMem = 1907 // 2GB In MiB : Warn at no lower than existing configurations . Kubeadm now has : : // Below that amount of RAM running a stable control plane would be difficult . ControlPlaneMem = 1700 . Since < URL > ( and < URL > from 2048M to 1700M ) : v 1.20.0 -alpha . 0-252-ge5bb66f8990 . : v 1.20.0 -alpha . 0-253-gc6975a77508 . Problem seen in #10014 #10071 : [ ERROR Mem ]: the system RAM ( 953 MB ) is less than the minimum 1700 MB . So we now need to require 2G more strict , and maybe suggest 4G ? Like suggestd in #8050 Alternatively try to run with : -- ignore-preflight-errors = Mem . , like we currently do for Swap ?  	1	0
2 2 1.4 2.0 1.6 2.0 1.3 1.0 1.3333333333333333 2.0 0.0 0 0.0 0 0 3 15	sch_netem kernel module missing for network emulation ( tc netem ) . The exact command to reproduce the issue : Application based : Run a container like debian in priviledged mode in a pod and one pod that uses the < URL > network emulation linux tool , in my case < URL > . Pumba has no effect trying to mess with the network of the debian container . Pumba uses netem . modprobe : Try to run : minikube ssh . and then : modprobe -n sch_netem . , which says netem module is missing . With netem on a priviledged debian container in minikube : Get a bash in the debian container and try to modify the eth0 virtual interface by changing : qdisc . to : netem loss . ( which is essentially what pumba does ) . The command is : tc qdisc add dev eth0 root netem loss random 100% . . A related issue , and the environment in which I found out this is not working , is described < URL > . This is a minikube specific issue , as it works with other kubeadm based clusters and GKE and others . It also works locally on a laptop just with docker images . The full output of the command that failed : * : modprobe -n sch_netem . -> : modprobe : FATAL : Module sch_netem not found in directory /lib/modules/ 4.19.76 . * : tc qdisc add dev eth0 root netem loss random 100 . -> : invalid qdisc name . The operating system version : * 5.4.1 -2-MANJARO * minikube version : v 1.5.2 commit : 792dbf92a1de583fcee76f8791cff12e0c9440ad Additional info I just wanted to let you know this issue exists . Maybe we can do something about it , as minikube is a great environment for starting with chaos engineering , and not being able to mess with the network ( in this way ) is a big drawback . Also it took me quite some time to figure this out , as I had no access to other clusters :D .	2	1
2 2 1.8 2.0 1.6 2.0 1.35 2.0 1.6666666666666667 2.0 0.5 2 0.5 3 4 15 57	kic : custom kic/docker/osx message getting displayed for all drivers/runtimes . On linux using service : : minikube service hello-minikube1 | -----------|----------------- | -------------|------------------------- | | NAMESPACE | NAME | TARGET PORT | URL | | -----------|----------------- | -------------|------------------------- | | default | hello-minikube1 | | < URL > | | -----------|----------------- | -------------|------------------------- | 棣冨竴 Opening service default/hello-minikube1 in default browser ... 閳跨媴绗?Because you are using docker driver on Mac , the terminal needs to be open to run it . . The warn message should be specific for kic/docker/osx .	0	0
1 1 1.4 1.0 1.6 2.0 1.1 1.0 1.3333333333333333 1.0 1.3333333333333333 3 1.0 2 5 7 19	Add start option , to download only . Make it possible to download files ahead-of-time , without actually install and loading them . : $ . minikube start -- cache-images -- download-only 棣冩 minikube v 0.34.1 on linux ( amd64 ) 棣冦仚 Caching images in the background ... 棣冩崚 Downloading Minikube ISO ... 184.30 MB / 184.30 MB [= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ==] 100.00% 0s 棣冩崙 Downloading kubeadm v 1.13.3 棣冩崙 Downloading kubelet v 1.13.3 閳?Waiting for image caching to complete ... 閴佹棑绗?Download complete and in download only mode . This will save the files for later in the local minikube cache , before actually starting the host .	2	1
2 2 1.4 2.0 1.1 1.5 1.1 1.0 1.3333333333333333 2.0 0.0 0 0.0 1 3 4 19	Add option to skip VM driver PreCreateCheck . Environment : Windows 10 Version 10.0.16299.904 Minikube : v 0.34.1 The supported drivers provide a : PreCreateCheck () . method that is < URL > by minikube in order to ensure the driver works properly and its prerequisites are fulfiled . Unfortunately those prerequisites are not without flaws . The one implemented in the < URL > invokes some < URL > as part of its checks . The one listed before checks if the user is part of the Hyper-V Administrators group . While this check is a good idea , it does not work if the computer joined a domain and cannot reach its Domain Controller . Therefore , when e.g. working in Home Office or in the train without network connectivity at all , I cannot use minikube in this setup . Please add an option to skip ( or ignore failing ) : PreCreateCheck . s . The minishift project is doing < URL > specific to hyperv , but I'd propose to just add a general : I-know-what-I'm-doing . -option to continue when those checks failed .	2	1
0 0 0.6 0.0 1.2 1.5 0.9 1.0 0.0 0.0 0.7333333333333333 15 0.0 1 1 2 28	Allow minikube to run in a non-default docker host . : docker-machine create -- driver = virtualbox default eval $(docker-machine env default . This is will create a docker host inside of vbox VM , which currently causes minikube to fail , if there is no docker daemon running directly on a user's machine .  	1	1
2 2 1.2 1.0 1.6 2.0 1.45 2.0 1.0 1.0 0.0 0 0.0 1 9 12 57	add support for rootless docker . Hello , I am trying to run minikube using docker driver in rootless mode . But minikube is not able to detect the docker daemon . ' docker ' driver reported an issue : ' docker version -- format {{ . Server . Os}}-{{ . Server . Version }}' exit status 1 : Cannot connect to the Docker daemon at unix :// /var/run/docker . sock . Is the docker daemon running ? The docker host env is set to export DOCKER_HOST = unix :// /run/user/1000/docker . sock	2	1
0 0 0.4 0.0 0.8 0.5 0.85 1.0 0.6666666666666666 0.0 0.0 0 0.0 1 4 17 45	minikube install uses winget on Windows . The fact that Windows Package Manager ( winget ) is still in preview , should be mentioned on the installation tab for Windows . Maybe hence winget also shouldn't be named as the first option to install minikube .  	2	2
1 1 0.8 1.0 0.6 0.0 0.85 1.0 0.6666666666666666 1.0 0.984375 64 1.0 7 10 12 31	Remove inaccurate documentation from kubernetes.io/ .  	2	2
0 0 0.8 0.0 0.7 0.0 0.95 1.0 0.0 0.0 0.0 0 0.0 2 2 4 30	rootless podman driver . Steps to reproduce the issue : : minikube start -- driver = podman . Full output of failed command : : 棣冩 minikube v 1.11.0 on Arch rolling 閳?KUBECONFIG = /path-to-my-config 閴?Using the podman ( experimental ) driver based on user configuration 閴?' podman ' driver reported an issue : ' sudo -k -n podman version -- format {{ . Version }}' exit status 1 : sudo : a password is required 棣冩寱 Suggestion : Add your user to the ' sudoers ' file : ' ejiek ALL =( ALL ) NOPASSWD : /usr/bin/podman ' 棣冩憣 Documentation : < URL > 棣冩寴 Failed to validate ' podman ' driver . This means that with podman driver user is obligated to have root access without password wich is not minimal provilage . < URL > to run podman rootless . < URL > is a bit of Arch wiki on configuring podman not to require root permissions . I suggest having a flag for this driver to either use sudo or not . There is a good chance that rootless podman is not suitable for minikube yet due to it's < URL > ( this is a linkg to a particular version of podman )	2	1
1 1 1.0 1.0 0.9 1.0 1.05 1.0 1.3333333333333333 1.0 0.0 0 0.0 2 2 5 21	VBOX_VERR_VMX_NO_VMX : Point users to better documentation . The exact command to reproduce the issue : The full output of the command that failed : The output of the : minikube logs . command : The operating system version :  	2	2
2 2 1.6 2.0 1.5 2.0 1.45 2.0 1.3333333333333333 2.0 0.0 0 0.0 0 1 1 6	minikube start -- allow additional arguments to -- mount-string . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): I'm leaning towards bug report as I would expect : -- mount-string . to mimic : minikube mount . Please provide the following details : Environment : Minikube version ( use : minikube version . ): v 0.32.0 - OS ( e.g. from /etc/os-release ): Windows 10 - VM Driver ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep DriverName . ): hyperv - ISO version ( e.g. : cat ~ / . minikube/machines/minikube/config . json | grep -i ISO . or : minikube ssh cat /etc/VERSION . ): v 0.32.0 - Install tools : What happened : Additional : mount . arguments passed into : -- mount-string . are ignored Ran : minikube start -- mount -- mount-string='C:/mydrive:/mydrive -- uid 911 -- gid 911 -- 9p-version = 9p2000 . L ' minikube ssh ls -l /mydrive . What you expected to happen : expected it to properly set ownership of : /mydrive . to 911 as a workaround to #2290 How to reproduce it ( as minimally and precisely as possible ): see what happened	2	0
0 0 0.4 0.0 0.8 0.5 0.95 1.0 0.0 0.0 0.0 0 0.0 0 5 12 40	Ability to add disks to the minikube VM on Hyperkit ? . While adding extra disks with the kvm2 driver is easy , I couldn't find anything to add disks on : hyperkit . . Anyone already doing this perhaps ? Thanks !	2	1
1 1 0.8 1.0 1.1 1.0 1.25 2.0 1.0 1.0 1.5 2 1.5 3 4 9 39	Provide more descriptive error codes for GUEST_START . GUEST_START serves as a catch-all error for cluster node failing to start . Can minikube provide more descriptive error codes ( is the failure owing to memory/cpu/disk constraints ) ?	0	1
1 1 0.8 1.0 0.9 1.0 1.0 1.0 1.0 1.0 0.6 15 0.0 1 4 5 19	Warn when /var is almost at memory capacity for VM drivers . Related to < URL >  	1	1
2 2 1.4 2.0 1.4 2.0 1.2 1.0 2.0 2.0 0.0 0 0.0 6 11 12 32	NO_PROXY doesn't respect addr/range - contrary to documentation . For whatever reason , NO_PROXY does not respect an addr/range argument in Ubuntu 16.04 . Therefore , 192.168.99.1 /24 will not make a no proxy exception for an address in that subnet In this case , what I found works is to replace it with a specific ip address of the minikube vm in NO_PROXY , which in my case was 192.168.99.100 . Please add this to the documentation , thanks !  	2	2
2 2 1.6 2.0 1.5 2.0 1.1 1.0 1.3333333333333333 2.0 1.1428571428571428 70 1.0 3 7 15 56	clarify ' m01 ' stopped in stop command . per slack conversation , < URL > the stop command shows . ' m01 ' stopped this could be confusing for the users . we could clarify . node ' m01 ' stopped	0	1
2 2 1.2 1.0 1.1 1.0 1.05 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 2 5 23	Question : no node-provided-ip annotation on node . Regarding to < URL > and < URL > I'm expecting to find the minikube external IP address in node annotation . : . kubernetes.io/provided-node-ip . Is it a bug ? or there is an option how to get it there ? The exact command to reproduce the issue : : kubectl describe node . The full output of the command that failed : : 閴?helm kubectl describe node Name : minikube Roles : master Labels : beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=minikube kubernetes.io/os=linux node-role.kubernetes.io/master= Annotations : kubeadm.alpha.kubernetes.io/cri-socket : /var/run/dockershim . sock node.alpha.kubernetes.io/ttl : 0 volumes.kubernetes.io/controller-managed-attach-detach : true CreationTimestamp : Fri , 05 Jul 2019 11:19:46 +0300 . The operating system version : OSX 閴?helm minikube version minikube version : v 1.2.0 OSX : 10.14.5 ( 18F132 )	2	1
0 0 0.8 0.0 0.8 0.5 1.1 1.0 0.6666666666666666 0.0 0.0 0 0.0 0 0 5 8	Allow set `ImageRepository` for kubeadm init . Is this a BUG REPORT or FEATURE REQUEST ? ( choose one ): FEATURE REQUEST Please provide the following details : Environment : : minikube version : v 0.28.2 OS : cat : /etc/os-release : No such file or directory VM driver : ' DriverName ' : ' hyperkit ' , ISO version ' Boot2DockerURL ' : ' file :// /path/to/ . minikube/cache/iso/minikube-v 0.28.1 . iso ' , . Can the kubeadm bootstrapper adds a configuration option support for < URL > , which enables setting different image base repository . This feature will help developers who are having difficulties to connect to k8s.gcr.io ( namely , China ) , as long as they can set the repository prefix to self managed proxies .  	1	1
2 2 1.4 2.0 1.3 1.5 1.35 2.0 2.0 2.0 0.0 0 0.0 0 1 4 22	minikube config emits no such file config . json when setting container-runtime . The exact command to reproduce the issue : Below , setting configuration options for my minikube environment . Minikube is not running when performing these commands . ( tensor ) ~ > minikube config set cpus 6 閳跨媴绗?These changes will take effect upon a minikube delete and then a minikube start ( tensor ) ~ > minikube config set memory 8192 閳跨媴绗?These changes will take effect upon a minikube delete and then a minikube start ( tensor ) ~ > minikube config set container-runtime crio 棣冩寴 Set failed : [ config . Load : stat /Users/mack/ . minikube/profiles/minikube/config . json : no such file or directory ] 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The full output of the command that failed : 棣冩寴 Set failed : [ config . Load : stat /Users/mack/ . minikube/profiles/minikube/config . json : no such file or directory ] 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The output of the : minikube logs . command : ( tensor ) ~ > minikube logs 棣冩寴 Error getting config : stat /Users/mack/ . minikube/profiles/minikube/config . json : no such file or directory 棣冩▼ Sorry that minikube crashed . If this was unexpected , we would love to hear from you : 棣冩啝 < URL > The operating system version : macOS 10.15.1 ( Catalina )	0	0
0 0 0.6 0.0 0.5 0.0 0.95 1.0 0.0 0.0 1.2857142857142858 7 1.0 1 2 6 29	Upgrade Docker to 18.09 ( and containerd ) . While Kubernetes still recommends * using 18.06 , some people are using the minikube Docker daemon for other purposes ( such as building ) and could benefit from upgrading to 18.09 . < URL > Version 18.06.2 is recommended , but 1.11 , 1.12 , 1.13 , 17.03 and 18.09 are known to work as well . Keep track of the latest verified Docker version in the Kubernetes release notes . < URL > The list of validated docker versions has changed . 1.11.1 and 1.12.1 have been removed . The current list is 1.13.1 , 17.03 , 17.06 , 17.09 , 18.06 , 18.09 . As a part of this upgrade , we can stop using : docker-runc . and : docker-containerd . / : docker-containerd-ctr . and instead use the regular : runc . and : containerd . / : ctr . . 18.06.3 -ce : docker/ docker/docker docker/docker-containerd docker/docker-containerd-ctr docker/docker-containerd-shim docker/dockerd docker/docker-init docker/docker-proxy docker/docker-run . 18.09.5 : docker/ docker/containerd docker/containerd-shim docker/ctr docker/docker docker/dockerd docker/docker-init docker/docker-proxy docker/runc .  	1	1
0 0 1.0 1.0 1.2 1.5 1.15 1.5 0.6666666666666666 0.0 0.47619047619047616 21 0.0 2 3 4 24	Add gopogh to scheduled stop windows github actions test . Currently gopogh doesn't work with these tests because of < URL > Once that is resolved , add gopogh back to those tests . Reference #10132	0	1
2 2 2.0 2.0 1.7 2.0 1.4 2.0 2.0 2.0 0.0 0 0.0 2 3 6 17	libvirt group check causes false positives . PR < URL > checks whether the user is in the libvirt group . However , there's many ways to give users permissions to use system libvirt that do not involve this group . It may be named differently , may be pointing to a remote libvirt instance , or a polkit rule like this one may be used : : polkit . addRule(function(action , subject ) { if ( action.id = = ' org . libvirt . unix . manage ' && subject . isInGroup('wheel ' )) { return polkit . Result . YES ; } }); . Minikube should first check whether it can actually access libvirt , and only then check for the missing group . Using : -- force . is not an appropriate workaround since that would skip other safety checks , too . Related : < URL > < URL >  	1	0
2 2 1.0 1.0 1.3 1.5 1.15 1.0 1.0 1.0 1.0 49 1.0 0 0 4 30	( meta ) Make Windows/Hyper-V a first-class solution . Here are the issues to be resolved : Hyper-V : Use the ' Default Switch ' ( NAT based ) by default - #4079 Hyper-V : Stable integration testing #3591 Hyper-V : minikube mount fails with windows 10 default switch - #3591 [ NON_C_DRIVE ] when run from C :/ but user config is on different drive - #4079 Please feel free to comment with any others that cause a large user experience gap for users of Windows + Hyper-V .	0	0
0 0 1.2 1.0 1.3 1.0 1.3 1.5 1.0 1.0 1.25 4 1.5 2 5 8 25	Change the error message when Config file is not available . The exact command to reproduce the issue : minikube-windows-amd64 . exe start -- alsologtostderr -- v = 8 The full output of the command that failed : The full output is correct but this first line kind of pops out - : Error reading config file at C : /Users/Pranav . Jituri/ . minikube/config/config . json : open C : /Users/Pranav . Jituri/ . minikube/config/config . json : The system cannot find the file specified . . The operating system version : Windows 10 Enterprise . Instead of showing it as an error , this needs a better solution message . Earlier , I used to see this and get confused as to what was wrong . Later on figured that this is rather the external custom config which is used .	2	1
2 2 1.6 2.0 1.3 1.5 0.95 1.0 1.6666666666666667 2.0 0.0 0 0.0 0 0 3 17	minikube tunnel hard coded docker . Steps to reproduce the issue : minikube start -- embed-certs -- kubernetes-version 1.20.10 -- addons registry-creds -- driver podman minikube tunnel E0927 19:02:29 . 077832 374498 out . go : 451 ] unable to execute get port 22 for ' minikube ' : docker container inspect -f '' {{( index ( index . NetworkSettings . Ports ' 22/tcp ' ) 0 ) . HostPort }}'' minikube : exec : ' docker ' : executable file not found in $PATH  	1	0
2 2 1.8 2.0 1.3 1.0 1.15 1.0 1.6666666666666667 2.0 1.1466666666666667 75 1.0 1 2 5 24	Fork the machine driver ' generic ' to minikube , prepare for renaming it . Previously we have forked the docker-machine ( libmachine ) driver called ' none ' : < URL > < URL > We want to do the same thing for the driver called ' generic ' , as used in PR #9545 < URL > < URL > Once it is in our ' drivers ' code base , we can rename it as see fit ... Possibly to something like ' remote ' , to be decided ( see #7772 )  	1	1
